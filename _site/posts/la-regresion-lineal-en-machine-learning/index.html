<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="es" xml:lang="es"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.552">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Hugo Valenzuela Chaparro">
<meta name="description" content="El alcance de la regresión lineal en machine learning, su ajuste de parámetros con descenso del gradiente y métodos de regularización">

<title>HUGOJIRA - La Regresión Lineal en Machine Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../favicon.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "Sin resultados",
    "search-matching-documents-text": "documentos encontrados",
    "search-copy-link-title": "Copiar el enlace en la búsqueda",
    "search-hide-matches-text": "Ocultar resultados adicionales",
    "search-more-match-text": "resultado adicional en este documento",
    "search-more-matches-text": "resultados adicionales en este documento",
    "search-clear-button-title": "Borrar",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancelar",
    "search-submit-button-title": "Enviar",
    "search-label": "Buscar"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="HUGOJIRA - La Regresión Lineal en Machine Learning">
<meta property="og:description" content="El alcance de la regresión lineal en machine learning, su ajuste de parámetros con descenso del gradiente y métodos de regularización">
<meta property="og:image" content="https://hugojira.com/posts/la-regresion-lineal-en-machine-learning/mco.png">
<meta property="og:site_name" content="HUGOJIRA">
<meta property="og:image:height" content="960">
<meta property="og:image:width" content="1344">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">HUGOJIRA</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Buscar"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Navegación de palanca" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../blog.html"> 
<span class="menu-text">huBlog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../proyectos/index.html"> 
<span class="menu-text">Proyectos</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/hugojira/hublog"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Alternar modo oscuro"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">La Regresión Lineal en Machine Learning</h1>
                  <div>
        <div class="description">
          El alcance de la regresión lineal en machine learning, su ajuste de parámetros con descenso del gradiente y métodos de regularización
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">Machine Learning</div>
                <div class="quarto-category">Regresión</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Autor/a</div>
      <div class="quarto-title-meta-contents">
               <p>Hugo Valenzuela Chaparro </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Fecha de publicación</div>
      <div class="quarto-title-meta-contents">
        <p class="date">17 de septiembre de 2024</p>
      </div>
    </div>
    
      <div>
      <div class="quarto-title-meta-heading">Fecha de modificación</div>
      <div class="quarto-title-meta-contents">
        <p class="date-modified">17 de septiembre de 2024</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">En esta página</h2>
   
  <ul>
  <li><a href="#introducción" id="toc-introducción" class="nav-link active" data-scroll-target="#introducción">Introducción</a></li>
  <li><a href="#regresión" id="toc-regresión" class="nav-link" data-scroll-target="#regresión">Regresión</a>
  <ul>
  <li><a href="#obtención-de-datos-para-regresión" id="toc-obtención-de-datos-para-regresión" class="nav-link" data-scroll-target="#obtención-de-datos-para-regresión">Obtención de datos para regresión</a></li>
  </ul></li>
  <li><a href="#regresión-lineal-simple" id="toc-regresión-lineal-simple" class="nav-link" data-scroll-target="#regresión-lineal-simple">Regresión Lineal Simple</a></li>
  <li><a href="#regresión-lineal-múltiple" id="toc-regresión-lineal-múltiple" class="nav-link" data-scroll-target="#regresión-lineal-múltiple">Regresión Lineal Múltiple</a></li>
  <li><a href="#regresión-polinomial" id="toc-regresión-polinomial" class="nav-link" data-scroll-target="#regresión-polinomial">Regresión Polinomial</a></li>
  <li><a href="#supuestos-de-la-regresión-lineal" id="toc-supuestos-de-la-regresión-lineal" class="nav-link" data-scroll-target="#supuestos-de-la-regresión-lineal">Supuestos de la regresión lineal</a></li>
  <li><a href="#mínimos-cuadrados-ordinarios-como-función-de-costo" id="toc-mínimos-cuadrados-ordinarios-como-función-de-costo" class="nav-link" data-scroll-target="#mínimos-cuadrados-ordinarios-como-función-de-costo">Mínimos cuadrados ordinarios como función de costo</a></li>
  <li><a href="#ajuste-de-los-parámetros-minimizando-la-función-de-costo" id="toc-ajuste-de-los-parámetros-minimizando-la-función-de-costo" class="nav-link" data-scroll-target="#ajuste-de-los-parámetros-minimizando-la-función-de-costo">Ajuste de los parámetros minimizando la función de costo</a>
  <ul>
  <li><a href="#el-método-de-mínimos-cuadrados-clásico" id="toc-el-método-de-mínimos-cuadrados-clásico" class="nav-link" data-scroll-target="#el-método-de-mínimos-cuadrados-clásico">El método de mínimos cuadrados clásico</a></li>
  <li><a href="#el-método-del-gradiente-descendiente" id="toc-el-método-del-gradiente-descendiente" class="nav-link" data-scroll-target="#el-método-del-gradiente-descendiente">El método del gradiente descendiente</a></li>
  <li><a href="#cuál-usar-de-los-2-métodos" id="toc-cuál-usar-de-los-2-métodos" class="nav-link" data-scroll-target="#cuál-usar-de-los-2-métodos">¿Cuál usar de los 2 métodos?</a></li>
  </ul></li>
  <li><a href="#métodos-de-regularización" id="toc-métodos-de-regularización" class="nav-link" data-scroll-target="#métodos-de-regularización">Métodos de regularización</a>
  <ul>
  <li><a href="#regresión-lineal-ridge-limitando-el-tamaño-de-los-parámetros" id="toc-regresión-lineal-ridge-limitando-el-tamaño-de-los-parámetros" class="nav-link" data-scroll-target="#regresión-lineal-ridge-limitando-el-tamaño-de-los-parámetros">Regresión Lineal Ridge: limitando el tamaño de los parámetros</a></li>
  <li><a href="#regresión-lineal-lasso-un-selector-de-características" id="toc-regresión-lineal-lasso-un-selector-de-características" class="nav-link" data-scroll-target="#regresión-lineal-lasso-un-selector-de-características">Regresión Lineal Lasso: un selector de características</a></li>
  <li><a href="#interpretación-geométrica-de-ridge-y-lasso" id="toc-interpretación-geométrica-de-ridge-y-lasso" class="nav-link" data-scroll-target="#interpretación-geométrica-de-ridge-y-lasso">Interpretación geométrica de ridge y lasso</a></li>
  </ul></li>
  <li><a href="#conclusiones" id="toc-conclusiones" class="nav-link" data-scroll-target="#conclusiones">Conclusiones</a></li>
  <li><a href="#referencias" id="toc-referencias" class="nav-link" data-scroll-target="#referencias">Referencias</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="introducción" class="level2">
<h2 class="anchored" data-anchor-id="introducción">Introducción</h2>
<p>La regresión lineal puede parecer algo muy simple, pero en realidad tiene un alcance muy grande para modelar datos. Puede verse desde una perspectiva meramente geométrica, sin embargo, tiene unos supuestos estadísticos que son sumamente importantes que se cumplan si queremos que el modelo funcione y sea aplicable.</p>
<p>En este post, quiero enfocarme en la regresión lineal desde la perspectiva del machine learning, por lo que asume un conocimiento básico previo de éste. Cubrirá los siguientes puntos:</p>
<ul>
<li>Qué es la regresión lineal en una y varias variables</li>
<li>Qué significa que la regresión lineal se puede aplicar mientras se tenga linealidad en los parámetros</li>
<li>Supuestos para la validez de la regresión lineal</li>
<li>Cómo se define la función de costo para la regresión lineal</li>
<li>Diferencia entre la solución cerrada para minimizar la función de costo y el método del gradiente descendiente</li>
<li>Métodos de regularización para regresión lineal y su interpretación geométrica</li>
</ul>
<p>Sin más por comentar en la introducción, comencemos con el post.</p>
</section>
<section id="regresión" class="level2">
<h2 class="anchored" data-anchor-id="regresión">Regresión</h2>
<p>La regresión es una técnica de modelación proveniente de la estadística para modelar y analizar la relación entre variables. Se tiene una variable <span class="math inline">\(y\)</span> llamada independiente, la cual se modela en función de otra variable <span class="math inline">\(x\)</span> llamada independiente, que bien se puede tener una <span class="math inline">\(n\)</span> cantidad de variables <span class="math inline">\(x_1, x_2, ... , x_n\)</span> como se verá más adelante.</p>
<p>Es importante mencionar que, para evitar confusiones con la noción de indepencia de probabilidad, se prefiere nombrar a <span class="math inline">\(y\)</span> como la variable respuesta y a <span class="math inline">\(x\)</span> como las variables predictoras. En el contexto de machine learning, a las variables <span class="math inline">\(x\)</span> se les denomina características (features) y a <span class="math inline">\(y\)</span> etiquetas (labels).</p>
<p>El tipo de datos con los que la regresión trabaja son numéricos (flotantes), y vienen en duplas <span class="math inline">\((x_1, y_1), ... , (x_m, y_m)\)</span>, donde <span class="math inline">\(m\)</span> es la cantidad de datos que tengamos, es decir, cardinalidad del conjunto de datos. Nótese que se pueden tener múltiples características <span class="math inline">\(x\)</span>.</p>
<p>Para trabajar con regresión, si la dimensión lo permite, suele hacerse un diagrama de dispersión que es una gráfica de los datos representados como puntos. Por ejemplo, consideremos de <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span>, con 15 registros (<span class="math inline">\(m = 15\)</span>), resultando el siguiente diagrama de dispersión</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-1-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>En este caso, se ve claramente que los puntos tienen una tendencia lineal, esto es, que se les puede ajustar una recta para modelarlos con un error considerablemente bajo. La ecuación de la recta, es <span class="math inline">\(y = ax + b\)</span>, con <span class="math inline">\(a\)</span> la pendiente y <span class="math inline">\(b\)</span> la intersección en el eje <span class="math inline">\(y\)</span>. Dicha ecuación discribiría adecuadamente ese comportamiento, en las siguientes secciones veremos cómo ajustarla a los datos para encontrar los valores adecuados de <span class="math inline">\(a\)</span> y <span class="math inline">\(b\)</span>.</p>
<p>En este diagrama en partícular, los puntos fueron simulados de una recta <span class="math inline">\(y = 2x\)</span>, a la que se les agregó un error <span class="math inline">\(\varepsilon\)</span> que tiene una distribución normal con media <span class="math inline">\(\mu = 0\)</span> y desviación estandar <span class="math inline">\(\sigma = 1.5\)</span>. Teniendo así los parámetros de la recta <span class="math inline">\(a = 2\)</span> y <span class="math inline">\(b = 0\)</span></p>
<section id="obtención-de-datos-para-regresión" class="level3">
<h3 class="anchored" data-anchor-id="obtención-de-datos-para-regresión">Obtención de datos para regresión</h3>
<p>Hay 3 maneras principales para obtener datos que puedan usarse en regresión, los cuales se listan a continuación</p>
<ul>
<li><strong>Estudio retrospectivo:</strong> Aquí se tienen datos que ya fueron capturados en el pasado, y no necesariamente con una intención de ser analizados científicamente, por lo que pueden no estar en la forma más ideal posible. Los datos y sus resultados ya se tienen, no se puede intervenir ni cambiar nada para mejorar la toma de datos. Este tipo de estudio es muy común en machine learning y big data, donde obtienes datos con ciertas características y ya etiquetados que fueron obtenidos en el pasado.</li>
<li><strong>Estudio observacional:</strong> Como su nombre puede indicarlo, en este tipo de estudio solamente se observa el proceso o experimento que quiere analizarse, interviniendo mínimamente solamente para la captura de los datos. Por temas éticos, este estudio es muy usado en el área médica, pues de lo contrario se estaría imponiendo sobre las personas las condiciones que deben tener (como empezar a fumar, por ejemplo).</li>
<li><strong>Diseño experimental:</strong> Este tipo de estudio posee más estrategia. Aquí se manipulan las variables predictoras (o características) de acuerdo a un diseño experimental, para tener ciertos valores y observar su efecto en la variable respuesta.</li>
</ul>
</section>
</section>
<section id="regresión-lineal-simple" class="level2">
<h2 class="anchored" data-anchor-id="regresión-lineal-simple">Regresión Lineal Simple</h2>
<p>La <strong>regresión lineal simple</strong> es útil cuando existe una tendencia lineal en los datos. Aunque veremos más adelante que va más allá de lo lineal. Se le llama simple porque solo hay una característica <span class="math inline">\(x\)</span>, y se modela como</p>
<p><span class="math display">\[y = w_0 + w_1 x + \varepsilon\]</span> La cual puede verse como la ecuación de regresión lineal poblacional, no obstante, en regresión lineal trabajamos con <span class="math inline">\(m\)</span> número de datos, entonces para cada punto se tendrá una ecuación de regresión lineal muestral</p>
<p><span class="math display">\[y_i = w_0 + w_1 x_i + \varepsilon_i \quad i=1, 2, 3, ..., m\]</span></p>
<p>Comparándo con la ecuación de la recta, podemos interpretar los parámetros. Se tiene que <span class="math inline">\(w_1\)</span> es el cambio en la media de la distribución de <span class="math inline">\(y\)</span> producido por un cambio de unidad en <span class="math inline">\(x\)</span>. Mientras que <span class="math inline">\(w_0\)</span> simplemente es la media de la distribución de <span class="math inline">\(y\)</span> cuando <span class="math inline">\(x = 0\)</span>, si el dominio de <span class="math inline">\(x\)</span> no incluye a <span class="math inline">\(0\)</span> entonces el parámetro <span class="math inline">\(w_0\)</span> no tiene interpretación práctica.</p>
<p>Por otro lado, <span class="math inline">\(\varepsilon\)</span> son los errores o ruido que se tiene en cada predicción de <span class="math inline">\(y\)</span>. Se asumen que dichos errores tienen una distribución normal con media igual a cero <span class="math inline">\(\mu = 0\)</span> y una varianza desconicida <span class="math inline">\(\sigma^2\)</span>. Es importante notar que, esa varianza es la misma en todos los puntos de la regresión, lo que se conoce como <em>homocedasticidad</em>. A los errores <span class="math inline">\(\varepsilon\)</span> se les llama residuales pues se definen como <span class="math inline">\(y - \hat{y}\)</span>.</p>
<p>Ahora bien, retomando el diagrama de dispersión que se vio anteriormente, el cual tiene proviene de la recta <span class="math inline">\(y = 2x\)</span> con un error <span class="math inline">\(\varepsilon\)</span> con media <span class="math inline">\(0\)</span> y varianza <span class="math inline">\(\sigma^2 = 1.5^2\)</span>. Se le puede ajustar una recta de regresión lineal y se vería como la siguiente figura</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Obteniendo los coeficientes estimados <span class="math inline">\(\hat{w_0} = 0.3045\)</span> y <span class="math inline">\(\hat{w_1} = 2.0274\)</span>. Teniendo así una ecuación de regresión lineal con la que podemos hacer predicciones</p>
<p><span class="math display">\[ \hat{y} = 0.3045 + 2.0274 x\]</span> Notemos que los parámetros reales de donde provienen los datos son <span class="math inline">\(0\)</span> y <span class="math inline">\(2\)</span>, pero por la naturaleza estocástica de los errores, los estimados no coinciden con exactitud. Entre mayor sea el número de datos <span class="math inline">\(m\)</span>, los parámetros estimados serán más cercanos a los originales.</p>
</section>
<section id="regresión-lineal-múltiple" class="level2">
<h2 class="anchored" data-anchor-id="regresión-lineal-múltiple">Regresión Lineal Múltiple</h2>
<p>Para el caso en que la variable respuesta <span class="math inline">\(y\)</span> está realcionada con <span class="math inline">\(n\)</span> variables regresoras, se tiene el modelo de la <strong>regresión lineal múltiple</strong></p>
<p><span class="math display">\[ y = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n + \varepsilon \]</span> Así, a los parámetros <span class="math inline">\(w_j \quad j = 0, 1, 2, ..., n\)</span> se les llama <em>coeficientes de regresión</em>. Este modelo, al ser de multivariable, describe un hiperplano en el espacio <span class="math inline">\(n\)</span>-dimensional de las variables regresoras <span class="math inline">\(x\)</span>.</p>
<p>Haciendo sentido de los parámetros. El parámetro <span class="math inline">\(w_j\)</span> representa el cambio esperado en la respuesta <span class="math inline">\(y\)</span> por unidad de cambio en <span class="math inline">\(x_j\)</span>, cuando todo las demás variables regresoras <span class="math inline">\(x_i(i\ne j)\)</span> se mantienen constantes.Por otra parte, el parámetro <span class="math inline">\(w_0\)</span> es la intersección del hiperplano en <span class="math inline">\(y\)</span>, si el dominio de los datos incluye <span class="math inline">\(x_1 = x_2 = ... = x_n = 0\)</span> entonces <span class="math inline">\(w_0\)</span> es la media de <span class="math inline">\(y\)</span> cuando <span class="math inline">\(x_1 = x_2 = ... = x_n = 0\)</span>; de lo contrario, <span class="math inline">\(w_0\)</span> no tiene interpretación física.</p>
<p>Concluyendo, en el modelo de regresión simple y múltiple, la linealidad se tiene en los parámetros. Esto extiende más allá el concepto de linealidad aunque las variables regresoras <span class="math inline">\(x\)</span> no sean lineales, esto se verá a continuación en la sección de <em>regresión polinomial</em>.</p>
</section>
<section id="regresión-polinomial" class="level2">
<h2 class="anchored" data-anchor-id="regresión-polinomial">Regresión Polinomial</h2>
<p>Ya vimos que una regresión lineal de <span class="math inline">\(n\)</span> variables regresoras genera un hiperplano <span class="math inline">\(n\)</span>-dimensional. Ahora bien, cualquier modelo que es lineal en los parámetros <span class="math inline">\(w\)</span>’s es un modelo de regresión lineal, independientemente de la forma de la superficie que genere.</p>
<p>Como su nombre lo indica, la <strong>regresión polinomial</strong> de una variable <span class="math inline">\(x\)</span> se define a partir de los polinomios, y se modela de la siguiente forma</p>
<p><span class="math display">\[ y = w_0 + w_1x + w_2x^2 + ... + w_nx^n + \varepsilon\]</span> La cual es <em>lineal en los parámetros</em>, por lo que se puede trabajar como una regresión lineal múltiple y usar la misma metodología. Para hacer esto, podemos definir <span class="math inline">\(x_j = x^j \quad j=1, 2, 3, ... , n\)</span>. Por ejemplo, para el caso de un polinomio de grado 2 con <span class="math inline">\(y = w_0 + w_1x + w_2x^2 + \varepsilon\)</span>, tendríamos <span class="math inline">\(x_1 = x, x_2 = x^2\)</span> y la regresión se convertiría en una lineal múltiple</p>
<p><span class="math display">\[ y = w_0 + w_1x_1 + w_2x_2 + \varepsilon\]</span> Con este tipo de regresión, hay que ser muy cuidadosos. Ya que un polinomio de grado considerablemente alto puede apróximar muy bien una curva no lineal, sin embargo, hay que tratar de hacer transformaciones y mantener el grado lo más bajo posible para evitar un sobreajuste del modelo (empezar a generalizar el error mismo).</p>
</section>
<section id="supuestos-de-la-regresión-lineal" class="level2">
<h2 class="anchored" data-anchor-id="supuestos-de-la-regresión-lineal">Supuestos de la regresión lineal</h2>
<p>Es importante saber cuándo es prudente utilizar un modelo de regresión lineal. Para que se cumplan las condiciones de la regresión lineal simple y múltiple, se tienen los siguientes supuestos necesarios</p>
<ol type="1">
<li><strong>Linealidad:</strong> La relación entre las variables regresoras <span class="math inline">\(x\)</span> y la variable repuesta <span class="math inline">\(y\)</span> debe ser lineal. Notemos que en la regresión polinomial esto va más allá, necesitando solo linealidad en los parámetros.</li>
<li><strong>Homocedasticidad:</strong> Significa que la varianza de los errores es constante.</li>
<li><strong>Normalidad de los errores:</strong> Los residulales, es decir los errores, deben seguir una distribución normal con media 0 y varianza <span class="math inline">\(\sigma^2\)</span>.</li>
<li><strong>Errores independientes (no autocorrelación):</strong> Los errores no deben estar correlacionados entre ellos, es otras palabras, deben ser independientes e idénticamente distribuidos.</li>
<li><strong>No multicolinealidad:</strong> No existe correlación entre las variables regresoras.</li>
<li><strong>No exogeneidad:</strong> Las variables regresoras y los errores no están correlacionados.</li>
</ol>
</section>
<section id="mínimos-cuadrados-ordinarios-como-función-de-costo" class="level2">
<h2 class="anchored" data-anchor-id="mínimos-cuadrados-ordinarios-como-función-de-costo">Mínimos cuadrados ordinarios como función de costo</h2>
<p>En machine learning, la base para ajustar los parámetros a nuestros datos, lo que se conoce como aprender, radica en la optimización de una función objetivo. Dicho de otra manera, es minimizar una función de costo, donde dicha función sería una medida del error en las predicciones del modelo.</p>
<p>En la regresión lineal no es la excepción. Es muy conocido el <em>método de mínimos cuadrados</em> para ajustar los parámetros, donde se define la función de costo <span class="math inline">\(J\)</span> de <em>mínimos cuadrados ordinarios</em> (MCO) de la siguiente manera</p>
<p><span class="math display">\[ J(y, \hat{y}) = \frac{1}{m} \sum_{i=1}^{m} (y_i - \hat{y}_i)^2 \]</span> Donde el factor <span class="math inline">\(\frac{1}{m}\)</span> se multiplica como una especie de promedio en todos los datos. Algunas referencias también manejan multiplicarlo por <span class="math inline">\(\frac{1}{2}\)</span> por conveniencia algebraica.</p>
<p>Notemos que la diferencia <span class="math inline">\(y_i - \hat{y}_i\)</span> es la distancia que hay desde un punto <span class="math inline">\(\hat{y} (x_i)\)</span> en la recta de la predicción <span class="math inline">\(\hat{y}\)</span> hasta el correspondiente punto <span class="math inline">\(y(x_i)\)</span> en la recta original <span class="math inline">\(y\)</span> de donde provienen los datos. Está al cuadrado para evitar que las sumas de esos términos puedan dar cero. Así, podemos interpretarlo gráficamente como unos cuadrados en dichos puntos <span class="math inline">\(x_i\)</span>, considerando el mismo diagrama de dispersión de este post, se vería</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Nótese que los ejes fueron escalados para una mejor visualización. Ahora, al ajustar los parámetros <span class="math inline">\(w\)</span> minimizando la función de costo de MCO, lo que se está haciendo gráficamente es ajustar la recta de predicción de tal manera que se minimize el área de esos cuadrados; de ahí el nombre de este método.</p>
<p>Para minimizar dicha función, es mejor expresarla en término de los parámetros <span class="math inline">\(w\)</span>, para esto, en el caso de una variable regresora sustituimos <span class="math inline">\(\hat{y} = w_0 + w_1 x\)</span> en la función de costo de MCO, así</p>
<p><span class="math display">\[ J(w_0, w_1) = \frac{1}{m} \sum_{i=1}^{m} (y_i - w_0 - w_1 x_i)^2 \]</span> En la siguiente sección veremos cómo minimizarla, ejemplificando el caso con una sola variable regresora <span class="math inline">\(x\)</span>.</p>
</section>
<section id="ajuste-de-los-parámetros-minimizando-la-función-de-costo" class="level2">
<h2 class="anchored" data-anchor-id="ajuste-de-los-parámetros-minimizando-la-función-de-costo">Ajuste de los parámetros minimizando la función de costo</h2>
<p>Hay 3 maneras principales de minimizar la función de costo de mínimos cuadrados ordinarios (MCO) <span class="math inline">\(J\)</span>. El método clásico, utiliza cálculo diferencial y el criterio de las derivadas para minimizar. La segunda, utiliza el método de la máxima verosimilitud, un concepto estadístico. Se encuentra matemáticamente que ambos métodos llegan a la misma forma solución. La tercera, usa el método más utilizado en machine learning, el del gradiente descendiente (gradient descent).</p>
<p>Mi objetivo de este post no es hacer demostraciones matemáticas ni cálculos completos, sino que ilustrar la metodología, así que nos enfocaremos en el método clásico y el de gradiente descendiente. Veamos a más detalle</p>
<section id="el-método-de-mínimos-cuadrados-clásico" class="level3">
<h3 class="anchored" data-anchor-id="el-método-de-mínimos-cuadrados-clásico">El método de mínimos cuadrados clásico</h3>
<p>Para minimizar la función de costo, los estimados de los parámetros <span class="math inline">\(w_0\)</span>, <span class="math inline">\(w_1\)</span>, que son <span class="math inline">\(\hat{w_0}\)</span>, <span class="math inline">\(\hat{w_1}\)</span> respectivamente, deben satisfacer</p>
<p><span class="math display">\[ \frac{\partial J}{\partial w_0} \bigg|_{\hat{w_0}, \hat{w_1}}= - \frac{2}{m} \sum_{i=1}^{m} (y_i - \hat{w_0} - \hat{w_1} x_i) = 0\]</span> y</p>
<p><span class="math display">\[ \frac{\partial J}{\partial w_1} \bigg|_{\hat{w_0}, \hat{w_1}} = - \frac{2}{m} \sum_{i=1}^{m} (y_i - \hat{w_0} - \hat{w_1} x_i) x_i = 0\]</span></p>
<p>de acuerdo al criterio de la primer derivada, ésta debe ser 0 para que sea un mínimo.</p>
<p>Simplificando las expresiones, separando las sumatorias y resolviendo para <span class="math inline">\(\hat{w_0}\)</span>, <span class="math inline">\(\hat{w_1}\)</span>, se tienen las soluciones a los parámetros estimados que minimizan la función de MCO</p>
<p><span class="math display">\[ \hat{w_0} =  \bar{y} - \hat{w_1} \bar{x}\]</span> y</p>
<p><span class="math display">\[ \hat{w_1} = \frac{ \sum_{i=1}^{m} y_i x_i - \frac{(\sum_{i=1}^{m} y_i)(\sum_{i=1}^{m} x_i)}{n} }{ \sum_{i=1}^{m} x_i^2 - \frac{(\sum_{i=1}^{m}x_i)^2}{m} } \]</span></p>
<p>donde <span class="math inline">\(\bar{x} = \frac{1}{m}  \sum_{i=1}^{m} x_i\)</span> y <span class="math inline">\(\bar{y} = \frac{1}{m}  \sum_{i=1}^{m} y_i\)</span> son los promedios de <span class="math inline">\(x_i\)</span>, <span class="math inline">\(y_i\)</span>, respectivamente.</p>
<p>Para el caso multivariable, se tiene una solución de los parámetros estimados <span class="math inline">\(W^{*}\)</span> en forma matricial, dado por</p>
<p><span class="math display">\[ W^{*} = (X^T X)^{-1} X^{T} Y \]</span> Donde <span class="math inline">\(X\)</span> es la <em>matriz de diseño</em>, la cual es de tamaño <span class="math inline">\(m \times (n + 1)\)</span>. Donde <span class="math inline">\(m\)</span> es el número de datos y <span class="math inline">\(n\)</span> el número de variables regresoras. Se construye poniendo en filas los vectores de datos <span class="math inline">\(x_j = (1, x_{j1}, x_{j2}, ..., x_{jn})\)</span>, donde <span class="math inline">\(j = 1, 2, 3, ..., m\)</span>, en todos los datos. Por otro lado, <span class="math inline">\(Y\)</span> es la matriz (o vector) de tamaño <span class="math inline">\(m \times 1\)</span>, que se construye poniendo todos los puntos <span class="math inline">\(y\)</span> de donde se hará la regresión.</p>
<p>Así,</p>
<p><span class="math display">\[ X = \begin{pmatrix}
1 &amp; x_{11} &amp; x_{12} &amp; ... &amp; x_{1n} \\
1 &amp; x_{21} &amp; x_{22} &amp; ... &amp; x_{2n} \\
&amp;  &amp; \vdots &amp;  &amp;  \\
1 &amp; x_{m1} &amp; x_{m2} &amp; ... &amp; x_{mn}
\end{pmatrix}  \]</span></p>
<p><span class="math display">\[ Y = \begin{pmatrix}
y_1 \\
y_2 \\
y_3 \\
\vdots \\
y_m
\end{pmatrix}  \]</span></p>
</section>
<section id="el-método-del-gradiente-descendiente" class="level3">
<h3 class="anchored" data-anchor-id="el-método-del-gradiente-descendiente">El método del gradiente descendiente</h3>
<p>Retomando conceptos de cálculo diferencial para una variable, la derivada de una función <span class="math inline">\(f\)</span> en un punto dado <span class="math inline">\(x_0\)</span>, nos da la pendiende de la recta tangente a la curva en ese punto. En otras palabras, esa pendiente puede interprestarse como la razón de cambio de la función en ese punto y el signo indica hacia donde está cambiando. Extendiendo este concepto hacia varias variables, el gradiente de una función <span class="math inline">\(f\)</span>, denotado como <span class="math inline">\(\nabla f\)</span>, es un vector que indica hacia dónde la función tiene el mayor incremento, y su magnitud es la razón de cambio.</p>
<p>El método del gradiente descendiente, las derivadas son respecto a los parámetros <span class="math inline">\(w\)</span> que queremos ajustar, y como su nombre lo indica, hace uso del gradiente para minimizar la función de costo <span class="math inline">\(J(w)\)</span>. Para el caso de la regresión lineal, el algoritmo se define de la siguiente manera</p>
<p><span class="math display">\[ w_i^{(\tau + 1)} := w_i^{(\tau)} - \alpha \frac{\partial J(w)}{\partial w_i}\]</span> donde <span class="math inline">\(J(w)\)</span> esl función de costo de mínimos cuadrados ordinarios. La derivada está multiplicada por un <span class="math inline">\(-1\)</span>, lo que asegura que se mueva en la dirección de menor crecimiento, tenemos que <span class="math inline">\(\alpha\)</span> es una constante llamada tasa de aprendizaje, controla qué tanto se ajusta el valor de <span class="math inline">\(w_i\)</span>. En machine learning, <span class="math inline">\(\alpha\)</span> suele tomar valores muy pequeños, como <span class="math inline">\(0.01, 0.001,\)</span> etc. Por otro lado, <span class="math inline">\(\tau\)</span> es un número entero que indica en la iteración que estamos del algoritmo.</p>
<p>Así, se estarán ajustando los parámetros <span class="math inline">\(w_i\)</span> de manera iterativa hasta que se cumpla un cierto criterio o se alcance una tolerancia del error. Cabe señalar que en la primera iteración, los parámetros se inicializan de manera aleatoria.</p>
<p>Por lo tanto, para el caso de regresión lineal simple, si utilizamos el método del gradiente descendiente tendriamos lo siguiente</p>
<p><span class="math display">\[ w_0^{(\tau + 1)} := w_0^{(\tau)} + \alpha \frac{2}{m} \sum_{i=1}^{m} (y_i - \hat{y_i})\]</span> <span class="math display">\[ w_1^{(\tau + 1)} := w_1^{(\tau)} + \alpha \frac{2}{m} \sum_{i=1}^{m} (y_i - \hat{y_i}) x_i\]</span> Donde <span class="math inline">\(\hat{y_i}\)</span> son las predicciones en la iteración <span class="math inline">\(\tau\)</span>, o sea antes de hacer la actualización de los parámetros.</p>
</section>
<section id="cuál-usar-de-los-2-métodos" class="level3">
<h3 class="anchored" data-anchor-id="cuál-usar-de-los-2-métodos">¿Cuál usar de los 2 métodos?</h3>
<p>La respuesta rádica en la complejidad computacional y el tiempo que toman los cálculos. En machine learning se trabajan con datos demasiado grandes, más de lo que se solía hacer en la estadística convencional, por lo tanto, la forma matricial de obtener los parámetros puede llegar a ser muy costosa computacionalmente hablando si se tienen muchos datos y muchas características (variables regresoras). Así, es más conveniente utilizar el método del gradiente descendiente, que en la práctica suele ser más eficiente.</p>
</section>
</section>
<section id="métodos-de-regularización" class="level2">
<h2 class="anchored" data-anchor-id="métodos-de-regularización">Métodos de regularización</h2>
<p>La regularización en regresión lineal, es para reducir la complejidad del modelo evitando así un sobreajuste. Esto se logra añadiendo un término a la función de costo de MCO, lo que acota los parámetros evitando que crezcan demasiado. Hay 2 casos, <em>Ridge</em> y <em>Lasso</em>, como se verá a continuación.</p>
<section id="regresión-lineal-ridge-limitando-el-tamaño-de-los-parámetros" class="level3">
<h3 class="anchored" data-anchor-id="regresión-lineal-ridge-limitando-el-tamaño-de-los-parámetros">Regresión Lineal Ridge: limitando el tamaño de los parámetros</h3>
<p>La <strong>regresión ridge</strong> hace que los parámetros <span class="math inline">\(w_i\)</span> se encojan, es decir, que no tomen valores tan grandes. Se define con la siguiente función de costo <span class="math inline">\(J_R(w)\)</span></p>
<p><span class="math display">\[ J_R(W) = \frac{1}{m} \sum_{i=1}^{m} (y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^{n} w_j^2 \]</span> Sumando así el término <span class="math inline">\(\lambda \sum_{j=1}^{n} w_j^2\)</span> de penalización. Donde <span class="math inline">\(\lambda\)</span> controla que tanto se encojen los parámetros, entre más grande es el valor de <span class="math inline">\(\lambda\)</span> están más límitados los parámetros. Se tiene que cuando <span class="math inline">\(\lambda \xrightarrow{}{} \infty\)</span>, los parámetros <span class="math inline">\(w_i\)</span> tienden a <span class="math inline">\(0\)</span>.</p>
<p>Notemos que el parámetro <span class="math inline">\(w_0\)</span> no se incluye, ya que solo se toma en cuenta <span class="math inline">\(w_1, w_2, ..., w_n\)</span>. Eso es porque <span class="math inline">\(w_0\)</span>, como ya vimos, es el valor promedio de <span class="math inline">\(y\)</span> cuando <span class="math inline">\(x_1=x_2=...=x_n=0\)</span>. No sería conveniente imponer restricciones o encoger dicho parámetro entonces.</p>
<p>A la regresión ridge también se le conoce como <em>regularización L2</em>, debido a que el término <span class="math inline">\(\sum_{j=1}^{n} w_j^2\)</span> es la norma L2 del vector de parámetros <span class="math inline">\(w = (w_1, w_2, ..., w_n)\)</span>.</p>
</section>
<section id="regresión-lineal-lasso-un-selector-de-características" class="level3">
<h3 class="anchored" data-anchor-id="regresión-lineal-lasso-un-selector-de-características">Regresión Lineal Lasso: un selector de características</h3>
<p>La <strong>regresión lasso</strong> actúa como un selector de características, ya que impone que algunos de los parámetros <span class="math inline">\(w_i\)</span> sean igual a <span class="math inline">\(0\)</span>. Se define con la siguiente función de costo <span class="math inline">\(J_l(w)\)</span></p>
<p><span class="math display">\[ J_l(W) = \frac{1}{m} \sum_{i=1}^{m} (y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^{n} |w_j| \]</span> Sumando así el término <span class="math inline">\(\lambda \sum_{j=1}^{n} |w_j|\)</span> de penalización. Similarmente, lasso también hace que los parámetros se encojan hacía 0, pero a diferencia de ridge, si <span class="math inline">\(\lambda\)</span> es suficientemente grande algunos parámetros <span class="math inline">\(w_i\)</span> van a ser <span class="math inline">\(0\)</span>. Así, la variable regresora (o característica) <span class="math inline">\(x_i\)</span> asociado al parámetro <span class="math inline">\(w_i\)</span> no estará involucrada en la ecuación de regresión, por esta razón lasso es un selector de características.</p>
<p>Igual que en el caso de regresión ridge, el parámetro <span class="math inline">\(w_0\)</span> no se incluye, ya que no sería conveniente imponer restricciones o hacer 0 el parámetro <span class="math inline">\(w_0\)</span>, siendo que es es el valor promedio de <span class="math inline">\(y\)</span> cuando <span class="math inline">\(x_1=x_2=...=x_n=0\)</span>.</p>
<p>A la regresión lasso también se le conoce como <em>regularización L1</em>, debido a que el término <span class="math inline">\(\sum_{j=1}^{n} |w_j|\)</span> es la norma L1 del vector de parámetros <span class="math inline">\(w = (w_1, w_2, ..., w_n)\)</span>.</p>
</section>
<section id="interpretación-geométrica-de-ridge-y-lasso" class="level3">
<h3 class="anchored" data-anchor-id="interpretación-geométrica-de-ridge-y-lasso">Interpretación geométrica de ridge y lasso</h3>
<p>Surge la pregunta, ¿Por qué la regresión ridge encoje los coeficientes pero la regresión lasso fuerza algunos de ellos a ser <span class="math inline">\(0\)</span>? Para contestar esto, encontré que la mejor manera es gráficamente.</p>
<p>La función de costo de mínimos cuadrados ordinarios MCO, es una función convexa, por lo que es posible minimizarla. Ahora bien, si la graficamos en el espacio de los parámetros al minimizar se pueden probar todos las combinaciones de parámetros, sin embargo, al añadir un término de penalización como en ridge y lasso, se hace una constricción en el espacio en el que puede bucarse el mínimo. Veamos la siguiente figura (extraída del libro Introduction to Statistical Learning)</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="contour-ridge-lasso.jpg" title="Curvas de nivel y constricciones de MCO con regularización" class="img-fluid figure-img"></p>
<figcaption>Curvas de nivel de la función de costo de MCO en rojo y las constricciones de ridge y lasso en azul. Graficadas en el espacio de 2 parámetros <span class="math inline">\(w_1\)</span> y <span class="math inline">\(w_2\)</span>. El mínimo se encuentra en <span class="math inline">\(\hat{w}\)</span>.</figcaption>
</figure>
</div>
<p>En este caso, es una regresión en 2 dimensiones con <span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span> y las gráficas están hechas en el espacio de los parámetros <span class="math inline">\(w_1\)</span>, <span class="math inline">\(w_2\)</span>. Las líneas rojas son las curvas de nivel de la función de costo de MCO, y las áreas sólidas azules son las regiones de constricciones impuestas en los parámetros <span class="math inline">\(w_1\)</span>, <span class="math inline">\(w_2\)</span> por los términos de penalización de lasso y ridge, respectivamente.</p>
<p>Para el caso de ridge, tenemos la constricción el círculo <span class="math inline">\(w_1^2 + w_2^2 \le s\)</span>, y para lasso el rombo <span class="math inline">\(|w_1| + |w_2| \le s\)</span>. El punto de minimización, donde están los mejores parámetros estimados, es la intersección más pronta entre la curva de nivel de MCO y el área de constricción. Podemos observar claramente que para lasso, este punto de intersección más pronto se encuentra en la punta del rombo, es decir, sobre un eje donde el parámetro <span class="math inline">\(w_2 = 0\)</span>; mientras que para ridge, el punto se encuentra sobre la circunferencia fuera de los ejes, por los que los parámetros estarán encojidos por la constricción pero no serán 0. Esto se extiende a cualquier dimensión, para <span class="math inline">\(n \gt 2\)</span>, el rombo se convierte en un politopo mientras que el círculo en una hiperesfera. Entonces para el politopo siempre habrá esquinas donde los parámetros sean 0.</p>
</section>
</section>
<section id="conclusiones" class="level2">
<h2 class="anchored" data-anchor-id="conclusiones">Conclusiones</h2>
<p>En este post, se vió cómo se define la regresión lineal en una y varias variables, y los supuestos que se deben cumplir para su utilización. Se explicó cómo minimizar su función de costo de mínimos cuadrados ordinarios en la manera clásica y con el método de gradiente descendiente. También, se vieron los métodos de regularización de regresión ridge y lasso, y cómo influyen en los parámetros encojiéndolos o haciendo algunos de ellos <span class="math inline">\(0\)</span>, respectivamente.</p>
<p>Como trabajo futuro, queda llevar este conocimiento a implementación en código, para ver su funcionalidad práctica. Existen librerías para hacer regresión lineal, ridge y lasso; en Python está <code>scikit-learn</code>; mientras que en R tenemos <code>stats</code> y <code>glmnet</code>.</p>
<p>Recomiendo ampliamente revisar las referencias usadas para este post, hay conocimiento muy bueno en ellas.</p>
</section>
<section id="referencias" class="level2">
<h2 class="anchored" data-anchor-id="referencias">Referencias</h2>
<ul>
<li><p>Douglas C. Montgomery, Elizabeth A. Peck, G. Geoffrey Vining (2021). <em>Introduction to Linear Regression Analysis</em>. Sixth Edition.</p></li>
<li><p>Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani (2023). <em>An Introduction to Statistical Learning with Applications in R</em>. 2nd Edition.</p></li>
<li><p>Yunpeng Tai (2021). <em>A Survey Of Regression Algorithms And Connections With Deep Learning</em>.</p></li>
<li><p>Mukul Ranjan (2022). <em>How does Lasso regression(L1) encourage zero coefficients but not the L2?</em>. Medium. <a href="https://medium.com/@mukulranjan/how-does-lasso-regression-l1-encourage-zero-coefficients-but-not-the-l2-20e4893cba5d">Post link</a></p></li>
<li><p>Terence Parr. <em>A visual explanation for regularization of linear models</em>. Explained.ai. <a href="https://explained.ai/regularization/index.html">Post link</a></p></li>
<li><p>User: QuantStyle. <em>What is the computational cost of gradient descent vs linear regression?</em>. StackExchange. <a href="https://stats.stackexchange.com/questions/407921/what-is-the-computational-cost-of-gradient-descent-vs-linear-regression">Post link</a></p></li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copiado");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copiado");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/hugojira\.com\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="hugojira/hublog" data-repo-id="R_kgDOLombnQ" data-category="General" data-category-id="DIC_kwDOLombnc4CeewM" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="es" crossorigin="anonymous" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Sitio web hecho con <a href="https://quarto.org/">Quarto</a>, por Hugo Valenzuela Chaparro</p>
</div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/hugo-vach/">
      <i class="bi bi-linkedin" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/hugojira/">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="../../license.html">
<p>Licencia: CC BY-SA 4.0</p>
</a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>