[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hugo Valenzuela Chaparro",
    "section": "",
    "text": "Soy cient铆fico de datos, f铆sico de formaci贸n. Comenc茅 trabajando en proyectos de consultor铆a estad铆stica en la universidad, relacionados con la miner铆a y el manejo de las islas de calor en ciudades, fue ah铆 donde me enamor茅 de la ciencia de datos usando R. Posteriormente curs茅 la maestr铆a en ciencia de datos, en la Universidad de Sonora, donde prosegu铆 con mi especializaci贸n en Machine Learning.\nActualmente entre mis intereses est谩n: GenAI, Machine Learning y su aplicaci贸n en audio, MLOps. Visualizaci贸n de datos y soluciones mediante dashboards.\nTrabajo profesionalmente tanto con R como Python.\nEste sitio web es mi portafolio, donde pueden encontrar mi blog personal huBlog (bueno, en realidad, m谩s que un blog se considerar谩 mi jard铆n digital) y mis proyectos.\n\n\n\nBI Support Engineer @ Orbtiware | Enero 2023 - presente\n\n\n\nUniversidad de Sonora | Licenciatura en F铆sica |  Hermosillo, Sonora, M茅xico | 2019\nUniversidad de Sonora | Maestr铆a en Ciencia de Datos |  Hermosillo, Sonora, M茅xico | 2023"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "huBlog",
    "section": "",
    "text": "El contenido escrito aqu铆 estar谩 en espa帽ol, pudiendo contener palabras en ingl茅s.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5 Cosas que Aprend铆 en F铆sica y Uso en Ciencia de Datos\n\n\n\n\n\n\nCiencia de Datos\n\n\nF铆sica\n\n\n\nConexiones de la f铆sica y la ciencia de datos\n\n\n\n\n\n28 sept 2024\n\n\nHugo Valenzuela Chaparro\n\n\n5 minutos\n\n\n\n\n\n\n\n\n\n\n\n\nLa Regresi贸n Lineal en Machine Learning\n\n\n\n\n\n\nMachine Learning\n\n\nRegresi贸n\n\n\n\nEl alcance de la regresi贸n lineal en machine learning, su ajuste de par谩metros con descenso del gradiente y m茅todos de regularizaci贸n\n\n\n\n\n\n17 sept 2024\n\n\nHugo Valenzuela Chaparro\n\n\n20 minutos\n\n\n\n\n\n\n\n\n\n\n\n\nPost de bienvenida\n\n\n\n\n\n\nInfo\n\n\n\n\n\n\n\n\n\n27 mar 2024\n\n\nHugo Valenzuela Chaparro\n\n\n1 minutos\n\n\n\n\n\n\nNo hay resultados"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Hugo Valenzuela Chaparro",
    "section": "",
    "text": "Universidad de Sonora | Licenciatura en F铆sica |  Hermosillo, Sonora, M茅xico | 2019\nUniversidad de Sonora | Maestr铆a en Ciencia de Datos |  Hermosillo, Sonora, M茅xico | 2023"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Hugo Valenzuela Chaparro",
    "section": "",
    "text": "Orbitware | Report Monitoring | Enero 2023 - presente"
  },
  {
    "objectID": "index.html#bio",
    "href": "index.html#bio",
    "title": "Hugo Valenzuela Chaparro",
    "section": "",
    "text": "Soy cient铆fico de datos, f铆sico de formaci贸n. Comenc茅 trabajando en proyectos de consultor铆a estad铆stica en la universidad, relacionados con la miner铆a y el manejo de las islas de calor en ciudades, fue ah铆 donde me enamor茅 de la ciencia de datos usando R. Posteriormente curs茅 la maestr铆a en ciencia de datos, en la Universidad de Sonora, donde prosegu铆 con mi especializaci贸n en Machine Learning.\nActualmente entre mis intereses est谩n: GenAI, Machine Learning y su aplicaci贸n en audio, MLOps. Visualizaci贸n de datos y soluciones mediante dashboards.\nTrabajo profesionalmente tanto con R como Python.\nEste sitio web es mi portafolio, donde pueden encontrar mi blog personal huBlog (bueno, en realidad, m谩s que un blog se considerar谩 mi jard铆n digital) y mis proyectos."
  },
  {
    "objectID": "index.html#educaci贸n",
    "href": "index.html#educaci贸n",
    "title": "Hugo Valenzuela Chaparro",
    "section": "",
    "text": "Universidad de Sonora | Licenciatura en F铆sica |  Hermosillo, Sonora, M茅xico | 2019\nUniversidad de Sonora | Maestr铆a en Ciencia de Datos |  Hermosillo, Sonora, M茅xico | 2023"
  },
  {
    "objectID": "index.html#experiencia",
    "href": "index.html#experiencia",
    "title": "Hugo Valenzuela Chaparro",
    "section": "",
    "text": "BI Support Engineer @ Orbtiware | Enero 2023 - presente"
  },
  {
    "objectID": "index.html#formaci贸n",
    "href": "index.html#formaci贸n",
    "title": "Hugo Valenzuela Chaparro",
    "section": "",
    "text": "Universidad de Sonora | Licenciatura en F铆sica |  Hermosillo, Sonora, M茅xico | 2019\nUniversidad de Sonora | Maestr铆a en Ciencia de Datos |  Hermosillo, Sonora, M茅xico | 2023"
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "CC BY-SA 4.0 DEED",
    "section": "",
    "text": "Todo el contenido de este sitio web, incluido el blog, est谩n bajo la licencia Creative Commons Atribuci贸n-CompartirIgual 4.0 Internacional."
  },
  {
    "objectID": "license.html#atribuci贸n-compartirigual-4.0-internacional",
    "href": "license.html#atribuci贸n-compartirigual-4.0-internacional",
    "title": "CC BY-SA 4.0 DEED",
    "section": "",
    "text": "Todo el contenido de este sitio web, incluido el blog, est谩n bajo la licencia Creative Commons Atribuci贸n-CompartirIgual 4.0 Internacional."
  },
  {
    "objectID": "posts/2024-03-30-post-de-bienvenida/index.html",
    "href": "posts/2024-03-30-post-de-bienvenida/index.html",
    "title": "Post de bienvenida",
    "section": "",
    "text": "隆Hola! Les doy la bienvenida a este primer post. Este blog en realidad es un jard铆n digital, esto quiere decir que es un espacio para almacenar mis notas de aprendizaje, as铆 c贸mo tambi茅n entradas de blog usuales. Este contenido se ir谩 mejorando y actualizando con el tiempo, nada es est谩tico.\nPr贸ximamente estar茅 populando este sitio con entradas de blog y notas, con contenido relacionado a la ciencia de datos que espero tambi茅n les sea de utilidad como a m铆.\nPara la creaci贸n del primer esqueleto de este sitio web, me inspir茅 en los sitios de Alison Hill, Beatriz Milz, Mike Mahoney, as铆 como en la documentaci贸n de Quarto.\nSin m谩s que decir, les invito a explorar el blog."
  },
  {
    "objectID": "proyectos/proyectos/2024-03-30-post-de-bienvenida/index.html",
    "href": "proyectos/proyectos/2024-03-30-post-de-bienvenida/index.html",
    "title": "Post de bienvenida",
    "section": "",
    "text": "隆Hola! Les doy la bienvenida a este primer post. Este blog en realidad es un jard铆n digital, esto quiere decir que es un espacio para almacenar mis notas de aprendizaje, as铆 c贸mo tambi茅n entradas de blog usuales. Este contenido se ir谩 mejorando y actualizando con el tiempo, nada es est谩tico.\nPr贸ximamente estar茅 populando este sitio con entradas de blog y notas, con contenido relacionado a la ciencia de datos que espero tambi茅n les sea de utilidad como a m铆.\nPara la creaci贸n del primer esqueleto de este sitio web, me inspir茅 en los sitios de Mike Mahoney, Alison Hill, as铆 como en la documentaci贸n de Quarto.\nSin m谩s que decir, les invito a explorar el blog."
  },
  {
    "objectID": "proyectos/index.html",
    "href": "proyectos/index.html",
    "title": "Proyectos",
    "section": "",
    "text": "Aqu铆 se encuentran los proyectos personales en los que he trabajado. No se incluyen aquellos proyectos en los que he colaborado en donde existan acuerdos de confidencialidad NDA.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTop Hits 1970-2019\n\n\n\nAudio\n\n\nAPI\n\n\nDashboard\n\n\nShiny\n\n\n\n\n\n\n\nHugo Valenzuela\n\n\n25 abr 2024\n\n\n\n\n\n\n\n\nNo hay resultados"
  },
  {
    "objectID": "proyectos/proyectos/2024-04-25-top-hits-features/index.html",
    "href": "proyectos/proyectos/2024-04-25-top-hits-features/index.html",
    "title": "Top Hits 1970-2019",
    "section": "",
    "text": "Para este proyecto de dashboard interactivo, se buscaba obtener patr贸n en las caracter铆sticas de audio de las canciones m谩s populares (Top Hits) desde 1970 a 2019. Esto con el objetivo de identificar la tendencia y ver c贸mo podr铆a entrarse a esas listas de reproducci贸n de popularidad.\nPara la construcci贸n del dashboard, se utiliz贸 R, con la librer铆a flexdashboard, agreg谩ndole shiny para la interactividad.\nEnlace al dashboard: https://hugojira.shinyapps.io/TopHits1970-2019/.\nEl dashboard cuenta con 2 pesta帽as principales que se ven de la siguiente manera:\n\n\n\nP谩gina de informaci贸n general\n\n\n\n\n\n\nP谩gina interactiva con shiny\n\n\n\n\nLos datos fueron descargados desde la API Web de Spotify con c贸digo en R, utilizando el wrapper Spotifyr. El c贸digo y una descripci贸n detallada puede ser encontrada en este repo de GitHub.\nLas caracter铆sticas de audio extra铆das y usadas en este Dashboard son las siguientes, con la traducci贸n al Espa帽ol inidcada en los par茅ntesis:\n\nLoudness (Sonoridad): Est谩 medida en decibelios relativos a escala completa (dBFS), lo que indica qu茅 tan amplificado est谩 el audio. Los valores normales son abajo de 0, de lo contrario suele ocurrir saturaci贸n y distorsi贸n en el audio.\n\nLas siguientes est谩n entre 0 y 1, y no tienen unidades\n\nAcousticness (Ac煤stica): Intervalo que muestra qu茅 tan ac煤stica es una canci贸n, valores cercanos a 0 es poco mientras que cercanos a 1 es mucho.\nDanceability (Bailable): Indicador calculado en base a propiedades como el tempo, estabilidad del ritmo, regularidad. Cercano a 0 indica que la canci贸n es poco bailable mientras que cercano a 1 lo es mucho.\nEnergy (Energ铆a): Intervalo que muestra qu茅 tan energ茅tica se siente una canci贸n, basado en propiedades como rango din谩mico o el timbre. El valor cercano a 0 se asocia a canciones no muy ruidosas, mientras que cercano a 1 es ruidosa.\nInstrumentalness (Instrumentaci贸n): Indica si una canci贸n sea instrumental, es decir si el valor es cercano a 1 es muy probable que sea instrumental mientras que cercano a 0 debe tener mucho contenido cantado.\nLiveness (En vivo): Indicador que se calcula en base a la detecci贸n de audiencia en la canci贸n, si es cercano a 1 es muy probable que la grabaci贸n fue hecha en vivo.\nValence (Valencia): Medida que representa la sencaci贸n de positivo o negativo de una canci贸n. Un valor cercano a 1 indica un sonido positivo (alegre, animador) mientras que cercano a 0 se asocia a un sonido m谩s negativo (deprimente, furioso).\n\nEstas y otras caracter铆sticas de audio pueden consultarse con m谩s detalle en la documentaci贸n de la API Web de Spotify."
  },
  {
    "objectID": "proyectos/proyectos/2024-04-25-top-hits-features/index.html#descripci贸n-general",
    "href": "proyectos/proyectos/2024-04-25-top-hits-features/index.html#descripci贸n-general",
    "title": "Top Hits 1970-2019",
    "section": "",
    "text": "Para este proyecto de dashboard interactivo, se buscaba obtener patr贸n en las caracter铆sticas de audio de las canciones m谩s populares (Top Hits) desde 1970 a 2019. Esto con el objetivo de identificar la tendencia y ver c贸mo podr铆a entrarse a esas listas de reproducci贸n de popularidad.\nPara la construcci贸n del dashboard, se utiliz贸 R, con la librer铆a flexdashboard, agreg谩ndole shiny para la interactividad.\nEnlace al dashboard: https://hugojira.shinyapps.io/TopHits1970-2019/.\nEl dashboard cuenta con 2 pesta帽as principales que se ven de la siguiente manera:\n\n\n\nP谩gina de informaci贸n general\n\n\n\n\n\n\nP谩gina interactiva con shiny\n\n\n\n\nLos datos fueron descargados desde la API Web de Spotify con c贸digo en R, utilizando el wrapper Spotifyr. El c贸digo y una descripci贸n detallada puede ser encontrada en este repo de GitHub.\nLas caracter铆sticas de audio extra铆das y usadas en este Dashboard son las siguientes, con la traducci贸n al Espa帽ol inidcada en los par茅ntesis:\n\nLoudness (Sonoridad): Est谩 medida en decibelios relativos a escala completa (dBFS), lo que indica qu茅 tan amplificado est谩 el audio. Los valores normales son abajo de 0, de lo contrario suele ocurrir saturaci贸n y distorsi贸n en el audio.\n\nLas siguientes est谩n entre 0 y 1, y no tienen unidades\n\nAcousticness (Ac煤stica): Intervalo que muestra qu茅 tan ac煤stica es una canci贸n, valores cercanos a 0 es poco mientras que cercanos a 1 es mucho.\nDanceability (Bailable): Indicador calculado en base a propiedades como el tempo, estabilidad del ritmo, regularidad. Cercano a 0 indica que la canci贸n es poco bailable mientras que cercano a 1 lo es mucho.\nEnergy (Energ铆a): Intervalo que muestra qu茅 tan energ茅tica se siente una canci贸n, basado en propiedades como rango din谩mico o el timbre. El valor cercano a 0 se asocia a canciones no muy ruidosas, mientras que cercano a 1 es ruidosa.\nInstrumentalness (Instrumentaci贸n): Indica si una canci贸n sea instrumental, es decir si el valor es cercano a 1 es muy probable que sea instrumental mientras que cercano a 0 debe tener mucho contenido cantado.\nLiveness (En vivo): Indicador que se calcula en base a la detecci贸n de audiencia en la canci贸n, si es cercano a 1 es muy probable que la grabaci贸n fue hecha en vivo.\nValence (Valencia): Medida que representa la sencaci贸n de positivo o negativo de una canci贸n. Un valor cercano a 1 indica un sonido positivo (alegre, animador) mientras que cercano a 0 se asocia a un sonido m谩s negativo (deprimente, furioso).\n\nEstas y otras caracter铆sticas de audio pueden consultarse con m谩s detalle en la documentaci贸n de la API Web de Spotify."
  },
  {
    "objectID": "proyectos/proyectos/2024-04-25-top-hits-features/index.html#resultados",
    "href": "proyectos/proyectos/2024-04-25-top-hits-features/index.html#resultados",
    "title": "Top Hits 1970-2019",
    "section": "Resultados",
    "text": "Resultados\nSe encontraron 3 tendencias principales analizando el cambio de las caracter铆sticas de audio a trav茅s de los a帽os.\nLa primera tendencia, fue un aumento considerable de la sonoridad, siendo medido con el promedio de todas las canciones de cada a帽o. Se observa que a partir del a帽o 1990 la sonoridad sube, lo que hace referencia al fen贸meno conocido como Loudness War (guerra del sonido), en el que se intentaba sonar cada vez m谩s fuerte en los masters de los discos. Se resume en la siguiente gr谩fica extra铆da del dashboard:\n\n\n\nLoudness War a partir de 1990\n\n\nEn la segunda tendencia, se encontr贸 que la duraci贸n de las canciones se reduce considerablemente conforme pasan los a帽os. Con lo que actualmente las canciones con corta duraci贸n son las que llegan a las listas de 茅xitos. Esto, por ejemplo, puede observarse comparando los diagramas de caja de 1970 y 2019 extra铆dos del dashboard:\n\n\n\n\nDuraci贸n de las canciones m谩s populares de 1970\n\n\n\n\n\n\nDuraci贸n de las canciones m谩s populares de 2019\n\n\nPara la tercera tendencia, se observa que la caracter铆stica de audio Ac煤stica va disminuyendo conforme pasan los a帽os. Esto es un reflejo del avance de la tecnolog铆a e instrumentos y su incorporaci贸n en la m煤sica, pues recordemos que dicha caracter铆stica de audio indica qu茅 tan ac煤stica es una canci贸n."
  },
  {
    "objectID": "proyectos/proyectos/2024-04-25-top-hits-features/index.html#conclusiones",
    "href": "proyectos/proyectos/2024-04-25-top-hits-features/index.html#conclusiones",
    "title": "Top Hits 1970-2019",
    "section": "Conclusiones",
    "text": "Conclusiones\nEn conclusi贸n, podemos ver que de las herramientas visuales dadas por un dashboard, se puede extraer informaci贸n muy 煤til para tomar decisiones, en este caso en c贸mo realizar la producci贸n de una canci贸n para tener m谩s probabilidad de llegar a los 茅xitos.\nLes agradezco su tiempo de lectura, cualquier comentario es bienvenido."
  },
  {
    "objectID": "proyectos/2024-04-25-top-hits-features/index.html",
    "href": "proyectos/2024-04-25-top-hits-features/index.html",
    "title": "Top Hits 1970-2019",
    "section": "",
    "text": "Para este proyecto de dashboard interactivo, se buscaba obtener patr贸n en las caracter铆sticas de audio de las canciones m谩s populares (Top Hits) desde 1970 a 2019. Esto con el objetivo de identificar la tendencia y ver c贸mo podr铆a entrarse a esas listas de reproducci贸n de popularidad.\nPara la construcci贸n del dashboard, se utiliz贸 R, con la librer铆a flexdashboard, agreg谩ndole shiny para la interactividad.\nEnlace al dashboard: https://hugojira.shinyapps.io/TopHits1970-2019/.\nEl dashboard cuenta con 2 pesta帽as principales que se ven de la siguiente manera:\n\n\n\nP谩gina de informaci贸n general\n\n\n\n\n\n\nP谩gina interactiva con shiny\n\n\n\n\nLos datos fueron descargados desde la API Web de Spotify con c贸digo en R, utilizando el wrapper Spotifyr. El c贸digo y una descripci贸n detallada puede ser encontrada en este repo de GitHub.\nLas caracter铆sticas de audio extra铆das y usadas en este Dashboard son las siguientes, con la traducci贸n al Espa帽ol inidcada en los par茅ntesis:\n\nLoudness (Sonoridad): Est谩 medida en decibelios relativos a escala completa (dBFS), lo que indica qu茅 tan amplificado est谩 el audio. Los valores normales son abajo de 0, de lo contrario suele ocurrir saturaci贸n y distorsi贸n en el audio.\n\nLas siguientes est谩n entre 0 y 1, y no tienen unidades\n\nAcousticness (Ac煤stica): Intervalo que muestra qu茅 tan ac煤stica es una canci贸n, valores cercanos a 0 es poco mientras que cercanos a 1 es mucho.\nDanceability (Bailable): Indicador calculado en base a propiedades como el tempo, estabilidad del ritmo, regularidad. Cercano a 0 indica que la canci贸n es poco bailable mientras que cercano a 1 lo es mucho.\nEnergy (Energ铆a): Intervalo que muestra qu茅 tan energ茅tica se siente una canci贸n, basado en propiedades como rango din谩mico o el timbre. El valor cercano a 0 se asocia a canciones no muy ruidosas, mientras que cercano a 1 es ruidosa.\nInstrumentalness (Instrumentaci贸n): Indica si una canci贸n sea instrumental, es decir si el valor es cercano a 1 es muy probable que sea instrumental mientras que cercano a 0 debe tener mucho contenido cantado.\nLiveness (En vivo): Indicador que se calcula en base a la detecci贸n de audiencia en la canci贸n, si es cercano a 1 es muy probable que la grabaci贸n fue hecha en vivo.\nValence (Valencia): Medida que representa la sencaci贸n de positivo o negativo de una canci贸n. Un valor cercano a 1 indica un sonido positivo (alegre, animador) mientras que cercano a 0 se asocia a un sonido m谩s negativo (deprimente, furioso).\n\nEstas y otras caracter铆sticas de audio pueden consultarse con m谩s detalle en la documentaci贸n de la API Web de Spotify."
  },
  {
    "objectID": "proyectos/2024-04-25-top-hits-features/index.html#descripci贸n-general",
    "href": "proyectos/2024-04-25-top-hits-features/index.html#descripci贸n-general",
    "title": "Top Hits 1970-2019",
    "section": "",
    "text": "Para este proyecto de dashboard interactivo, se buscaba obtener patr贸n en las caracter铆sticas de audio de las canciones m谩s populares (Top Hits) desde 1970 a 2019. Esto con el objetivo de identificar la tendencia y ver c贸mo podr铆a entrarse a esas listas de reproducci贸n de popularidad.\nPara la construcci贸n del dashboard, se utiliz贸 R, con la librer铆a flexdashboard, agreg谩ndole shiny para la interactividad.\nEnlace al dashboard: https://hugojira.shinyapps.io/TopHits1970-2019/.\nEl dashboard cuenta con 2 pesta帽as principales que se ven de la siguiente manera:\n\n\n\nP谩gina de informaci贸n general\n\n\n\n\n\n\nP谩gina interactiva con shiny\n\n\n\n\nLos datos fueron descargados desde la API Web de Spotify con c贸digo en R, utilizando el wrapper Spotifyr. El c贸digo y una descripci贸n detallada puede ser encontrada en este repo de GitHub.\nLas caracter铆sticas de audio extra铆das y usadas en este Dashboard son las siguientes, con la traducci贸n al Espa帽ol inidcada en los par茅ntesis:\n\nLoudness (Sonoridad): Est谩 medida en decibelios relativos a escala completa (dBFS), lo que indica qu茅 tan amplificado est谩 el audio. Los valores normales son abajo de 0, de lo contrario suele ocurrir saturaci贸n y distorsi贸n en el audio.\n\nLas siguientes est谩n entre 0 y 1, y no tienen unidades\n\nAcousticness (Ac煤stica): Intervalo que muestra qu茅 tan ac煤stica es una canci贸n, valores cercanos a 0 es poco mientras que cercanos a 1 es mucho.\nDanceability (Bailable): Indicador calculado en base a propiedades como el tempo, estabilidad del ritmo, regularidad. Cercano a 0 indica que la canci贸n es poco bailable mientras que cercano a 1 lo es mucho.\nEnergy (Energ铆a): Intervalo que muestra qu茅 tan energ茅tica se siente una canci贸n, basado en propiedades como rango din谩mico o el timbre. El valor cercano a 0 se asocia a canciones no muy ruidosas, mientras que cercano a 1 es ruidosa.\nInstrumentalness (Instrumentaci贸n): Indica si una canci贸n sea instrumental, es decir si el valor es cercano a 1 es muy probable que sea instrumental mientras que cercano a 0 debe tener mucho contenido cantado.\nLiveness (En vivo): Indicador que se calcula en base a la detecci贸n de audiencia en la canci贸n, si es cercano a 1 es muy probable que la grabaci贸n fue hecha en vivo.\nValence (Valencia): Medida que representa la sencaci贸n de positivo o negativo de una canci贸n. Un valor cercano a 1 indica un sonido positivo (alegre, animador) mientras que cercano a 0 se asocia a un sonido m谩s negativo (deprimente, furioso).\n\nEstas y otras caracter铆sticas de audio pueden consultarse con m谩s detalle en la documentaci贸n de la API Web de Spotify."
  },
  {
    "objectID": "proyectos/2024-04-25-top-hits-features/index.html#resultados",
    "href": "proyectos/2024-04-25-top-hits-features/index.html#resultados",
    "title": "Top Hits 1970-2019",
    "section": "Resultados",
    "text": "Resultados\nSe encontraron 3 tendencias principales analizando el cambio de las caracter铆sticas de audio a trav茅s de los a帽os.\nLa primera tendencia, fue un aumento considerable de la sonoridad, siendo medido con el promedio de todas las canciones de cada a帽o. Se observa que a partir del a帽o 1990 la sonoridad sube, lo que hace referencia al fen贸meno conocido como Loudness War (guerra del sonido), en el que se intentaba sonar cada vez m谩s fuerte en los masters de los discos. Se resume en la siguiente gr谩fica extra铆da del dashboard:\n\n\n\nLoudness War a partir de 1990\n\n\nEn la segunda tendencia, se encontr贸 que la duraci贸n de las canciones se reduce considerablemente conforme pasan los a帽os. Con lo que actualmente las canciones con corta duraci贸n son las que llegan a las listas de 茅xitos. Esto, por ejemplo, puede observarse comparando los diagramas de caja de 1970 y 2019 extra铆dos del dashboard:\n\n\n\n\nDuraci贸n de las canciones m谩s populares de 1970\n\n\n\n\n\n\nDuraci贸n de las canciones m谩s populares de 2019\n\n\nPara la tercera tendencia, se observa que la caracter铆stica de audio Ac煤stica va disminuyendo conforme pasan los a帽os. Esto es un reflejo del avance de la tecnolog铆a e instrumentos y su incorporaci贸n en la m煤sica, pues recordemos que dicha caracter铆stica de audio indica qu茅 tan ac煤stica es una canci贸n."
  },
  {
    "objectID": "proyectos/2024-04-25-top-hits-features/index.html#conclusiones",
    "href": "proyectos/2024-04-25-top-hits-features/index.html#conclusiones",
    "title": "Top Hits 1970-2019",
    "section": "Conclusiones",
    "text": "Conclusiones\nEn conclusi贸n, podemos ver que de las herramientas visuales dadas por un dashboard, se puede extraer informaci贸n muy 煤til para tomar decisiones, en este caso en c贸mo realizar la producci贸n de una canci贸n para tener m谩s probabilidad de llegar a los 茅xitos.\nLes agradezco su tiempo de lectura, cualquier comentario es bienvenido."
  },
  {
    "objectID": "posts/post-de-bienvenida/index.html",
    "href": "posts/post-de-bienvenida/index.html",
    "title": "Post de bienvenida",
    "section": "",
    "text": "隆Hola! Les doy la bienvenida a este primer post. Este blog en realidad es un jard铆n digital, esto quiere decir que es un espacio para almacenar mis notas de aprendizaje, as铆 c贸mo tambi茅n entradas de blog usuales. Este contenido se ir谩 mejorando y actualizando con el tiempo, nada es est谩tico.\nPr贸ximamente estar茅 populando este sitio con entradas de blog y notas, con contenido relacionado a la ciencia de datos que espero tambi茅n les sea de utilidad como a m铆.\nPara la creaci贸n del primer esqueleto de este sitio web, me inspir茅 en los sitios de Alison Hill, Beatriz Milz, Mike Mahoney, as铆 como en la documentaci贸n de Quarto.\nSin m谩s que decir, les invito a explorar el blog."
  },
  {
    "objectID": "posts/el-poder-de-la-regresion-lineal/index.html#regresi贸n",
    "href": "posts/el-poder-de-la-regresion-lineal/index.html#regresi贸n",
    "title": "La Regresi贸n Lineal en Machine Learning",
    "section": "Regresi贸n",
    "text": "Regresi贸n\nLa regresi贸n es una t茅cnica de modelaci贸n proveniente de la estad铆stica para modelar y analizar la relaci贸n entre variables. Se tiene una variable \\(y\\) llamada independiente, la cual se modela en funci贸n de otra variable \\(x\\) llamada independiente, que bien se puede tener una \\(n\\) cantidad de variables \\(x_1, x_2, ... , x_n\\) como se ver谩 m谩s adelante.\nEs importante mencionar que, para evitar confusiones con la noci贸n de indepencia de probabilidad, se prefiere nombrar a \\(y\\) como la variable respuesta y a \\(x\\) como las variables predictoras. En el contexto de machine learning, a las variables \\(x\\) se les denomina caracter铆sticas (features) y a \\(y\\) etiquetas (labels).\nEl tipo de datos con los que la regresi贸n trabaja son num茅ricos (flotantes), y vienen en duplas \\((x_1, y_1), ... , (x_m, y_m)\\), donde \\(m\\) es la cantidad de datos que tengamos, es decir, cardinalidad del conjunto de datos. N贸tese que se pueden tener m煤ltiples caracter铆sticas \\(x\\).\nPara trabajar con regresi贸n, si la dimensi贸n lo permite, suele hacerse un diagrama de dispersi贸n que es una gr谩fica de los datos representados como puntos. Por ejemplo, consideremos de \\(x\\) e \\(y\\), con 15 registros (\\(m = 15\\)), resultando el siguiente diagrama de dispersi贸n\n\n\n\n\n\n\n\n\n\nEn este caso, se ve claramente que los puntos tienen una tendencia lineal, esto es, que se les puede ajustar una recta para modelarlos con un error considerablemente bajo. La ecuaci贸n de la recta, es \\(y = ax + b\\), con \\(a\\) la pendiente y \\(b\\) la intersecci贸n en el eje \\(y\\). Dicha ecuaci贸n discribir铆a adecuadamente ese comportamiento, en las siguientes secciones veremos c贸mo ajustarla a los datos para encontrar los valores adecuados de \\(a\\) y \\(b\\).\nEn este diagrama en part铆cular, los puntos fueron simulados de una recta \\(y = 2x\\), a la que se les agreg贸 un error \\(\\varepsilon\\) que tiene una distribuci贸n normal con media \\(\\mu = 0\\) y desviaci贸n estandar \\(\\sigma = 1.5\\). Teniendo as铆 los par谩metros de la recta \\(a = 2\\) y \\(b = 0\\)\n\nObtenci贸n de datos para regresi贸n\nHay 3 maneras principales para obtener datos que puedan usarse en regresi贸n, los cuales se listan a continuaci贸n\n\nEstudio retrospectivo: Aqu铆 se tienen datos que ya fueron capturados en el pasado, y no necesariamente con una intenci贸n de ser analizados cient铆ficamente, por lo que pueden no estar en la forma m谩s ideal posible. Los datos y sus resultados ya se tienen, no se puede intervenir ni cambiar nada para mejorar la toma de datos. Este tipo de estudio es muy com煤n en machine learning y big data, donde obtienes datos con ciertas caracter铆sticas y ya etiquetados que fueron obtenidos en el pasado.\nEstudio observacional: Como su nombre puede indicarlo, en este tipo de estudio solamente se observa el proceso o experimento que quiere analizarse, interviniendo m铆nimamente solamente para la captura de los datos. Por temas 茅ticos, este estudio es muy usado en el 谩rea m茅dica, pues de lo contrario se estar铆a imponiendo sobre las personas las condiciones que deben tener (como empezar a fumar, por ejemplo).\nDise帽o experimental: Este tipo de estudio posee m谩s estrategia. Aqu铆 se manipulan las variables predictoras (o caracter铆sticas) de acuerdo a un dise帽o experimental, para tener ciertos valores y observar su efecto en la variable respuesta."
  },
  {
    "objectID": "posts/el-poder-de-la-regresion-lineal/index.html#regresi贸n-lineal-simple",
    "href": "posts/el-poder-de-la-regresion-lineal/index.html#regresi贸n-lineal-simple",
    "title": "La Regresi贸n Lineal en Machine Learning",
    "section": "Regresi贸n Lineal Simple",
    "text": "Regresi贸n Lineal Simple\nLa regresi贸n lineal simple es 煤til cuando existe una tendencia lineal en los datos. Aunque veremos m谩s adelante que va m谩s all谩 de lo lineal. Se le llama simple porque solo hay una caracter铆stica \\(x\\), y se modela como\n\\[y = w_0 + w_1 x + \\varepsilon\\] La cual puede verse como la ecuaci贸n de regresi贸n lineal poblacional, no obstante, en regresi贸n lineal trabajamos con \\(m\\) n煤mero de datos, entonces para cada punto se tendr谩 una ecuaci贸n de regresi贸n lineal muestral\n\\[y_i = w_0 + w_1 x_i + \\varepsilon_i \\quad i=1, 2, 3, ..., m\\]\nCompar谩ndo con la ecuaci贸n de la recta, podemos interpretar los par谩metros. Se tiene que \\(w_1\\) es el cambio en la media de la distribuci贸n de \\(y\\) producido por un cambio de unidad en \\(x\\). Mientras que \\(w_0\\) simplemente es la media de la distribuci贸n de \\(y\\) cuando \\(x = 0\\), si el dominio de \\(x\\) no incluye a \\(0\\) entonces el par谩metro \\(w_0\\) no tiene interpretaci贸n pr谩ctica.\nPor otro lado, \\(\\varepsilon\\) son los errores o ruido que se tiene en cada predicci贸n de \\(y\\). Se asumen que dichos errores tienen una distribuci贸n normal con media igual a cero \\(\\mu = 0\\) y una varianza desconicida \\(\\sigma^2\\). Es importante notar que, esa varianza es la misma en todos los puntos de la regresi贸n, lo que se conoce como homocedasticidad. A los errores \\(\\varepsilon\\) se les llama residuales pues se definen como \\(y - \\hat{y}\\).\nAhora bien, retomando el diagrama de dispersi贸n que se vio anteriormente, el cual tiene proviene de la recta \\(y = 2x\\) con un error \\(\\varepsilon\\) con media \\(0\\) y varianza \\(\\sigma^2 = 1.5^2\\). Se le puede ajustar una recta de regresi贸n lineal y se ver铆a como la siguiente figura\n\n\n\n\n\n\n\n\n\nObteniendo los coeficientes estimados \\(\\hat{w_0} = 0.3045\\) y \\(\\hat{w_1} = 2.0274\\). Teniendo as铆 una ecuaci贸n de regresi贸n lineal con la que podemos hacer predicciones\n\\[ \\hat{y} = 0.3045 + 2.0274 x\\] Notemos que los par谩metros reales de donde provienen los datos son \\(0\\) y \\(2\\), pero por la naturaleza estoc谩stica de los errores, los estimados no coinciden con exactitud. Entre mayor sea el n煤mero de datos \\(m\\), los par谩metros estimados ser谩n m谩s cercanos a los originales."
  },
  {
    "objectID": "posts/el-poder-de-la-regresion-lineal/index.html#regresi贸n-lineal-m煤ltiple",
    "href": "posts/el-poder-de-la-regresion-lineal/index.html#regresi贸n-lineal-m煤ltiple",
    "title": "La Regresi贸n Lineal en Machine Learning",
    "section": "Regresi贸n Lineal M煤ltiple",
    "text": "Regresi贸n Lineal M煤ltiple\nPara el caso en que la variable respuesta \\(y\\) est谩 realcionada con \\(n\\) variables regresoras, se tiene el modelo de la regresi贸n lineal m煤ltiple\n\\[ y = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n + \\varepsilon \\] As铆, a los par谩metros \\(w_j \\quad j = 0, 1, 2, ..., n\\) se les llama coeficientes de regresi贸n. Este modelo, al ser de multivariable, describe un hiperplano en el espacio \\(n\\)-dimensional de las variables regresoras \\(x\\).\nHaciendo sentido de los par谩metros. El par谩metro \\(w_j\\) representa el cambio esperado en la respuesta \\(y\\) por unidad de cambio en \\(x_j\\), cuando todo las dem谩s variables regresoras \\(x_i(i\\ne j)\\) se mantienen constantes.Por otra parte, el par谩metro \\(w_0\\) es la intersecci贸n del hiperplano en \\(y\\), si el dominio de los datos incluye \\(x_1 = x_2 = ... = x_n = 0\\) entonces \\(w_0\\) es la media de \\(y\\) cuando \\(x_1 = x_2 = ... = x_n = 0\\); de lo contrario, \\(w_0\\) no tiene interpretaci贸n f铆sica.\nConcluyendo, en el modelo de regresi贸n simple y m煤ltiple, la linealidad se tiene en los par谩metros. Esto extiende m谩s all谩 el concepto de linealidad aunque las variables regresoras \\(x\\) no sean lineales, esto se ver谩 a continuaci贸n en la secci贸n de regresi贸n polinomial."
  },
  {
    "objectID": "posts/el-poder-de-la-regresion-lineal/index.html#regresi贸n-polinomial",
    "href": "posts/el-poder-de-la-regresion-lineal/index.html#regresi贸n-polinomial",
    "title": "La Regresi贸n Lineal en Machine Learning",
    "section": "Regresi贸n Polinomial",
    "text": "Regresi贸n Polinomial\nYa vimos que una regresi贸n lineal de \\(n\\) variables regresoras genera un hiperplano \\(n\\)-dimensional. Ahora bien, cualquier modelo que es lineal en los par谩metros \\(w\\)s es un modelo de regresi贸n lineal, independientemente de la forma de la superficie que genere.\nComo su nombre lo indica, la regresi贸n polinomial de una variable \\(x\\) se define a partir de los polinomios, y se modela de la siguiente forma\n\\[ y = w_0 + w_1x + w_2x^2 + ... + w_nx^n + \\varepsilon\\] La cual es lineal en los par谩metros, por lo que se puede trabajar como una regresi贸n lineal m煤ltiple y usar la misma metodolog铆a. Para hacer esto, podemos definir \\(x_j = x^j \\quad j=1, 2, 3, ... , n\\). Por ejemplo, para el caso de un polinomio de grado 2 con \\(y = w_0 + w_1x + w_2x^2 + \\varepsilon\\), tendr铆amos \\(x_1 = x, x_2 = x^2\\) y la regresi贸n se convertir铆a en una lineal m煤ltiple\n\\[ y = w_0 + w_1x_1 + w_2x_2 + \\varepsilon\\] Con este tipo de regresi贸n, hay que ser muy cuidadosos. Ya que un polinomio de grado considerablemente alto puede apr贸ximar muy bien una curva no lineal, sin embargo, hay que tratar de hacer transformaciones y mantener el grado lo m谩s bajo posible para evitar un sobreajuste del modelo (empezar a generalizar el error mismo)."
  },
  {
    "objectID": "posts/el-poder-de-la-regresion-lineal/index.html#m铆nimos-cuadrados-ordinarios-como-funci贸n-de-costo",
    "href": "posts/el-poder-de-la-regresion-lineal/index.html#m铆nimos-cuadrados-ordinarios-como-funci贸n-de-costo",
    "title": "La Regresi贸n Lineal en Machine Learning",
    "section": "M铆nimos cuadrados ordinarios como funci贸n de costo",
    "text": "M铆nimos cuadrados ordinarios como funci贸n de costo\nEn machine learning, la base para ajustar los par谩metros a nuestros datos, lo que se conoce como aprender, radica en la optimizaci贸n de una funci贸n objetivo. Dicho de otra manera, es minimizar una funci贸n de costo, donde dicha funci贸n ser铆a una medida del error en las predicciones del modelo.\nEn la regresi贸n lineal no es la excepci贸n. Es muy conocido el m茅todo de m铆nimos cuadrados para ajustar los par谩metros, donde se define la funci贸n de costo \\(J\\) de m铆nimos cuadrados ordinarios (MCO) de la siguiente manera\n\\[ J(y, \\hat{y}) = \\frac{1}{m} \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2 \\] Donde el factor \\(\\frac{1}{m}\\) se multiplica como una especie de promedio en todos los datos. Algunas referencias tambi茅n manejan multiplicarlo por \\(\\frac{1}{2}\\) por conveniencia algebraica.\nNotemos que la diferencia \\(y_i - \\hat{y}_i\\) es la distancia que hay desde un punto \\(\\hat{y} (x_i)\\) en la recta de la predicci贸n \\(\\hat{y}\\) hasta el correspondiente punto \\(y(x_i)\\) en la recta original \\(y\\) de donde provienen los datos. Est谩 al cuadrado para evitar que las sumas de esos t茅rminos puedan dar cero. As铆, podemos interpretarlo gr谩ficamente como unos cuadrados en dichos puntos \\(x_i\\), considerando el mismo diagrama de dispersi贸n de este post, se ver铆a\n\n\n\n\n\n\n\n\n\nN贸tese que los ejes fueron escalados para una mejor visualizaci贸n. Ahora, al ajustar los par谩metros \\(w\\) minimizando la funci贸n de costo de MCO, lo que se est谩 haciendo gr谩ficamente es ajustar la recta de predicci贸n de tal manera que se minimize el 谩rea de esos cuadrados; de ah铆 el nombre de este m茅todo.\nPara minimizar dicha funci贸n, es mejor expresarla en t茅rmino de los par谩metros \\(w\\), para esto, en el caso de una variable regresora sustituimos \\(\\hat{y} = w_0 + w_1 x\\) en la funci贸n de costo de MCO, as铆\n\\[ J(w_0, w_1) = \\frac{1}{m} \\sum_{i=1}^{m} (y_i - w_0 - w_1 x_i)^2 \\] En la siguiente secci贸n veremos c贸mo minimizarla, ejemplificando el caso con una sola variable regresora \\(x\\)."
  },
  {
    "objectID": "posts/el-poder-de-la-regresion-lineal/index.html#ajuste-de-los-par谩metros-minimizando-la-funci贸n-de-costo",
    "href": "posts/el-poder-de-la-regresion-lineal/index.html#ajuste-de-los-par谩metros-minimizando-la-funci贸n-de-costo",
    "title": "La Regresi贸n Lineal en Machine Learning",
    "section": "Ajuste de los par谩metros minimizando la funci贸n de costo",
    "text": "Ajuste de los par谩metros minimizando la funci贸n de costo\nHay 3 maneras principales de minimizar la funci贸n de costo de m铆nimos cuadrados ordinarios (MCO) \\(J\\). El m茅todo cl谩sico, utiliza c谩lculo diferencial y el criterio de las derivadas para minimizar. La segunda, utiliza el m茅todo de la m谩xima verosimilitud, un concepto estad铆stico. Se encuentra matem谩ticamente que ambos m茅todos llegan a la misma forma soluci贸n. La tercera, usa el m茅todo m谩s utilizado en machine learning, el del gradiente descendiente (gradient descent).\nMi objetivo de este post no es hacer demostraciones matem谩ticas ni c谩lculos completos, sino que ilustrar la metodolog铆a, as铆 que nos enfocaremos en el m茅todo cl谩sico y el de gradiente descendiente. Veamos a m谩s detalle\n\nEl m茅todo de m铆nimos cuadrados cl谩sico\nPara minimizar la funci贸n de costo, los estimados de los par谩metros \\(w_0\\), \\(w_1\\), que son \\(\\hat{w_0}\\), \\(\\hat{w_1}\\) respectivamente, deben satisfacer\n\\[ \\frac{\\partial J}{\\partial w_0} \\bigg|_{\\hat{w_0}, \\hat{w_1}}= - \\frac{2}{m} \\sum_{i=1}^{m} (y_i - \\hat{w_0} - \\hat{w_1} x_i) = 0\\] y\n\\[ \\frac{\\partial J}{\\partial w_1} \\bigg|_{\\hat{w_0}, \\hat{w_1}} = - \\frac{2}{m} \\sum_{i=1}^{m} (y_i - \\hat{w_0} - \\hat{w_1} x_i) x_i = 0\\]\nde acuerdo al criterio de la primer derivada, 茅sta debe ser 0 para que sea un m铆nimo.\nSimplificando las expresiones, separando las sumatorias y resolviendo para \\(\\hat{w_0}\\), \\(\\hat{w_1}\\), se tienen las soluciones a los par谩metros estimados que minimizan la funci贸n de MCO\n\\[ \\hat{w_0} =  \\bar{y} - \\hat{w_1} \\bar{x}\\] y\n\\[ \\hat{w_1} = \\frac{ \\sum_{i=1}^{m} y_i x_i - \\frac{(\\sum_{i=1}^{m} y_i)(\\sum_{i=1}^{m} x_i)}{n} }{ \\sum_{i=1}^{m} x_i^2 - \\frac{(\\sum_{i=1}^{m}x_i)^2}{m} } \\]\ndonde \\(\\bar{x} = \\frac{1}{m}  \\sum_{i=1}^{m} x_i\\) y \\(\\bar{y} = \\frac{1}{m}  \\sum_{i=1}^{m} y_i\\) son los promedios de \\(x_i\\), \\(y_i\\), respectivamente.\nPara el caso multivariable, se tiene una soluci贸n de los par谩metros estimados \\(W^{*}\\) en forma matricial, dado por\n\\[ W^{*} = (X^T X)^{-1} X^{T} Y \\] Donde \\(X\\) es la matriz de dise帽o, la cual es de tama帽o \\(m \\times (n + 1)\\). Donde \\(m\\) es el n煤mero de datos y \\(n\\) el n煤mero de variables regresoras. Se construye poniendo en filas los vectores de datos \\(x_j = (1, x_{j1}, x_{j2}, ..., x_{jn})\\), donde \\(j = 1, 2, 3, ..., m\\), en todos los datos. Por otro lado, \\(Y\\) es la matriz (o vector) de tama帽o \\(m \\times 1\\), que se construye poniendo todos los puntos \\(y\\) de donde se har谩 la regresi贸n.\nAs铆,\n\\[ X = \\begin{pmatrix}\n1 & x_{11} & x_{12} & ... & x_{1n} \\\\\n1 & x_{21} & x_{22} & ... & x_{2n} \\\\\n&  & \\vdots &  &  \\\\\n1 & x_{m1} & x_{m2} & ... & x_{mn}\n\\end{pmatrix}  \\]\n\\[ Y = \\begin{pmatrix}\ny_1 \\\\\ny_2 \\\\\ny_3 \\\\\n\\vdots \\\\\ny_m\n\\end{pmatrix}  \\]\n\n\nEl m茅todo del gradiente descendiente\nRetomando conceptos de c谩lculo diferencial para una variable, la derivada de una funci贸n \\(f\\) en un punto dado \\(x_0\\), nos da la pendiende de la recta tangente a la curva en ese punto. En otras palabras, esa pendiente puede interprestarse como la raz贸n de cambio de la funci贸n en ese punto y el signo indica hacia donde est谩 cambiando. Extendiendo este concepto hacia varias variables, el gradiente de una funci贸n \\(f\\), denotado como \\(\\nabla f\\), es un vector que indica hacia d贸nde la funci贸n tiene el mayor incremento, y su magnitud es la raz贸n de cambio.\nEl m茅todo del gradiente descendiente, las derivadas son respecto a los par谩metros \\(w\\) que queremos ajustar, y como su nombre lo indica, hace uso del gradiente para minimizar la funci贸n de costo \\(J(w)\\). Para el caso de la regresi贸n lineal, el algoritmo se define de la siguiente manera\n\\[ w_i^{(\\tau + 1)} := w_i^{(\\tau)} - \\alpha \\frac{\\partial J(w)}{\\partial w_i}\\] donde \\(J(w)\\) esl funci贸n de costo de m铆nimos cuadrados ordinarios. La derivada est谩 multiplicada por un \\(-1\\), lo que asegura que se mueva en la direcci贸n de menor crecimiento, tenemos que \\(\\alpha\\) es una constante llamada tasa de aprendizaje, controla qu茅 tanto se ajusta el valor de \\(w_i\\). En machine learning, \\(\\alpha\\) suele tomar valores muy peque帽os, como \\(0.01, 0.001,\\) etc. Por otro lado, \\(\\tau\\) es un n煤mero entero que indica en la iteraci贸n que estamos del algoritmo.\nAs铆, se estar谩n ajustando los par谩metros \\(w_i\\) de manera iterativa hasta que se cumpla un cierto criterio o se alcance una tolerancia del error. Cabe se帽alar que en la primera iteraci贸n, los par谩metros se inicializan de manera aleatoria.\nPor lo tanto, para el caso de regresi贸n lineal simple, si utilizamos el m茅todo del gradiente descendiente tendriamos lo siguiente\n\\[ w_0^{(\\tau + 1)} := w_0^{(\\tau)} + \\alpha \\frac{2}{m} \\sum_{i=1}^{m} (y_i - \\hat{y_i})\\] \\[ w_1^{(\\tau + 1)} := w_1^{(\\tau)} + \\alpha \\frac{2}{m} \\sum_{i=1}^{m} (y_i - \\hat{y_i}) x_i\\] Donde \\(\\hat{y_i}\\) son las predicciones en la iteraci贸n \\(\\tau\\), o sea antes de hacer la actualizaci贸n de los par谩metros.\n\n\n驴Cu谩l usar de los 2 m茅todos?\nLa respuesta r谩dica en la complejidad computacional y el tiempo que toman los c谩lculos. En machine learning se trabajan con datos demasiado grandes, m谩s de lo que se sol铆a hacer en la estad铆stica convencional, por lo tanto, la forma matricial de obtener los par谩metros puede llegar a ser muy costosa computacionalmente hablando si se tienen muchos datos y muchas caracter铆sticas (variables regresoras). As铆, es m谩s conveniente utilizar el m茅todo del gradiente descendiente, que en la pr谩ctica suele ser m谩s eficiente."
  },
  {
    "objectID": "posts/el-poder-de-la-regresion-lineal/index.html#m茅todos-de-regularizaci贸n",
    "href": "posts/el-poder-de-la-regresion-lineal/index.html#m茅todos-de-regularizaci贸n",
    "title": "La Regresi贸n Lineal en Machine Learning",
    "section": "M茅todos de regularizaci贸n",
    "text": "M茅todos de regularizaci贸n\nLa regularizaci贸n en regresi贸n lineal, es para reducir la complejidad del modelo evitando as铆 un sobreajuste. Esto se logra a帽adiendo un t茅rmino a la funci贸n de costo de MCO, lo que acota los par谩metros evitando que crezcan demasiado. Hay 2 casos, Ridge y Lasso, como se ver谩 a continuaci贸n.\n\nRegresi贸n Lineal Ridge: limitando el tama帽o de los par谩metros\nLa regresi贸n ridge hace que los par谩metros \\(w_i\\) se encojan, es decir, que no tomen valores tan grandes. Se define con la siguiente funci贸n de costo \\(J_R(w)\\)\n\\[ J_R(W) = \\frac{1}{m} \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{n} w_j^2 \\] Sumando as铆 el t茅rmino \\(\\lambda \\sum_{j=1}^{n} w_j^2\\) de penalizaci贸n. Donde \\(\\lambda\\) controla que tanto se encojen los par谩metros, entre m谩s grande es el valor de \\(\\lambda\\) est谩n m谩s l铆mitados los par谩metros. Se tiene que cuando \\(\\lambda \\xrightarrow{}{} \\infty\\), los par谩metros \\(w_i\\) tienden a \\(0\\).\nNotemos que el par谩metro \\(w_0\\) no se incluye, ya que solo se toma en cuenta \\(w_1, w_2, ..., w_n\\). Eso es porque \\(w_0\\), como ya vimos, es el valor promedio de \\(y\\) cuando \\(x_1=x_2=...=x_n=0\\). No ser铆a conveniente imponer restricciones o encoger dicho par谩metro entonces.\nA la regresi贸n ridge tambi茅n se le conoce como regularizaci贸n L2, debido a que el t茅rmino \\(\\sum_{j=1}^{n} w_j^2\\) es la norma L2 del vector de par谩metros \\(w = (w_1, w_2, ..., w_n)\\).\n\n\nRegresi贸n Lineal Lasso: un selector de caracter铆sticas\nLa regresi贸n lasso act煤a como un selector de caracter铆sticas, ya que impone que algunos de los par谩metros \\(w_i\\) sean igual a \\(0\\). Se define con la siguiente funci贸n de costo \\(J_l(w)\\)\n\\[ J_l(W) = \\frac{1}{m} \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{n} |w_j| \\] Sumando as铆 el t茅rmino \\(\\lambda \\sum_{j=1}^{n} |w_j|\\) de penalizaci贸n. Similarmente, lasso tambi茅n hace que los par谩metros se encojan hac铆a 0, pero a diferencia de ridge, si \\(\\lambda\\) es suficientemente grande algunos par谩metros \\(w_i\\) van a ser \\(0\\). As铆, la variable regresora (o caracter铆stica) \\(x_i\\) asociado al par谩metro \\(w_i\\) no estar谩 involucrada en la ecuaci贸n de regresi贸n, por esta raz贸n lasso es un selector de caracter铆sticas.\nIgual que en el caso de regresi贸n ridge, el par谩metro \\(w_0\\) no se incluye, ya que no ser铆a conveniente imponer restricciones o hacer 0 el par谩metro \\(w_0\\), siendo que es es el valor promedio de \\(y\\) cuando \\(x_1=x_2=...=x_n=0\\).\nA la regresi贸n lasso tambi茅n se le conoce como regularizaci贸n L1, debido a que el t茅rmino \\(\\sum_{j=1}^{n} |w_j|\\) es la norma L1 del vector de par谩metros \\(w = (w_1, w_2, ..., w_n)\\).\n\n\nInterpretaci贸n geom茅trica de ridge y lasso\nSurge la pregunta, 驴Por qu茅 la regresi贸n ridge encoje los coeficientes pero la regresi贸n lasso fuerza algunos de ellos a ser \\(0\\)? Para contestar esto, encontr茅 que la mejor manera es gr谩ficamente.\nLa funci贸n de costo de m铆nimos cuadrados ordinarios MCO, es una funci贸n convexa, por lo que es posible minimizarla. Ahora bien, si la graficamos en el espacio de los par谩metros al minimizar se pueden probar todos las combinaciones de par谩metros, sin embargo, al a帽adir un t茅rmino de penalizaci贸n como en ridge y lasso, se hace una constricci贸n en el espacio en el que puede bucarse el m铆nimo. Veamos la siguiente figura (extra铆da del libro Introduction to Statistical Learning)\n\n\n\nCurvas de nivel de la funci贸n de costo de MCO en rojo y las constricciones de ridge y lasso en azul. Graficadas en el espacio de 2 par谩metros \\(w_1\\) y \\(w_2\\). El m铆nimo se encuentra en \\(\\hat{w}\\).\n\n\nEn este caso, es una regresi贸n en 2 dimensiones con \\(x_1\\), \\(x_2\\) y las gr谩ficas est谩n hechas en el espacio de los par谩metros \\(w_1\\), \\(w_2\\). Las l铆neas rojas son las curvas de nivel de la funci贸n de costo de MCO, y las 谩reas s贸lidas azules son las regiones de constricciones impuestas en los par谩metros \\(w_1\\), \\(w_2\\) por los t茅rminos de penalizaci贸n de lasso y ridge, respectivamente.\nPara el caso de ridge, tenemos la constricci贸n el c铆rculo \\(w_1^2 + w_2^2 \\le s\\), y para lasso el rombo \\(|w_1| + |w_2| \\le s\\). El punto de minimizaci贸n, donde est谩n los mejores par谩metros estimados, es la intersecci贸n m谩s pronta entre la curva de nivel de MCO y el 谩rea de constricci贸n. Podemos observar claramente que para lasso, este punto de intersecci贸n m谩s pronto se encuentra en la punta del rombo, es decir, sobre un eje donde el par谩metro \\(w_2 = 0\\); mientras que para ridge, el punto se encuentra sobre la circunferencia fuera de los ejes, por los que los par谩metros estar谩n encojidos por la constricci贸n pero no ser谩n 0. Esto se extiende a cualquier dimensi贸n, para \\(n \\gt 2\\), el rombo se convierte en un politopo mientras que el c铆rculo en una hiperesfera. Entonces para el politopo siempre habr谩 esquinas donde los par谩metros sean 0."
  },
  {
    "objectID": "posts/el-poder-de-la-regresion-lineal/index.html#conclusiones",
    "href": "posts/el-poder-de-la-regresion-lineal/index.html#conclusiones",
    "title": "La Regresi贸n Lineal en Machine Learning",
    "section": "Conclusiones",
    "text": "Conclusiones\nEn este post, se vi贸 c贸mo se define la regresi贸n lineal en una y varias variables, y los supuestos que se deben cumplir para su utilizaci贸n. Se explic贸 c贸mo minimizar su funci贸n de costo de m铆nimos cuadrados ordinarios en la manera cl谩sica y con el m茅todo de gradiente descendiente. Tambi茅n, se vieron los m茅todos de regularizaci贸n de regresi贸n ridge y lasso, y c贸mo influyen en los par谩metros encoji茅ndolos o haciendo algunos de ellos \\(0\\), respectivamente.\nComo trabajo futuro, queda llevar este conocimiento a implementaci贸n en c贸digo, para ver su funcionalidad pr谩ctica. Existen librer铆as para hacer regresi贸n lineal, ridge y lasso; en Python est谩 scikit-learn; mientras que en R tenemos stats y glmnet.\nRecomiendo ampliamente revisar las referencias usadas para este post, hay conocimiento muy bueno en ellas."
  },
  {
    "objectID": "posts/el-poder-de-la-regresion-lineal/index.html#supuestos-de-la-regresi贸n-lineal",
    "href": "posts/el-poder-de-la-regresion-lineal/index.html#supuestos-de-la-regresi贸n-lineal",
    "title": "La Regresi贸n Lineal en Machine Learning",
    "section": "Supuestos de la regresi贸n lineal",
    "text": "Supuestos de la regresi贸n lineal\nEs importante saber cu谩ndo es prudente utilizar un modelo de regresi贸n lineal. Para que se cumplan las condiciones de la regresi贸n lineal simple y m煤ltiple, se tienen los siguientes supuestos necesarios\n\nLinealidad: La relaci贸n entre las variables regresoras \\(x\\) y la variable repuesta \\(y\\) debe ser lineal. Notemos que en la regresi贸n polinomial esto va m谩s all谩, necesitando solo linealidad en los par谩metros.\nHomocedasticidad: Significa que la varianza de los errores es constante.\nNormalidad de los errores: Los residulales, es decir los errores, deben seguir una distribuci贸n normal con media 0 y varianza \\(\\sigma^2\\).\nErrores independientes (no autocorrelaci贸n): Los errores no deben estar correlacionados entre ellos, es otras palabras, deben ser independientes e id茅nticamente distribuidos.\nNo multicolinealidad: No existe correlaci贸n entre las variables regresoras.\nNo exogeneidad: Las variables regresoras y los errores no est谩n correlacionados."
  },
  {
    "objectID": "posts/el-poder-de-la-regresion-lineal/index.html",
    "href": "posts/el-poder-de-la-regresion-lineal/index.html",
    "title": "La Regresi贸n Lineal en Machine Learning",
    "section": "",
    "text": "La regresi贸n lineal puede parecer algo muy simple, pero en realidad tiene un alcance muy grande para modelar datos. Puede verse desde una perspectiva meramente geom茅trica, sin embargo, tiene unos supuestos estad铆sticos que son sumamente importantes que se cumplan si queremos que el modelo funcione y sea aplicable.\nEn este post, quiero enfocarme en la regresi贸n lineal desde la perspectiva del machine learning, por lo que asume un conocimiento b谩sico previo de 茅ste. Cubrir谩 los siguientes puntos:\n\nQu茅 es la regresi贸n lineal en una y varias variables\nQu茅 significa que la regresi贸n lineal se puede aplicar mientras se tenga linealidad en los par谩metros\nSupuestos para la validez de la regresi贸n lineal\nC贸mo se define la funci贸n de costo para la regresi贸n lineal\nDiferencia entre la soluci贸n cerrada para minimizar la funci贸n de costo y el m茅todo del gradiente descendiente\nM茅todos de regularizaci贸n para regresi贸n lineal y su interpretaci贸n geom茅trica\n\nSin m谩s por comentar en la introducci贸n, comencemos con el post."
  },
  {
    "objectID": "posts/el-poder-de-la-regresion-lineal/index.html#introducci贸n",
    "href": "posts/el-poder-de-la-regresion-lineal/index.html#introducci贸n",
    "title": "La Regresi贸n Lineal en Machine Learning",
    "section": "",
    "text": "La regresi贸n lineal puede parecer algo muy simple, pero en realidad tiene un alcance muy grande para modelar datos. Puede verse desde una perspectiva meramente geom茅trica, sin embargo, tiene unos supuestos estad铆sticos que son sumamente importantes que se cumplan si queremos que el modelo funcione y sea aplicable.\nEn este post, quiero enfocarme en la regresi贸n lineal desde la perspectiva del machine learning, por lo que asume un conocimiento b谩sico previo de 茅ste. Cubrir谩 los siguientes puntos:\n\nQu茅 es la regresi贸n lineal en una y varias variables\nQu茅 significa que la regresi贸n lineal se puede aplicar mientras se tenga linealidad en los par谩metros\nSupuestos para la validez de la regresi贸n lineal\nC贸mo se define la funci贸n de costo para la regresi贸n lineal\nDiferencia entre la soluci贸n cerrada para minimizar la funci贸n de costo y el m茅todo del gradiente descendiente\nM茅todos de regularizaci贸n para regresi贸n lineal y su interpretaci贸n geom茅trica\n\nSin m谩s por comentar en la introducci贸n, comencemos con el post."
  },
  {
    "objectID": "posts/el-poder-de-la-regresion-lineal/index.html#referencias",
    "href": "posts/el-poder-de-la-regresion-lineal/index.html#referencias",
    "title": "La Regresi贸n Lineal en Machine Learning",
    "section": "Referencias",
    "text": "Referencias\n\nDouglas C. Montgomery, Elizabeth A. Peck, G. Geoffrey Vining (2021). Introduction to Linear Regression Analysis. Sixth Edition.\nGareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani (2023). An Introduction to Statistical Learning with Applications in R. 2nd Edition.\nYunpeng Tai (2021). A Survey Of Regression Algorithms And Connections With Deep Learning.\nMukul Ranjan (2022). How does Lasso regression(L1) encourage zero coefficients but not the L2?. Medium. Post link\nTerence Parr. A visual explanation for regularization of linear models. Explained.ai. Post link\nUser: QuantStyle. What is the computational cost of gradient descent vs linear regression?. StackExchange. Post link"
  },
  {
    "objectID": "posts/la-regresion-lneal-en-machine-learning/index.html",
    "href": "posts/la-regresion-lneal-en-machine-learning/index.html",
    "title": "La Regresi贸n Lineal en Machine Learning",
    "section": "",
    "text": "La regresi贸n lineal puede parecer algo muy simple, pero en realidad tiene un alcance muy grande para modelar datos. Puede verse desde una perspectiva meramente geom茅trica, sin embargo, tiene unos supuestos estad铆sticos que son sumamente importantes que se cumplan si queremos que el modelo funcione y sea aplicable.\nEn este post, quiero enfocarme en la regresi贸n lineal desde la perspectiva del machine learning, por lo que asume un conocimiento b谩sico previo de 茅ste. Cubrir谩 los siguientes puntos:\n\nQu茅 es la regresi贸n lineal en una y varias variables\nQu茅 significa que la regresi贸n lineal se puede aplicar mientras se tenga linealidad en los par谩metros\nSupuestos para la validez de la regresi贸n lineal\nC贸mo se define la funci贸n de costo para la regresi贸n lineal\nDiferencia entre la soluci贸n cerrada para minimizar la funci贸n de costo y el m茅todo del gradiente descendiente\nM茅todos de regularizaci贸n para regresi贸n lineal y su interpretaci贸n geom茅trica\n\nSin m谩s por comentar en la introducci贸n, comencemos con el post."
  },
  {
    "objectID": "posts/la-regresion-lneal-en-machine-learning/index.html#introducci贸n",
    "href": "posts/la-regresion-lneal-en-machine-learning/index.html#introducci贸n",
    "title": "La Regresi贸n Lineal en Machine Learning",
    "section": "",
    "text": "La regresi贸n lineal puede parecer algo muy simple, pero en realidad tiene un alcance muy grande para modelar datos. Puede verse desde una perspectiva meramente geom茅trica, sin embargo, tiene unos supuestos estad铆sticos que son sumamente importantes que se cumplan si queremos que el modelo funcione y sea aplicable.\nEn este post, quiero enfocarme en la regresi贸n lineal desde la perspectiva del machine learning, por lo que asume un conocimiento b谩sico previo de 茅ste. Cubrir谩 los siguientes puntos:\n\nQu茅 es la regresi贸n lineal en una y varias variables\nQu茅 significa que la regresi贸n lineal se puede aplicar mientras se tenga linealidad en los par谩metros\nSupuestos para la validez de la regresi贸n lineal\nC贸mo se define la funci贸n de costo para la regresi贸n lineal\nDiferencia entre la soluci贸n cerrada para minimizar la funci贸n de costo y el m茅todo del gradiente descendiente\nM茅todos de regularizaci贸n para regresi贸n lineal y su interpretaci贸n geom茅trica\n\nSin m谩s por comentar en la introducci贸n, comencemos con el post."
  },
  {
    "objectID": "posts/la-regresion-lneal-en-machine-learning/index.html#regresi贸n",
    "href": "posts/la-regresion-lneal-en-machine-learning/index.html#regresi贸n",
    "title": "La Regresi贸n Lineal en Machine Learning",
    "section": "Regresi贸n",
    "text": "Regresi贸n\nLa regresi贸n es una t茅cnica de modelaci贸n proveniente de la estad铆stica para modelar y analizar la relaci贸n entre variables. Se tiene una variable \\(y\\) llamada independiente, la cual se modela en funci贸n de otra variable \\(x\\) llamada independiente, que bien se puede tener una \\(n\\) cantidad de variables \\(x_1, x_2, ... , x_n\\) como se ver谩 m谩s adelante.\nEs importante mencionar que, para evitar confusiones con la noci贸n de indepencia de probabilidad, se prefiere nombrar a \\(y\\) como la variable respuesta y a \\(x\\) como las variables predictoras. En el contexto de machine learning, a las variables \\(x\\) se les denomina caracter铆sticas (features) y a \\(y\\) etiquetas (labels).\nEl tipo de datos con los que la regresi贸n trabaja son num茅ricos (flotantes), y vienen en duplas \\((x_1, y_1), ... , (x_m, y_m)\\), donde \\(m\\) es la cantidad de datos que tengamos, es decir, cardinalidad del conjunto de datos. N贸tese que se pueden tener m煤ltiples caracter铆sticas \\(x\\).\nPara trabajar con regresi贸n, si la dimensi贸n lo permite, suele hacerse un diagrama de dispersi贸n que es una gr谩fica de los datos representados como puntos. Por ejemplo, consideremos de \\(x\\) e \\(y\\), con 15 registros (\\(m = 15\\)), resultando el siguiente diagrama de dispersi贸n\n\n\n\n\n\n\n\n\n\nEn este caso, se ve claramente que los puntos tienen una tendencia lineal, esto es, que se les puede ajustar una recta para modelarlos con un error considerablemente bajo. La ecuaci贸n de la recta, es \\(y = ax + b\\), con \\(a\\) la pendiente y \\(b\\) la intersecci贸n en el eje \\(y\\). Dicha ecuaci贸n discribir铆a adecuadamente ese comportamiento, en las siguientes secciones veremos c贸mo ajustarla a los datos para encontrar los valores adecuados de \\(a\\) y \\(b\\).\nEn este diagrama en part铆cular, los puntos fueron simulados de una recta \\(y = 2x\\), a la que se les agreg贸 un error \\(\\varepsilon\\) que tiene una distribuci贸n normal con media \\(\\mu = 0\\) y desviaci贸n estandar \\(\\sigma = 1.5\\). Teniendo as铆 los par谩metros de la recta \\(a = 2\\) y \\(b = 0\\)\n\nObtenci贸n de datos para regresi贸n\nHay 3 maneras principales para obtener datos que puedan usarse en regresi贸n, los cuales se listan a continuaci贸n\n\nEstudio retrospectivo: Aqu铆 se tienen datos que ya fueron capturados en el pasado, y no necesariamente con una intenci贸n de ser analizados cient铆ficamente, por lo que pueden no estar en la forma m谩s ideal posible. Los datos y sus resultados ya se tienen, no se puede intervenir ni cambiar nada para mejorar la toma de datos. Este tipo de estudio es muy com煤n en machine learning y big data, donde obtienes datos con ciertas caracter铆sticas y ya etiquetados que fueron obtenidos en el pasado.\nEstudio observacional: Como su nombre puede indicarlo, en este tipo de estudio solamente se observa el proceso o experimento que quiere analizarse, interviniendo m铆nimamente solamente para la captura de los datos. Por temas 茅ticos, este estudio es muy usado en el 谩rea m茅dica, pues de lo contrario se estar铆a imponiendo sobre las personas las condiciones que deben tener (como empezar a fumar, por ejemplo).\nDise帽o experimental: Este tipo de estudio posee m谩s estrategia. Aqu铆 se manipulan las variables predictoras (o caracter铆sticas) de acuerdo a un dise帽o experimental, para tener ciertos valores y observar su efecto en la variable respuesta."
  },
  {
    "objectID": "posts/la-regresion-lneal-en-machine-learning/index.html#regresi贸n-lineal-simple",
    "href": "posts/la-regresion-lneal-en-machine-learning/index.html#regresi贸n-lineal-simple",
    "title": "La Regresi贸n Lineal en Machine Learning",
    "section": "Regresi贸n Lineal Simple",
    "text": "Regresi贸n Lineal Simple\nLa regresi贸n lineal simple es 煤til cuando existe una tendencia lineal en los datos. Aunque veremos m谩s adelante que va m谩s all谩 de lo lineal. Se le llama simple porque solo hay una caracter铆stica \\(x\\), y se modela como\n\\[y = w_0 + w_1 x + \\varepsilon\\] La cual puede verse como la ecuaci贸n de regresi贸n lineal poblacional, no obstante, en regresi贸n lineal trabajamos con \\(m\\) n煤mero de datos, entonces para cada punto se tendr谩 una ecuaci贸n de regresi贸n lineal muestral\n\\[y_i = w_0 + w_1 x_i + \\varepsilon_i \\quad i=1, 2, 3, ..., m\\]\nCompar谩ndo con la ecuaci贸n de la recta, podemos interpretar los par谩metros. Se tiene que \\(w_1\\) es el cambio en la media de la distribuci贸n de \\(y\\) producido por un cambio de unidad en \\(x\\). Mientras que \\(w_0\\) simplemente es la media de la distribuci贸n de \\(y\\) cuando \\(x = 0\\), si el dominio de \\(x\\) no incluye a \\(0\\) entonces el par谩metro \\(w_0\\) no tiene interpretaci贸n pr谩ctica.\nPor otro lado, \\(\\varepsilon\\) son los errores o ruido que se tiene en cada predicci贸n de \\(y\\). Se asumen que dichos errores tienen una distribuci贸n normal con media igual a cero \\(\\mu = 0\\) y una varianza desconicida \\(\\sigma^2\\). Es importante notar que, esa varianza es la misma en todos los puntos de la regresi贸n, lo que se conoce como homocedasticidad. A los errores \\(\\varepsilon\\) se les llama residuales pues se definen como \\(y - \\hat{y}\\).\nAhora bien, retomando el diagrama de dispersi贸n que se vio anteriormente, el cual tiene proviene de la recta \\(y = 2x\\) con un error \\(\\varepsilon\\) con media \\(0\\) y varianza \\(\\sigma^2 = 1.5^2\\). Se le puede ajustar una recta de regresi贸n lineal y se ver铆a como la siguiente figura\n\n\n\n\n\n\n\n\n\nObteniendo los coeficientes estimados \\(\\hat{w_0} = 0.3045\\) y \\(\\hat{w_1} = 2.0274\\). Teniendo as铆 una ecuaci贸n de regresi贸n lineal con la que podemos hacer predicciones\n\\[ \\hat{y} = 0.3045 + 2.0274 x\\] Notemos que los par谩metros reales de donde provienen los datos son \\(0\\) y \\(2\\), pero por la naturaleza estoc谩stica de los errores, los estimados no coinciden con exactitud. Entre mayor sea el n煤mero de datos \\(m\\), los par谩metros estimados ser谩n m谩s cercanos a los originales."
  },
  {
    "objectID": "posts/la-regresion-lneal-en-machine-learning/index.html#regresi贸n-lineal-m煤ltiple",
    "href": "posts/la-regresion-lneal-en-machine-learning/index.html#regresi贸n-lineal-m煤ltiple",
    "title": "La Regresi贸n Lineal en Machine Learning",
    "section": "Regresi贸n Lineal M煤ltiple",
    "text": "Regresi贸n Lineal M煤ltiple\nPara el caso en que la variable respuesta \\(y\\) est谩 realcionada con \\(n\\) variables regresoras, se tiene el modelo de la regresi贸n lineal m煤ltiple\n\\[ y = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n + \\varepsilon \\] As铆, a los par谩metros \\(w_j \\quad j = 0, 1, 2, ..., n\\) se les llama coeficientes de regresi贸n. Este modelo, al ser de multivariable, describe un hiperplano en el espacio \\(n\\)-dimensional de las variables regresoras \\(x\\).\nHaciendo sentido de los par谩metros. El par谩metro \\(w_j\\) representa el cambio esperado en la respuesta \\(y\\) por unidad de cambio en \\(x_j\\), cuando todo las dem谩s variables regresoras \\(x_i(i\\ne j)\\) se mantienen constantes.Por otra parte, el par谩metro \\(w_0\\) es la intersecci贸n del hiperplano en \\(y\\), si el dominio de los datos incluye \\(x_1 = x_2 = ... = x_n = 0\\) entonces \\(w_0\\) es la media de \\(y\\) cuando \\(x_1 = x_2 = ... = x_n = 0\\); de lo contrario, \\(w_0\\) no tiene interpretaci贸n f铆sica.\nConcluyendo, en el modelo de regresi贸n simple y m煤ltiple, la linealidad se tiene en los par谩metros. Esto extiende m谩s all谩 el concepto de linealidad aunque las variables regresoras \\(x\\) no sean lineales, esto se ver谩 a continuaci贸n en la secci贸n de regresi贸n polinomial."
  },
  {
    "objectID": "posts/la-regresion-lneal-en-machine-learning/index.html#regresi贸n-polinomial",
    "href": "posts/la-regresion-lneal-en-machine-learning/index.html#regresi贸n-polinomial",
    "title": "La Regresi贸n Lineal en Machine Learning",
    "section": "Regresi贸n Polinomial",
    "text": "Regresi贸n Polinomial\nYa vimos que una regresi贸n lineal de \\(n\\) variables regresoras genera un hiperplano \\(n\\)-dimensional. Ahora bien, cualquier modelo que es lineal en los par谩metros \\(w\\)s es un modelo de regresi贸n lineal, independientemente de la forma de la superficie que genere.\nComo su nombre lo indica, la regresi贸n polinomial de una variable \\(x\\) se define a partir de los polinomios, y se modela de la siguiente forma\n\\[ y = w_0 + w_1x + w_2x^2 + ... + w_nx^n + \\varepsilon\\] La cual es lineal en los par谩metros, por lo que se puede trabajar como una regresi贸n lineal m煤ltiple y usar la misma metodolog铆a. Para hacer esto, podemos definir \\(x_j = x^j \\quad j=1, 2, 3, ... , n\\). Por ejemplo, para el caso de un polinomio de grado 2 con \\(y = w_0 + w_1x + w_2x^2 + \\varepsilon\\), tendr铆amos \\(x_1 = x, x_2 = x^2\\) y la regresi贸n se convertir铆a en una lineal m煤ltiple\n\\[ y = w_0 + w_1x_1 + w_2x_2 + \\varepsilon\\] Con este tipo de regresi贸n, hay que ser muy cuidadosos. Ya que un polinomio de grado considerablemente alto puede apr贸ximar muy bien una curva no lineal, sin embargo, hay que tratar de hacer transformaciones y mantener el grado lo m谩s bajo posible para evitar un sobreajuste del modelo (empezar a generalizar el error mismo)."
  },
  {
    "objectID": "posts/la-regresion-lneal-en-machine-learning/index.html#supuestos-de-la-regresi贸n-lineal",
    "href": "posts/la-regresion-lneal-en-machine-learning/index.html#supuestos-de-la-regresi贸n-lineal",
    "title": "La Regresi贸n Lineal en Machine Learning",
    "section": "Supuestos de la regresi贸n lineal",
    "text": "Supuestos de la regresi贸n lineal\nEs importante saber cu谩ndo es prudente utilizar un modelo de regresi贸n lineal. Para que se cumplan las condiciones de la regresi贸n lineal simple y m煤ltiple, se tienen los siguientes supuestos necesarios\n\nLinealidad: La relaci贸n entre las variables regresoras \\(x\\) y la variable repuesta \\(y\\) debe ser lineal. Notemos que en la regresi贸n polinomial esto va m谩s all谩, necesitando solo linealidad en los par谩metros.\nHomocedasticidad: Significa que la varianza de los errores es constante.\nNormalidad de los errores: Los residulales, es decir los errores, deben seguir una distribuci贸n normal con media 0 y varianza \\(\\sigma^2\\).\nErrores independientes (no autocorrelaci贸n): Los errores no deben estar correlacionados entre ellos, es otras palabras, deben ser independientes e id茅nticamente distribuidos.\nNo multicolinealidad: No existe correlaci贸n entre las variables regresoras.\nNo exogeneidad: Las variables regresoras y los errores no est谩n correlacionados."
  },
  {
    "objectID": "posts/la-regresion-lneal-en-machine-learning/index.html#m铆nimos-cuadrados-ordinarios-como-funci贸n-de-costo",
    "href": "posts/la-regresion-lneal-en-machine-learning/index.html#m铆nimos-cuadrados-ordinarios-como-funci贸n-de-costo",
    "title": "La Regresi贸n Lineal en Machine Learning",
    "section": "M铆nimos cuadrados ordinarios como funci贸n de costo",
    "text": "M铆nimos cuadrados ordinarios como funci贸n de costo\nEn machine learning, la base para ajustar los par谩metros a nuestros datos, lo que se conoce como aprender, radica en la optimizaci贸n de una funci贸n objetivo. Dicho de otra manera, es minimizar una funci贸n de costo, donde dicha funci贸n ser铆a una medida del error en las predicciones del modelo.\nEn la regresi贸n lineal no es la excepci贸n. Es muy conocido el m茅todo de m铆nimos cuadrados para ajustar los par谩metros, donde se define la funci贸n de costo \\(J\\) de m铆nimos cuadrados ordinarios (MCO) de la siguiente manera\n\\[ J(y, \\hat{y}) = \\frac{1}{m} \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2 \\] Donde el factor \\(\\frac{1}{m}\\) se multiplica como una especie de promedio en todos los datos. Algunas referencias tambi茅n manejan multiplicarlo por \\(\\frac{1}{2}\\) por conveniencia algebraica.\nNotemos que la diferencia \\(y_i - \\hat{y}_i\\) es la distancia que hay desde un punto \\(\\hat{y} (x_i)\\) en la recta de la predicci贸n \\(\\hat{y}\\) hasta el correspondiente punto \\(y(x_i)\\) en la recta original \\(y\\) de donde provienen los datos. Est谩 al cuadrado para evitar que las sumas de esos t茅rminos puedan dar cero. As铆, podemos interpretarlo gr谩ficamente como unos cuadrados en dichos puntos \\(x_i\\), considerando el mismo diagrama de dispersi贸n de este post, se ver铆a\n\n\n\n\n\n\n\n\n\nN贸tese que los ejes fueron escalados para una mejor visualizaci贸n. Ahora, al ajustar los par谩metros \\(w\\) minimizando la funci贸n de costo de MCO, lo que se est谩 haciendo gr谩ficamente es ajustar la recta de predicci贸n de tal manera que se minimize el 谩rea de esos cuadrados; de ah铆 el nombre de este m茅todo.\nPara minimizar dicha funci贸n, es mejor expresarla en t茅rmino de los par谩metros \\(w\\), para esto, en el caso de una variable regresora sustituimos \\(\\hat{y} = w_0 + w_1 x\\) en la funci贸n de costo de MCO, as铆\n\\[ J(w_0, w_1) = \\frac{1}{m} \\sum_{i=1}^{m} (y_i - w_0 - w_1 x_i)^2 \\] En la siguiente secci贸n veremos c贸mo minimizarla, ejemplificando el caso con una sola variable regresora \\(x\\)."
  },
  {
    "objectID": "posts/la-regresion-lneal-en-machine-learning/index.html#ajuste-de-los-par谩metros-minimizando-la-funci贸n-de-costo",
    "href": "posts/la-regresion-lneal-en-machine-learning/index.html#ajuste-de-los-par谩metros-minimizando-la-funci贸n-de-costo",
    "title": "La Regresi贸n Lineal en Machine Learning",
    "section": "Ajuste de los par谩metros minimizando la funci贸n de costo",
    "text": "Ajuste de los par谩metros minimizando la funci贸n de costo\nHay 3 maneras principales de minimizar la funci贸n de costo de m铆nimos cuadrados ordinarios (MCO) \\(J\\). El m茅todo cl谩sico, utiliza c谩lculo diferencial y el criterio de las derivadas para minimizar. La segunda, utiliza el m茅todo de la m谩xima verosimilitud, un concepto estad铆stico. Se encuentra matem谩ticamente que ambos m茅todos llegan a la misma forma soluci贸n. La tercera, usa el m茅todo m谩s utilizado en machine learning, el del gradiente descendiente (gradient descent).\nMi objetivo de este post no es hacer demostraciones matem谩ticas ni c谩lculos completos, sino que ilustrar la metodolog铆a, as铆 que nos enfocaremos en el m茅todo cl谩sico y el de gradiente descendiente. Veamos a m谩s detalle\n\nEl m茅todo de m铆nimos cuadrados cl谩sico\nPara minimizar la funci贸n de costo, los estimados de los par谩metros \\(w_0\\), \\(w_1\\), que son \\(\\hat{w_0}\\), \\(\\hat{w_1}\\) respectivamente, deben satisfacer\n\\[ \\frac{\\partial J}{\\partial w_0} \\bigg|_{\\hat{w_0}, \\hat{w_1}}= - \\frac{2}{m} \\sum_{i=1}^{m} (y_i - \\hat{w_0} - \\hat{w_1} x_i) = 0\\] y\n\\[ \\frac{\\partial J}{\\partial w_1} \\bigg|_{\\hat{w_0}, \\hat{w_1}} = - \\frac{2}{m} \\sum_{i=1}^{m} (y_i - \\hat{w_0} - \\hat{w_1} x_i) x_i = 0\\]\nde acuerdo al criterio de la primer derivada, 茅sta debe ser 0 para que sea un m铆nimo.\nSimplificando las expresiones, separando las sumatorias y resolviendo para \\(\\hat{w_0}\\), \\(\\hat{w_1}\\), se tienen las soluciones a los par谩metros estimados que minimizan la funci贸n de MCO\n\\[ \\hat{w_0} =  \\bar{y} - \\hat{w_1} \\bar{x}\\] y\n\\[ \\hat{w_1} = \\frac{ \\sum_{i=1}^{m} y_i x_i - \\frac{(\\sum_{i=1}^{m} y_i)(\\sum_{i=1}^{m} x_i)}{n} }{ \\sum_{i=1}^{m} x_i^2 - \\frac{(\\sum_{i=1}^{m}x_i)^2}{m} } \\]\ndonde \\(\\bar{x} = \\frac{1}{m}  \\sum_{i=1}^{m} x_i\\) y \\(\\bar{y} = \\frac{1}{m}  \\sum_{i=1}^{m} y_i\\) son los promedios de \\(x_i\\), \\(y_i\\), respectivamente.\nPara el caso multivariable, se tiene una soluci贸n de los par谩metros estimados \\(W^{*}\\) en forma matricial, dado por\n\\[ W^{*} = (X^T X)^{-1} X^{T} Y \\] Donde \\(X\\) es la matriz de dise帽o, la cual es de tama帽o \\(m \\times (n + 1)\\). Donde \\(m\\) es el n煤mero de datos y \\(n\\) el n煤mero de variables regresoras. Se construye poniendo en filas los vectores de datos \\(x_j = (1, x_{j1}, x_{j2}, ..., x_{jn})\\), donde \\(j = 1, 2, 3, ..., m\\), en todos los datos. Por otro lado, \\(Y\\) es la matriz (o vector) de tama帽o \\(m \\times 1\\), que se construye poniendo todos los puntos \\(y\\) de donde se har谩 la regresi贸n.\nAs铆,\n\\[ X = \\begin{pmatrix}\n1 & x_{11} & x_{12} & ... & x_{1n} \\\\\n1 & x_{21} & x_{22} & ... & x_{2n} \\\\\n&  & \\vdots &  &  \\\\\n1 & x_{m1} & x_{m2} & ... & x_{mn}\n\\end{pmatrix}  \\]\n\\[ Y = \\begin{pmatrix}\ny_1 \\\\\ny_2 \\\\\ny_3 \\\\\n\\vdots \\\\\ny_m\n\\end{pmatrix}  \\]\n\n\nEl m茅todo del gradiente descendiente\nRetomando conceptos de c谩lculo diferencial para una variable, la derivada de una funci贸n \\(f\\) en un punto dado \\(x_0\\), nos da la pendiende de la recta tangente a la curva en ese punto. En otras palabras, esa pendiente puede interprestarse como la raz贸n de cambio de la funci贸n en ese punto y el signo indica hacia donde est谩 cambiando. Extendiendo este concepto hacia varias variables, el gradiente de una funci贸n \\(f\\), denotado como \\(\\nabla f\\), es un vector que indica hacia d贸nde la funci贸n tiene el mayor incremento, y su magnitud es la raz贸n de cambio.\nEl m茅todo del gradiente descendiente, las derivadas son respecto a los par谩metros \\(w\\) que queremos ajustar, y como su nombre lo indica, hace uso del gradiente para minimizar la funci贸n de costo \\(J(w)\\). Para el caso de la regresi贸n lineal, el algoritmo se define de la siguiente manera\n\\[ w_i^{(\\tau + 1)} := w_i^{(\\tau)} - \\alpha \\frac{\\partial J(w)}{\\partial w_i}\\] donde \\(J(w)\\) esl funci贸n de costo de m铆nimos cuadrados ordinarios. La derivada est谩 multiplicada por un \\(-1\\), lo que asegura que se mueva en la direcci贸n de menor crecimiento, tenemos que \\(\\alpha\\) es una constante llamada tasa de aprendizaje, controla qu茅 tanto se ajusta el valor de \\(w_i\\). En machine learning, \\(\\alpha\\) suele tomar valores muy peque帽os, como \\(0.01, 0.001,\\) etc. Por otro lado, \\(\\tau\\) es un n煤mero entero que indica en la iteraci贸n que estamos del algoritmo.\nAs铆, se estar谩n ajustando los par谩metros \\(w_i\\) de manera iterativa hasta que se cumpla un cierto criterio o se alcance una tolerancia del error. Cabe se帽alar que en la primera iteraci贸n, los par谩metros se inicializan de manera aleatoria.\nPor lo tanto, para el caso de regresi贸n lineal simple, si utilizamos el m茅todo del gradiente descendiente tendriamos lo siguiente\n\\[ w_0^{(\\tau + 1)} := w_0^{(\\tau)} + \\alpha \\frac{2}{m} \\sum_{i=1}^{m} (y_i - \\hat{y_i})\\] \\[ w_1^{(\\tau + 1)} := w_1^{(\\tau)} + \\alpha \\frac{2}{m} \\sum_{i=1}^{m} (y_i - \\hat{y_i}) x_i\\] Donde \\(\\hat{y_i}\\) son las predicciones en la iteraci贸n \\(\\tau\\), o sea antes de hacer la actualizaci贸n de los par谩metros.\n\n\n驴Cu谩l usar de los 2 m茅todos?\nLa respuesta r谩dica en la complejidad computacional y el tiempo que toman los c谩lculos. En machine learning se trabajan con datos demasiado grandes, m谩s de lo que se sol铆a hacer en la estad铆stica convencional, por lo tanto, la forma matricial de obtener los par谩metros puede llegar a ser muy costosa computacionalmente hablando si se tienen muchos datos y muchas caracter铆sticas (variables regresoras). As铆, es m谩s conveniente utilizar el m茅todo del gradiente descendiente, que en la pr谩ctica suele ser m谩s eficiente."
  },
  {
    "objectID": "posts/la-regresion-lneal-en-machine-learning/index.html#m茅todos-de-regularizaci贸n",
    "href": "posts/la-regresion-lneal-en-machine-learning/index.html#m茅todos-de-regularizaci贸n",
    "title": "La Regresi贸n Lineal en Machine Learning",
    "section": "M茅todos de regularizaci贸n",
    "text": "M茅todos de regularizaci贸n\nLa regularizaci贸n en regresi贸n lineal, es para reducir la complejidad del modelo evitando as铆 un sobreajuste. Esto se logra a帽adiendo un t茅rmino a la funci贸n de costo de MCO, lo que acota los par谩metros evitando que crezcan demasiado. Hay 2 casos, Ridge y Lasso, como se ver谩 a continuaci贸n.\n\nRegresi贸n Lineal Ridge: limitando el tama帽o de los par谩metros\nLa regresi贸n ridge hace que los par谩metros \\(w_i\\) se encojan, es decir, que no tomen valores tan grandes. Se define con la siguiente funci贸n de costo \\(J_R(w)\\)\n\\[ J_R(W) = \\frac{1}{m} \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{n} w_j^2 \\] Sumando as铆 el t茅rmino \\(\\lambda \\sum_{j=1}^{n} w_j^2\\) de penalizaci贸n. Donde \\(\\lambda\\) controla que tanto se encojen los par谩metros, entre m谩s grande es el valor de \\(\\lambda\\) est谩n m谩s l铆mitados los par谩metros. Se tiene que cuando \\(\\lambda \\xrightarrow{}{} \\infty\\), los par谩metros \\(w_i\\) tienden a \\(0\\).\nNotemos que el par谩metro \\(w_0\\) no se incluye, ya que solo se toma en cuenta \\(w_1, w_2, ..., w_n\\). Eso es porque \\(w_0\\), como ya vimos, es el valor promedio de \\(y\\) cuando \\(x_1=x_2=...=x_n=0\\). No ser铆a conveniente imponer restricciones o encoger dicho par谩metro entonces.\nA la regresi贸n ridge tambi茅n se le conoce como regularizaci贸n L2, debido a que el t茅rmino \\(\\sum_{j=1}^{n} w_j^2\\) es la norma L2 del vector de par谩metros \\(w = (w_1, w_2, ..., w_n)\\).\n\n\nRegresi贸n Lineal Lasso: un selector de caracter铆sticas\nLa regresi贸n lasso act煤a como un selector de caracter铆sticas, ya que impone que algunos de los par谩metros \\(w_i\\) sean igual a \\(0\\). Se define con la siguiente funci贸n de costo \\(J_l(w)\\)\n\\[ J_l(W) = \\frac{1}{m} \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{n} |w_j| \\] Sumando as铆 el t茅rmino \\(\\lambda \\sum_{j=1}^{n} |w_j|\\) de penalizaci贸n. Similarmente, lasso tambi茅n hace que los par谩metros se encojan hac铆a 0, pero a diferencia de ridge, si \\(\\lambda\\) es suficientemente grande algunos par谩metros \\(w_i\\) van a ser \\(0\\). As铆, la variable regresora (o caracter铆stica) \\(x_i\\) asociado al par谩metro \\(w_i\\) no estar谩 involucrada en la ecuaci贸n de regresi贸n, por esta raz贸n lasso es un selector de caracter铆sticas.\nIgual que en el caso de regresi贸n ridge, el par谩metro \\(w_0\\) no se incluye, ya que no ser铆a conveniente imponer restricciones o hacer 0 el par谩metro \\(w_0\\), siendo que es es el valor promedio de \\(y\\) cuando \\(x_1=x_2=...=x_n=0\\).\nA la regresi贸n lasso tambi茅n se le conoce como regularizaci贸n L1, debido a que el t茅rmino \\(\\sum_{j=1}^{n} |w_j|\\) es la norma L1 del vector de par谩metros \\(w = (w_1, w_2, ..., w_n)\\).\n\n\nInterpretaci贸n geom茅trica de ridge y lasso\nSurge la pregunta, 驴Por qu茅 la regresi贸n ridge encoje los coeficientes pero la regresi贸n lasso fuerza algunos de ellos a ser \\(0\\)? Para contestar esto, encontr茅 que la mejor manera es gr谩ficamente.\nLa funci贸n de costo de m铆nimos cuadrados ordinarios MCO, es una funci贸n convexa, por lo que es posible minimizarla. Ahora bien, si la graficamos en el espacio de los par谩metros al minimizar se pueden probar todos las combinaciones de par谩metros, sin embargo, al a帽adir un t茅rmino de penalizaci贸n como en ridge y lasso, se hace una constricci贸n en el espacio en el que puede bucarse el m铆nimo. Veamos la siguiente figura (extra铆da del libro Introduction to Statistical Learning)\n\n\n\nCurvas de nivel de la funci贸n de costo de MCO en rojo y las constricciones de ridge y lasso en azul. Graficadas en el espacio de 2 par谩metros \\(w_1\\) y \\(w_2\\). El m铆nimo se encuentra en \\(\\hat{w}\\).\n\n\nEn este caso, es una regresi贸n en 2 dimensiones con \\(x_1\\), \\(x_2\\) y las gr谩ficas est谩n hechas en el espacio de los par谩metros \\(w_1\\), \\(w_2\\). Las l铆neas rojas son las curvas de nivel de la funci贸n de costo de MCO, y las 谩reas s贸lidas azules son las regiones de constricciones impuestas en los par谩metros \\(w_1\\), \\(w_2\\) por los t茅rminos de penalizaci贸n de lasso y ridge, respectivamente.\nPara el caso de ridge, tenemos la constricci贸n el c铆rculo \\(w_1^2 + w_2^2 \\le s\\), y para lasso el rombo \\(|w_1| + |w_2| \\le s\\). El punto de minimizaci贸n, donde est谩n los mejores par谩metros estimados, es la intersecci贸n m谩s pronta entre la curva de nivel de MCO y el 谩rea de constricci贸n. Podemos observar claramente que para lasso, este punto de intersecci贸n m谩s pronto se encuentra en la punta del rombo, es decir, sobre un eje donde el par谩metro \\(w_2 = 0\\); mientras que para ridge, el punto se encuentra sobre la circunferencia fuera de los ejes, por los que los par谩metros estar谩n encojidos por la constricci贸n pero no ser谩n 0. Esto se extiende a cualquier dimensi贸n, para \\(n \\gt 2\\), el rombo se convierte en un politopo mientras que el c铆rculo en una hiperesfera. Entonces para el politopo siempre habr谩 esquinas donde los par谩metros sean 0."
  },
  {
    "objectID": "posts/la-regresion-lneal-en-machine-learning/index.html#conclusiones",
    "href": "posts/la-regresion-lneal-en-machine-learning/index.html#conclusiones",
    "title": "La Regresi贸n Lineal en Machine Learning",
    "section": "Conclusiones",
    "text": "Conclusiones\nEn este post, se vi贸 c贸mo se define la regresi贸n lineal en una y varias variables, y los supuestos que se deben cumplir para su utilizaci贸n. Se explic贸 c贸mo minimizar su funci贸n de costo de m铆nimos cuadrados ordinarios en la manera cl谩sica y con el m茅todo de gradiente descendiente. Tambi茅n, se vieron los m茅todos de regularizaci贸n de regresi贸n ridge y lasso, y c贸mo influyen en los par谩metros encoji茅ndolos o haciendo algunos de ellos \\(0\\), respectivamente.\nComo trabajo futuro, queda llevar este conocimiento a implementaci贸n en c贸digo, para ver su funcionalidad pr谩ctica. Existen librer铆as para hacer regresi贸n lineal, ridge y lasso; en Python est谩 scikit-learn; mientras que en R tenemos stats y glmnet.\nRecomiendo ampliamente revisar las referencias usadas para este post, hay conocimiento muy bueno en ellas."
  },
  {
    "objectID": "posts/la-regresion-lneal-en-machine-learning/index.html#referencias",
    "href": "posts/la-regresion-lneal-en-machine-learning/index.html#referencias",
    "title": "La Regresi贸n Lineal en Machine Learning",
    "section": "Referencias",
    "text": "Referencias\n\nDouglas C. Montgomery, Elizabeth A. Peck, G. Geoffrey Vining (2021). Introduction to Linear Regression Analysis. Sixth Edition.\nGareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani (2023). An Introduction to Statistical Learning with Applications in R. 2nd Edition.\nYunpeng Tai (2021). A Survey Of Regression Algorithms And Connections With Deep Learning.\nMukul Ranjan (2022). How does Lasso regression(L1) encourage zero coefficients but not the L2?. Medium. Post link\nTerence Parr. A visual explanation for regularization of linear models. Explained.ai. Post link\nUser: QuantStyle. What is the computational cost of gradient descent vs linear regression?. StackExchange. Post link"
  },
  {
    "objectID": "posts/la-regresion-lineal-en-machine-learning/index.html",
    "href": "posts/la-regresion-lineal-en-machine-learning/index.html",
    "title": "La Regresi贸n Lineal en Machine Learning",
    "section": "",
    "text": "La regresi贸n lineal puede parecer algo muy simple, pero en realidad tiene un alcance muy grande para modelar datos. Puede verse desde una perspectiva meramente geom茅trica, sin embargo, tiene unos supuestos estad铆sticos que son sumamente importantes que se cumplan si queremos que el modelo funcione y sea aplicable.\nEn este post, quiero enfocarme en la regresi贸n lineal desde la perspectiva del machine learning, por lo que asume un conocimiento b谩sico previo de 茅ste. Cubrir谩 los siguientes puntos:\n\nQu茅 es la regresi贸n lineal en una y varias variables\nQu茅 significa que la regresi贸n lineal se puede aplicar mientras se tenga linealidad en los par谩metros\nSupuestos para la validez de la regresi贸n lineal\nC贸mo se define la funci贸n de costo para la regresi贸n lineal\nDiferencia entre la soluci贸n cerrada para minimizar la funci贸n de costo y el m茅todo del gradiente descendiente\nM茅todos de regularizaci贸n para regresi贸n lineal y su interpretaci贸n geom茅trica\n\nSin m谩s por comentar en la introducci贸n, comencemos con el post."
  },
  {
    "objectID": "posts/la-regresion-lineal-en-machine-learning/index.html#introducci贸n",
    "href": "posts/la-regresion-lineal-en-machine-learning/index.html#introducci贸n",
    "title": "La Regresi贸n Lineal en Machine Learning",
    "section": "",
    "text": "La regresi贸n lineal puede parecer algo muy simple, pero en realidad tiene un alcance muy grande para modelar datos. Puede verse desde una perspectiva meramente geom茅trica, sin embargo, tiene unos supuestos estad铆sticos que son sumamente importantes que se cumplan si queremos que el modelo funcione y sea aplicable.\nEn este post, quiero enfocarme en la regresi贸n lineal desde la perspectiva del machine learning, por lo que asume un conocimiento b谩sico previo de 茅ste. Cubrir谩 los siguientes puntos:\n\nQu茅 es la regresi贸n lineal en una y varias variables\nQu茅 significa que la regresi贸n lineal se puede aplicar mientras se tenga linealidad en los par谩metros\nSupuestos para la validez de la regresi贸n lineal\nC贸mo se define la funci贸n de costo para la regresi贸n lineal\nDiferencia entre la soluci贸n cerrada para minimizar la funci贸n de costo y el m茅todo del gradiente descendiente\nM茅todos de regularizaci贸n para regresi贸n lineal y su interpretaci贸n geom茅trica\n\nSin m谩s por comentar en la introducci贸n, comencemos con el post."
  },
  {
    "objectID": "posts/la-regresion-lineal-en-machine-learning/index.html#regresi贸n",
    "href": "posts/la-regresion-lineal-en-machine-learning/index.html#regresi贸n",
    "title": "La Regresi贸n Lineal en Machine Learning",
    "section": "Regresi贸n",
    "text": "Regresi贸n\nLa regresi贸n es una t茅cnica de modelaci贸n proveniente de la estad铆stica para modelar y analizar la relaci贸n entre variables. Se tiene una variable \\(y\\) llamada independiente, la cual se modela en funci贸n de otra variable \\(x\\) llamada independiente, que bien se puede tener una \\(n\\) cantidad de variables \\(x_1, x_2, ... , x_n\\) como se ver谩 m谩s adelante.\nEs importante mencionar que, para evitar confusiones con la noci贸n de indepencia de probabilidad, se prefiere nombrar a \\(y\\) como la variable respuesta y a \\(x\\) como las variables predictoras. En el contexto de machine learning, a las variables \\(x\\) se les denomina caracter铆sticas (features) y a \\(y\\) etiquetas (labels).\nEl tipo de datos con los que la regresi贸n trabaja son num茅ricos (flotantes), y vienen en duplas \\((x_1, y_1), ... , (x_m, y_m)\\), donde \\(m\\) es la cantidad de datos que tengamos, es decir, cardinalidad del conjunto de datos. N贸tese que se pueden tener m煤ltiples caracter铆sticas \\(x\\).\nPara trabajar con regresi贸n, si la dimensi贸n lo permite, suele hacerse un diagrama de dispersi贸n que es una gr谩fica de los datos representados como puntos. Por ejemplo, consideremos de \\(x\\) e \\(y\\), con 15 registros (\\(m = 15\\)), resultando el siguiente diagrama de dispersi贸n\n\n\n\n\n\n\n\n\n\nEn este caso, se ve claramente que los puntos tienen una tendencia lineal, esto es, que se les puede ajustar una recta para modelarlos con un error considerablemente bajo. La ecuaci贸n de la recta, es \\(y = ax + b\\), con \\(a\\) la pendiente y \\(b\\) la intersecci贸n en el eje \\(y\\). Dicha ecuaci贸n discribir铆a adecuadamente ese comportamiento, en las siguientes secciones veremos c贸mo ajustarla a los datos para encontrar los valores adecuados de \\(a\\) y \\(b\\).\nEn este diagrama en part铆cular, los puntos fueron simulados de una recta \\(y = 2x\\), a la que se les agreg贸 un error \\(\\varepsilon\\) que tiene una distribuci贸n normal con media \\(\\mu = 0\\) y desviaci贸n estandar \\(\\sigma = 1.5\\). Teniendo as铆 los par谩metros de la recta \\(a = 2\\) y \\(b = 0\\)\n\nObtenci贸n de datos para regresi贸n\nHay 3 maneras principales para obtener datos que puedan usarse en regresi贸n, los cuales se listan a continuaci贸n\n\nEstudio retrospectivo: Aqu铆 se tienen datos que ya fueron capturados en el pasado, y no necesariamente con una intenci贸n de ser analizados cient铆ficamente, por lo que pueden no estar en la forma m谩s ideal posible. Los datos y sus resultados ya se tienen, no se puede intervenir ni cambiar nada para mejorar la toma de datos. Este tipo de estudio es muy com煤n en machine learning y big data, donde obtienes datos con ciertas caracter铆sticas y ya etiquetados que fueron obtenidos en el pasado.\nEstudio observacional: Como su nombre puede indicarlo, en este tipo de estudio solamente se observa el proceso o experimento que quiere analizarse, interviniendo m铆nimamente solamente para la captura de los datos. Por temas 茅ticos, este estudio es muy usado en el 谩rea m茅dica, pues de lo contrario se estar铆a imponiendo sobre las personas las condiciones que deben tener (como empezar a fumar, por ejemplo).\nDise帽o experimental: Este tipo de estudio posee m谩s estrategia. Aqu铆 se manipulan las variables predictoras (o caracter铆sticas) de acuerdo a un dise帽o experimental, para tener ciertos valores y observar su efecto en la variable respuesta."
  },
  {
    "objectID": "posts/la-regresion-lineal-en-machine-learning/index.html#regresi贸n-lineal-simple",
    "href": "posts/la-regresion-lineal-en-machine-learning/index.html#regresi贸n-lineal-simple",
    "title": "La Regresi贸n Lineal en Machine Learning",
    "section": "Regresi贸n Lineal Simple",
    "text": "Regresi贸n Lineal Simple\nLa regresi贸n lineal simple es 煤til cuando existe una tendencia lineal en los datos. Aunque veremos m谩s adelante que va m谩s all谩 de lo lineal. Se le llama simple porque solo hay una caracter铆stica \\(x\\), y se modela como\n\\[y = w_0 + w_1 x + \\varepsilon\\] La cual puede verse como la ecuaci贸n de regresi贸n lineal poblacional, no obstante, en regresi贸n lineal trabajamos con \\(m\\) n煤mero de datos, entonces para cada punto se tendr谩 una ecuaci贸n de regresi贸n lineal muestral\n\\[y_i = w_0 + w_1 x_i + \\varepsilon_i \\quad i=1, 2, 3, ..., m\\]\nCompar谩ndo con la ecuaci贸n de la recta, podemos interpretar los par谩metros. Se tiene que \\(w_1\\) es el cambio en la media de la distribuci贸n de \\(y\\) producido por un cambio de unidad en \\(x\\). Mientras que \\(w_0\\) simplemente es la media de la distribuci贸n de \\(y\\) cuando \\(x = 0\\), si el dominio de \\(x\\) no incluye a \\(0\\) entonces el par谩metro \\(w_0\\) no tiene interpretaci贸n pr谩ctica.\nPor otro lado, \\(\\varepsilon\\) son los errores o ruido que se tiene en cada predicci贸n de \\(y\\). Se asumen que dichos errores tienen una distribuci贸n normal con media igual a cero \\(\\mu = 0\\) y una varianza desconicida \\(\\sigma^2\\). Es importante notar que, esa varianza es la misma en todos los puntos de la regresi贸n, lo que se conoce como homocedasticidad. A los errores \\(\\varepsilon\\) se les llama residuales pues se definen como \\(y - \\hat{y}\\).\nAhora bien, retomando el diagrama de dispersi贸n que se vio anteriormente, el cual tiene proviene de la recta \\(y = 2x\\) con un error \\(\\varepsilon\\) con media \\(0\\) y varianza \\(\\sigma^2 = 1.5^2\\). Se le puede ajustar una recta de regresi贸n lineal y se ver铆a como la siguiente figura\n\n\n\n\n\n\n\n\n\nObteniendo los coeficientes estimados \\(\\hat{w_0} = 0.3045\\) y \\(\\hat{w_1} = 2.0274\\). Teniendo as铆 una ecuaci贸n de regresi贸n lineal con la que podemos hacer predicciones\n\\[ \\hat{y} = 0.3045 + 2.0274 x\\] Notemos que los par谩metros reales de donde provienen los datos son \\(0\\) y \\(2\\), pero por la naturaleza estoc谩stica de los errores, los estimados no coinciden con exactitud. Entre mayor sea el n煤mero de datos \\(m\\), los par谩metros estimados ser谩n m谩s cercanos a los originales."
  },
  {
    "objectID": "posts/la-regresion-lineal-en-machine-learning/index.html#regresi贸n-lineal-m煤ltiple",
    "href": "posts/la-regresion-lineal-en-machine-learning/index.html#regresi贸n-lineal-m煤ltiple",
    "title": "La Regresi贸n Lineal en Machine Learning",
    "section": "Regresi贸n Lineal M煤ltiple",
    "text": "Regresi贸n Lineal M煤ltiple\nPara el caso en que la variable respuesta \\(y\\) est谩 realcionada con \\(n\\) variables regresoras, se tiene el modelo de la regresi贸n lineal m煤ltiple\n\\[ y = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n + \\varepsilon \\] As铆, a los par谩metros \\(w_j \\quad j = 0, 1, 2, ..., n\\) se les llama coeficientes de regresi贸n. Este modelo, al ser de multivariable, describe un hiperplano en el espacio \\(n\\)-dimensional de las variables regresoras \\(x\\).\nHaciendo sentido de los par谩metros. El par谩metro \\(w_j\\) representa el cambio esperado en la respuesta \\(y\\) por unidad de cambio en \\(x_j\\), cuando todo las dem谩s variables regresoras \\(x_i(i\\ne j)\\) se mantienen constantes.Por otra parte, el par谩metro \\(w_0\\) es la intersecci贸n del hiperplano en \\(y\\), si el dominio de los datos incluye \\(x_1 = x_2 = ... = x_n = 0\\) entonces \\(w_0\\) es la media de \\(y\\) cuando \\(x_1 = x_2 = ... = x_n = 0\\); de lo contrario, \\(w_0\\) no tiene interpretaci贸n f铆sica.\nConcluyendo, en el modelo de regresi贸n simple y m煤ltiple, la linealidad se tiene en los par谩metros. Esto extiende m谩s all谩 el concepto de linealidad aunque las variables regresoras \\(x\\) no sean lineales, esto se ver谩 a continuaci贸n en la secci贸n de regresi贸n polinomial."
  },
  {
    "objectID": "posts/la-regresion-lineal-en-machine-learning/index.html#regresi贸n-polinomial",
    "href": "posts/la-regresion-lineal-en-machine-learning/index.html#regresi贸n-polinomial",
    "title": "La Regresi贸n Lineal en Machine Learning",
    "section": "Regresi贸n Polinomial",
    "text": "Regresi贸n Polinomial\nYa vimos que una regresi贸n lineal de \\(n\\) variables regresoras genera un hiperplano \\(n\\)-dimensional. Ahora bien, cualquier modelo que es lineal en los par谩metros \\(w\\)s es un modelo de regresi贸n lineal, independientemente de la forma de la superficie que genere.\nComo su nombre lo indica, la regresi贸n polinomial de una variable \\(x\\) se define a partir de los polinomios, y se modela de la siguiente forma\n\\[ y = w_0 + w_1x + w_2x^2 + ... + w_nx^n + \\varepsilon\\] La cual es lineal en los par谩metros, por lo que se puede trabajar como una regresi贸n lineal m煤ltiple y usar la misma metodolog铆a. Para hacer esto, podemos definir \\(x_j = x^j \\quad j=1, 2, 3, ... , n\\). Por ejemplo, para el caso de un polinomio de grado 2 con \\(y = w_0 + w_1x + w_2x^2 + \\varepsilon\\), tendr铆amos \\(x_1 = x, x_2 = x^2\\) y la regresi贸n se convertir铆a en una lineal m煤ltiple\n\\[ y = w_0 + w_1x_1 + w_2x_2 + \\varepsilon\\] Con este tipo de regresi贸n, hay que ser muy cuidadosos. Ya que un polinomio de grado considerablemente alto puede apr贸ximar muy bien una curva no lineal, sin embargo, hay que tratar de hacer transformaciones y mantener el grado lo m谩s bajo posible para evitar un sobreajuste del modelo (empezar a generalizar el error mismo)."
  },
  {
    "objectID": "posts/la-regresion-lineal-en-machine-learning/index.html#supuestos-de-la-regresi贸n-lineal",
    "href": "posts/la-regresion-lineal-en-machine-learning/index.html#supuestos-de-la-regresi贸n-lineal",
    "title": "La Regresi贸n Lineal en Machine Learning",
    "section": "Supuestos de la regresi贸n lineal",
    "text": "Supuestos de la regresi贸n lineal\nEs importante saber cu谩ndo es prudente utilizar un modelo de regresi贸n lineal. Para que se cumplan las condiciones de la regresi贸n lineal simple y m煤ltiple, se tienen los siguientes supuestos necesarios\n\nLinealidad: La relaci贸n entre las variables regresoras \\(x\\) y la variable repuesta \\(y\\) debe ser lineal. Notemos que en la regresi贸n polinomial esto va m谩s all谩, necesitando solo linealidad en los par谩metros.\nHomocedasticidad: Significa que la varianza de los errores es constante.\nNormalidad de los errores: Los residulales, es decir los errores, deben seguir una distribuci贸n normal con media 0 y varianza \\(\\sigma^2\\).\nErrores independientes (no autocorrelaci贸n): Los errores no deben estar correlacionados entre ellos, es otras palabras, deben ser independientes e id茅nticamente distribuidos.\nNo multicolinealidad: No existe correlaci贸n entre las variables regresoras.\nNo exogeneidad: Las variables regresoras y los errores no est谩n correlacionados."
  },
  {
    "objectID": "posts/la-regresion-lineal-en-machine-learning/index.html#m铆nimos-cuadrados-ordinarios-como-funci贸n-de-costo",
    "href": "posts/la-regresion-lineal-en-machine-learning/index.html#m铆nimos-cuadrados-ordinarios-como-funci贸n-de-costo",
    "title": "La Regresi贸n Lineal en Machine Learning",
    "section": "M铆nimos cuadrados ordinarios como funci贸n de costo",
    "text": "M铆nimos cuadrados ordinarios como funci贸n de costo\nEn machine learning, la base para ajustar los par谩metros a nuestros datos, lo que se conoce como aprender, radica en la optimizaci贸n de una funci贸n objetivo. Dicho de otra manera, es minimizar una funci贸n de costo, donde dicha funci贸n ser铆a una medida del error en las predicciones del modelo.\nEn la regresi贸n lineal no es la excepci贸n. Es muy conocido el m茅todo de m铆nimos cuadrados para ajustar los par谩metros, donde se define la funci贸n de costo \\(J\\) de m铆nimos cuadrados ordinarios (MCO) de la siguiente manera\n\\[ J(y, \\hat{y}) = \\frac{1}{m} \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2 \\] Donde el factor \\(\\frac{1}{m}\\) se multiplica como una especie de promedio en todos los datos. Algunas referencias tambi茅n manejan multiplicarlo por \\(\\frac{1}{2}\\) por conveniencia algebraica.\nNotemos que la diferencia \\(y_i - \\hat{y}_i\\) es la distancia que hay desde un punto \\(\\hat{y} (x_i)\\) en la recta de la predicci贸n \\(\\hat{y}\\) hasta el correspondiente punto \\(y(x_i)\\) en la recta original \\(y\\) de donde provienen los datos. Est谩 al cuadrado para evitar que las sumas de esos t茅rminos puedan dar cero. As铆, podemos interpretarlo gr谩ficamente como unos cuadrados en dichos puntos \\(x_i\\), considerando el mismo diagrama de dispersi贸n de este post, se ver铆a\n\n\n\n\n\n\n\n\n\nN贸tese que los ejes fueron escalados para una mejor visualizaci贸n. Ahora, al ajustar los par谩metros \\(w\\) minimizando la funci贸n de costo de MCO, lo que se est谩 haciendo gr谩ficamente es ajustar la recta de predicci贸n de tal manera que se minimize el 谩rea de esos cuadrados; de ah铆 el nombre de este m茅todo.\nPara minimizar dicha funci贸n, es mejor expresarla en t茅rmino de los par谩metros \\(w\\), para esto, en el caso de una variable regresora sustituimos \\(\\hat{y} = w_0 + w_1 x\\) en la funci贸n de costo de MCO, as铆\n\\[ J(w_0, w_1) = \\frac{1}{m} \\sum_{i=1}^{m} (y_i - w_0 - w_1 x_i)^2 \\] En la siguiente secci贸n veremos c贸mo minimizarla, ejemplificando el caso con una sola variable regresora \\(x\\)."
  },
  {
    "objectID": "posts/la-regresion-lineal-en-machine-learning/index.html#ajuste-de-los-par谩metros-minimizando-la-funci贸n-de-costo",
    "href": "posts/la-regresion-lineal-en-machine-learning/index.html#ajuste-de-los-par谩metros-minimizando-la-funci贸n-de-costo",
    "title": "La Regresi贸n Lineal en Machine Learning",
    "section": "Ajuste de los par谩metros minimizando la funci贸n de costo",
    "text": "Ajuste de los par谩metros minimizando la funci贸n de costo\nHay 3 maneras principales de minimizar la funci贸n de costo de m铆nimos cuadrados ordinarios (MCO) \\(J\\). El m茅todo cl谩sico, utiliza c谩lculo diferencial y el criterio de las derivadas para minimizar. La segunda, utiliza el m茅todo de la m谩xima verosimilitud, un concepto estad铆stico. Se encuentra matem谩ticamente que ambos m茅todos llegan a la misma forma soluci贸n. La tercera, usa el m茅todo m谩s utilizado en machine learning, el del gradiente descendiente (gradient descent).\nMi objetivo de este post no es hacer demostraciones matem谩ticas ni c谩lculos completos, sino que ilustrar la metodolog铆a, as铆 que nos enfocaremos en el m茅todo cl谩sico y el de gradiente descendiente. Veamos a m谩s detalle\n\nEl m茅todo de m铆nimos cuadrados cl谩sico\nPara minimizar la funci贸n de costo, los estimados de los par谩metros \\(w_0\\), \\(w_1\\), que son \\(\\hat{w_0}\\), \\(\\hat{w_1}\\) respectivamente, deben satisfacer\n\\[ \\frac{\\partial J}{\\partial w_0} \\bigg|_{\\hat{w_0}, \\hat{w_1}}= - \\frac{2}{m} \\sum_{i=1}^{m} (y_i - \\hat{w_0} - \\hat{w_1} x_i) = 0\\] y\n\\[ \\frac{\\partial J}{\\partial w_1} \\bigg|_{\\hat{w_0}, \\hat{w_1}} = - \\frac{2}{m} \\sum_{i=1}^{m} (y_i - \\hat{w_0} - \\hat{w_1} x_i) x_i = 0\\]\nde acuerdo al criterio de la primer derivada, 茅sta debe ser 0 para que sea un m铆nimo.\nSimplificando las expresiones, separando las sumatorias y resolviendo para \\(\\hat{w_0}\\), \\(\\hat{w_1}\\), se tienen las soluciones a los par谩metros estimados que minimizan la funci贸n de MCO\n\\[ \\hat{w_0} =  \\bar{y} - \\hat{w_1} \\bar{x}\\] y\n\\[ \\hat{w_1} = \\frac{ \\sum_{i=1}^{m} y_i x_i - \\frac{(\\sum_{i=1}^{m} y_i)(\\sum_{i=1}^{m} x_i)}{n} }{ \\sum_{i=1}^{m} x_i^2 - \\frac{(\\sum_{i=1}^{m}x_i)^2}{m} } \\]\ndonde \\(\\bar{x} = \\frac{1}{m}  \\sum_{i=1}^{m} x_i\\) y \\(\\bar{y} = \\frac{1}{m}  \\sum_{i=1}^{m} y_i\\) son los promedios de \\(x_i\\), \\(y_i\\), respectivamente.\nPara el caso multivariable, se tiene una soluci贸n de los par谩metros estimados \\(W^{*}\\) en forma matricial, dado por\n\\[ W^{*} = (X^T X)^{-1} X^{T} Y \\] Donde \\(X\\) es la matriz de dise帽o, la cual es de tama帽o \\(m \\times (n + 1)\\). Donde \\(m\\) es el n煤mero de datos y \\(n\\) el n煤mero de variables regresoras. Se construye poniendo en filas los vectores de datos \\(x_j = (1, x_{j1}, x_{j2}, ..., x_{jn})\\), donde \\(j = 1, 2, 3, ..., m\\), en todos los datos. Por otro lado, \\(Y\\) es la matriz (o vector) de tama帽o \\(m \\times 1\\), que se construye poniendo todos los puntos \\(y\\) de donde se har谩 la regresi贸n.\nAs铆,\n\\[ X = \\begin{pmatrix}\n1 & x_{11} & x_{12} & ... & x_{1n} \\\\\n1 & x_{21} & x_{22} & ... & x_{2n} \\\\\n&  & \\vdots &  &  \\\\\n1 & x_{m1} & x_{m2} & ... & x_{mn}\n\\end{pmatrix}  \\]\n\\[ Y = \\begin{pmatrix}\ny_1 \\\\\ny_2 \\\\\ny_3 \\\\\n\\vdots \\\\\ny_m\n\\end{pmatrix}  \\]\n\n\nEl m茅todo del gradiente descendiente\nRetomando conceptos de c谩lculo diferencial para una variable, la derivada de una funci贸n \\(f\\) en un punto dado \\(x_0\\), nos da la pendiende de la recta tangente a la curva en ese punto. En otras palabras, esa pendiente puede interprestarse como la raz贸n de cambio de la funci贸n en ese punto y el signo indica hacia donde est谩 cambiando. Extendiendo este concepto hacia varias variables, el gradiente de una funci贸n \\(f\\), denotado como \\(\\nabla f\\), es un vector que indica hacia d贸nde la funci贸n tiene el mayor incremento, y su magnitud es la raz贸n de cambio.\nEl m茅todo del gradiente descendiente, las derivadas son respecto a los par谩metros \\(w\\) que queremos ajustar, y como su nombre lo indica, hace uso del gradiente para minimizar la funci贸n de costo \\(J(w)\\). Para el caso de la regresi贸n lineal, el algoritmo se define de la siguiente manera\n\\[ w_i^{(\\tau + 1)} := w_i^{(\\tau)} - \\alpha \\frac{\\partial J(w)}{\\partial w_i}\\] donde \\(J(w)\\) esl funci贸n de costo de m铆nimos cuadrados ordinarios. La derivada est谩 multiplicada por un \\(-1\\), lo que asegura que se mueva en la direcci贸n de menor crecimiento, tenemos que \\(\\alpha\\) es una constante llamada tasa de aprendizaje, controla qu茅 tanto se ajusta el valor de \\(w_i\\). En machine learning, \\(\\alpha\\) suele tomar valores muy peque帽os, como \\(0.01, 0.001,\\) etc. Por otro lado, \\(\\tau\\) es un n煤mero entero que indica en la iteraci贸n que estamos del algoritmo.\nAs铆, se estar谩n ajustando los par谩metros \\(w_i\\) de manera iterativa hasta que se cumpla un cierto criterio o se alcance una tolerancia del error. Cabe se帽alar que en la primera iteraci贸n, los par谩metros se inicializan de manera aleatoria.\nPor lo tanto, para el caso de regresi贸n lineal simple, si utilizamos el m茅todo del gradiente descendiente tendriamos lo siguiente\n\\[ w_0^{(\\tau + 1)} := w_0^{(\\tau)} + \\alpha \\frac{2}{m} \\sum_{i=1}^{m} (y_i - \\hat{y_i})\\] \\[ w_1^{(\\tau + 1)} := w_1^{(\\tau)} + \\alpha \\frac{2}{m} \\sum_{i=1}^{m} (y_i - \\hat{y_i}) x_i\\] Donde \\(\\hat{y_i}\\) son las predicciones en la iteraci贸n \\(\\tau\\), o sea antes de hacer la actualizaci贸n de los par谩metros.\n\n\n驴Cu谩l usar de los 2 m茅todos?\nLa respuesta r谩dica en la complejidad computacional y el tiempo que toman los c谩lculos. En machine learning se trabajan con datos demasiado grandes, m谩s de lo que se sol铆a hacer en la estad铆stica convencional, por lo tanto, la forma matricial de obtener los par谩metros puede llegar a ser muy costosa computacionalmente hablando si se tienen muchos datos y muchas caracter铆sticas (variables regresoras). As铆, es m谩s conveniente utilizar el m茅todo del gradiente descendiente, que en la pr谩ctica suele ser m谩s eficiente."
  },
  {
    "objectID": "posts/la-regresion-lineal-en-machine-learning/index.html#m茅todos-de-regularizaci贸n",
    "href": "posts/la-regresion-lineal-en-machine-learning/index.html#m茅todos-de-regularizaci贸n",
    "title": "La Regresi贸n Lineal en Machine Learning",
    "section": "M茅todos de regularizaci贸n",
    "text": "M茅todos de regularizaci贸n\nLa regularizaci贸n en regresi贸n lineal, es para reducir la complejidad del modelo evitando as铆 un sobreajuste. Esto se logra a帽adiendo un t茅rmino a la funci贸n de costo de MCO, lo que acota los par谩metros evitando que crezcan demasiado. Hay 2 casos, Ridge y Lasso, como se ver谩 a continuaci贸n.\n\nRegresi贸n Lineal Ridge: limitando el tama帽o de los par谩metros\nLa regresi贸n ridge hace que los par谩metros \\(w_i\\) se encojan, es decir, que no tomen valores tan grandes. Se define con la siguiente funci贸n de costo \\(J_R(w)\\)\n\\[ J_R(W) = \\frac{1}{m} \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{n} w_j^2 \\] Sumando as铆 el t茅rmino \\(\\lambda \\sum_{j=1}^{n} w_j^2\\) de penalizaci贸n. Donde \\(\\lambda\\) controla que tanto se encojen los par谩metros, entre m谩s grande es el valor de \\(\\lambda\\) est谩n m谩s l铆mitados los par谩metros. Se tiene que cuando \\(\\lambda \\xrightarrow{}{} \\infty\\), los par谩metros \\(w_i\\) tienden a \\(0\\).\nNotemos que el par谩metro \\(w_0\\) no se incluye, ya que solo se toma en cuenta \\(w_1, w_2, ..., w_n\\). Eso es porque \\(w_0\\), como ya vimos, es el valor promedio de \\(y\\) cuando \\(x_1=x_2=...=x_n=0\\). No ser铆a conveniente imponer restricciones o encoger dicho par谩metro entonces.\nA la regresi贸n ridge tambi茅n se le conoce como regularizaci贸n L2, debido a que el t茅rmino \\(\\sum_{j=1}^{n} w_j^2\\) es la norma L2 del vector de par谩metros \\(w = (w_1, w_2, ..., w_n)\\).\n\n\nRegresi贸n Lineal Lasso: un selector de caracter铆sticas\nLa regresi贸n lasso act煤a como un selector de caracter铆sticas, ya que impone que algunos de los par谩metros \\(w_i\\) sean igual a \\(0\\). Se define con la siguiente funci贸n de costo \\(J_l(w)\\)\n\\[ J_l(W) = \\frac{1}{m} \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{n} |w_j| \\] Sumando as铆 el t茅rmino \\(\\lambda \\sum_{j=1}^{n} |w_j|\\) de penalizaci贸n. Similarmente, lasso tambi茅n hace que los par谩metros se encojan hac铆a 0, pero a diferencia de ridge, si \\(\\lambda\\) es suficientemente grande algunos par谩metros \\(w_i\\) van a ser \\(0\\). As铆, la variable regresora (o caracter铆stica) \\(x_i\\) asociado al par谩metro \\(w_i\\) no estar谩 involucrada en la ecuaci贸n de regresi贸n, por esta raz贸n lasso es un selector de caracter铆sticas.\nIgual que en el caso de regresi贸n ridge, el par谩metro \\(w_0\\) no se incluye, ya que no ser铆a conveniente imponer restricciones o hacer 0 el par谩metro \\(w_0\\), siendo que es es el valor promedio de \\(y\\) cuando \\(x_1=x_2=...=x_n=0\\).\nA la regresi贸n lasso tambi茅n se le conoce como regularizaci贸n L1, debido a que el t茅rmino \\(\\sum_{j=1}^{n} |w_j|\\) es la norma L1 del vector de par谩metros \\(w = (w_1, w_2, ..., w_n)\\).\n\n\nInterpretaci贸n geom茅trica de ridge y lasso\nSurge la pregunta, 驴Por qu茅 la regresi贸n ridge encoje los coeficientes pero la regresi贸n lasso fuerza algunos de ellos a ser \\(0\\)? Para contestar esto, encontr茅 que la mejor manera es gr谩ficamente.\nLa funci贸n de costo de m铆nimos cuadrados ordinarios MCO, es una funci贸n convexa, por lo que es posible minimizarla. Ahora bien, si la graficamos en el espacio de los par谩metros al minimizar se pueden probar todos las combinaciones de par谩metros, sin embargo, al a帽adir un t茅rmino de penalizaci贸n como en ridge y lasso, se hace una constricci贸n en el espacio en el que puede bucarse el m铆nimo. Veamos la siguiente figura (extra铆da del libro Introduction to Statistical Learning)\n\n\n\nCurvas de nivel de la funci贸n de costo de MCO en rojo y las constricciones de ridge y lasso en azul. Graficadas en el espacio de 2 par谩metros \\(w_1\\) y \\(w_2\\). El m铆nimo se encuentra en \\(\\hat{w}\\).\n\n\nEn este caso, es una regresi贸n en 2 dimensiones con \\(x_1\\), \\(x_2\\) y las gr谩ficas est谩n hechas en el espacio de los par谩metros \\(w_1\\), \\(w_2\\). Las l铆neas rojas son las curvas de nivel de la funci贸n de costo de MCO, y las 谩reas s贸lidas azules son las regiones de constricciones impuestas en los par谩metros \\(w_1\\), \\(w_2\\) por los t茅rminos de penalizaci贸n de lasso y ridge, respectivamente.\nPara el caso de ridge, tenemos la constricci贸n el c铆rculo \\(w_1^2 + w_2^2 \\le s\\), y para lasso el rombo \\(|w_1| + |w_2| \\le s\\). El punto de minimizaci贸n, donde est谩n los mejores par谩metros estimados, es la intersecci贸n m谩s pronta entre la curva de nivel de MCO y el 谩rea de constricci贸n. Podemos observar claramente que para lasso, este punto de intersecci贸n m谩s pronto se encuentra en la punta del rombo, es decir, sobre un eje donde el par谩metro \\(w_2 = 0\\); mientras que para ridge, el punto se encuentra sobre la circunferencia fuera de los ejes, por los que los par谩metros estar谩n encojidos por la constricci贸n pero no ser谩n 0. Esto se extiende a cualquier dimensi贸n, para \\(n \\gt 2\\), el rombo se convierte en un politopo mientras que el c铆rculo en una hiperesfera. Entonces para el politopo siempre habr谩 esquinas donde los par谩metros sean 0."
  },
  {
    "objectID": "posts/la-regresion-lineal-en-machine-learning/index.html#conclusiones",
    "href": "posts/la-regresion-lineal-en-machine-learning/index.html#conclusiones",
    "title": "La Regresi贸n Lineal en Machine Learning",
    "section": "Conclusiones",
    "text": "Conclusiones\nEn este post, se vi贸 c贸mo se define la regresi贸n lineal en una y varias variables, y los supuestos que se deben cumplir para su utilizaci贸n. Se explic贸 c贸mo minimizar su funci贸n de costo de m铆nimos cuadrados ordinarios en la manera cl谩sica y con el m茅todo de gradiente descendiente. Tambi茅n, se vieron los m茅todos de regularizaci贸n de regresi贸n ridge y lasso, y c贸mo influyen en los par谩metros encoji茅ndolos o haciendo algunos de ellos \\(0\\), respectivamente.\nComo trabajo futuro, queda llevar este conocimiento a implementaci贸n en c贸digo, para ver su funcionalidad pr谩ctica. Existen librer铆as para hacer regresi贸n lineal, ridge y lasso; en Python est谩 scikit-learn; mientras que en R tenemos stats y glmnet.\nRecomiendo ampliamente revisar las referencias usadas para este post, hay conocimiento muy bueno en ellas."
  },
  {
    "objectID": "posts/la-regresion-lineal-en-machine-learning/index.html#referencias",
    "href": "posts/la-regresion-lineal-en-machine-learning/index.html#referencias",
    "title": "La Regresi贸n Lineal en Machine Learning",
    "section": "Referencias",
    "text": "Referencias\n\nDouglas C. Montgomery, Elizabeth A. Peck, G. Geoffrey Vining (2021). Introduction to Linear Regression Analysis. Sixth Edition.\nGareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani (2023). An Introduction to Statistical Learning with Applications in R. 2nd Edition.\nYunpeng Tai (2021). A Survey Of Regression Algorithms And Connections With Deep Learning.\nMukul Ranjan (2022). How does Lasso regression(L1) encourage zero coefficients but not the L2?. Medium. Post link\nTerence Parr. A visual explanation for regularization of linear models. Explained.ai. Post link\nUser: QuantStyle. What is the computational cost of gradient descent vs linear regression?. StackExchange. Post link"
  },
  {
    "objectID": "posts/5-cosas-que-aprendi-en-fisica-y-uso-en-ciencia-de-datos/index.html",
    "href": "posts/5-cosas-que-aprendi-en-fisica-y-uso-en-ciencia-de-datos/index.html",
    "title": "5 Cosas que Aprend铆 en F铆sica y Uso en Ciencia de Datos",
    "section": "",
    "text": "En este post voy a compartir 5 cosas que aprend铆 en f铆sica, que me han sido de utilidad en la ciencia de datos. Ya sea una metodolog铆a en particular o un concepto an谩logo. Desde las matem谩ticas del 谩lgebra lineal y c谩lculo diferencial, hasta como la relatividad especial de Einstein me record贸 al concepto de Big Data."
  },
  {
    "objectID": "posts/5-cosas-que-aprendi-en-fisica-y-uso-en-ciencia-de-datos/index.html#los-datos-son-lo-m谩s-valioso",
    "href": "posts/5-cosas-que-aprendi-en-fisica-y-uso-en-ciencia-de-datos/index.html#los-datos-son-lo-m谩s-valioso",
    "title": "5 Cosas que Aprend铆 en F铆sica y Uso en Ciencia de Datos",
    "section": "Los datos son lo m谩s valioso",
    "text": "Los datos son lo m谩s valioso\n\nGarbage in, garbage out.\n\nEn f铆sica los datos se obtienen de mediciones hechas en experimentos, ya sea directamente o indirectamente mediante observaciones. Tambi茅n est谩 la posibilidad de las simulaciones computacionales. Lo que aprend铆 durante los laboratorios que curs茅, es que los datos son lo m谩s valioso.\nAl hacer experimentos, se debe procurar no perturbar el sistema en estudio para no inducir ruido en los datos, para que as铆 las mediciones sean m谩s cercanas a las reales, adem谩s de verificar que no est茅n influyendo factores externos. Similarmente en machine learning, los datos no deben tener sesgos que afecten el resultado de los modelos. En ambas disciplinas, si se quieren modelos y explicaciones s贸lidas, los datos tienen que ser m谩s que buenos."
  },
  {
    "objectID": "posts/5-cosas-que-aprendi-en-fisica-y-uso-en-ciencia-de-datos/index.html#el-concepto-de-big-data-recuerda-a-la-relatividad-especial-de-einstein",
    "href": "posts/5-cosas-que-aprendi-en-fisica-y-uso-en-ciencia-de-datos/index.html#el-concepto-de-big-data-recuerda-a-la-relatividad-especial-de-einstein",
    "title": "5 Cosas que Aprend铆 en F铆sica y Uso en Ciencia de Datos",
    "section": "El concepto de Big Data recuerda a la relatividad especial de Einstein",
    "text": "El concepto de Big Data recuerda a la relatividad especial de Einstein\nLa mec谩nica cl谩sica de Newton formula 3 leyes, de las cuales se derivan las ecuaciones de movimiento. Resolviendo dichas ecuaciones, se puede describir el movimiento de un objeto con su posici贸n en funci贸n del tiempo, como lo son el movimiento de un paracaidista en ca铆da libre, el de un tren o el de una pelota de baseball que va de home run.\nPara el a帽o 1905, Albert Einstein desarroll贸 su teor铆a especial de la relatividad, la cual postula que la velocidad de la luz (denotada como c) es una constante universal. Esto tiene implicaciones en las ecuaciones de la mec谩nica cl谩sica antes mencionadas, pues ya hay un l铆mite en la velocidad m谩xima que se puede alcanzar, por lo que las f贸rmulas se tienen que ajustar para cuando las velocidades son muy cercanas a la de la luz. Usualmente esto resulta con un factor de multuplicaci贸n llamado gamma.\nAhora, Big Data surge cuando los datos son tan masivos que los m茅todos y tecnolog铆a convencionales no pueden procesar esa cantidad. Big Data suele definirse con las 3 Vs, Volumen, Velocidad y Variedad. Para solventar este problema, surgieron metodolog铆as como MapReduce, tecnolog铆as como Hadoop y Spark, adem谩s de los avances en el hardware de c贸mputo.\nAs铆, Big Data me recuerda a la relatividad especial de Einstein, en el sentido que cuando traspasamos un cierto umbral (ya sea de cantidad de datos o de velocidad), tenemos que aprender a trabajar con nuevas metodolog铆as y hacer ajustes a lo ya establecido."
  },
  {
    "objectID": "posts/5-cosas-que-aprendi-en-fisica-y-uso-en-ciencia-de-datos/index.html#el-m茅todo-cient铆fico-y-la-reproducibilidad",
    "href": "posts/5-cosas-que-aprendi-en-fisica-y-uso-en-ciencia-de-datos/index.html#el-m茅todo-cient铆fico-y-la-reproducibilidad",
    "title": "5 Cosas que Aprend铆 en F铆sica y Uso en Ciencia de Datos",
    "section": "El m茅todo cient铆fico y la reproducibilidad",
    "text": "El m茅todo cient铆fico y la reproducibilidad\nLa f铆sica utiliza el m茅todo cient铆fico para sus experimentos y teor铆as. En base a esto, cada experimento debe ser replicable si se tienen las mismas circunstancias. Esto asegura la solidez de una teor铆a.\nSimilarmente, en ciencia de datos todos los experimentos que se hagan con modelos deben ser replicables. Es por esto que el c贸digo es tan utilizado en esta 谩rea, ya que es m谩s f谩cil de replicar todas las configuraciones y experimentos en c贸digo, que hacerlo mediante clicks donde se pueden perder pasos. Vaya, es por algo que ciencia de datos lleva el nombre ciencia."
  },
  {
    "objectID": "posts/5-cosas-que-aprendi-en-fisica-y-uso-en-ciencia-de-datos/index.html#los-modelos-son-representaciones-de-la-realidad",
    "href": "posts/5-cosas-que-aprendi-en-fisica-y-uso-en-ciencia-de-datos/index.html#los-modelos-son-representaciones-de-la-realidad",
    "title": "5 cosas que aprend铆 en f铆sica y uso en ciencia de datos",
    "section": "Los modelos son representaciones de la realidad",
    "text": "Los modelos son representaciones de la realidad\n\nAll models are wrong, but some are useful. - George Box\n\nLos modelos que surgen de las teor铆as f铆sicas que se fundamentan en experimentos, en realidad son representaciones aproximadas de la realidad. Pero pueden ser aproximaciones tan exactas que funcionan perfectamente. En f铆sica siempre puedes ajustar modelos tomando en cuenta o ignorando ciertos factores, y no siempre el modelo debe ser una representaci贸n id茅ntica a la realidad, si no que debe resolver el problema del sistema que se est茅 estudiando.\nSimilarmente en la ciencia de datos, en el machine learning, los modelos no van a ser una representaci贸n exacta de la tarea ala que se le est茅 ajustando al modelo. Siempre hay que tener en cuenta que el modelo aprende de los datos, generaliza patrones pero debe tenerse cuidado de no obviar que el modelo es una explicaci贸n de la realidad."
  },
  {
    "objectID": "posts/5-cosas-que-aprendi-en-fisica-y-uso-en-ciencia-de-datos/index.html#se-comparten-fundamentos-matem谩ticos",
    "href": "posts/5-cosas-que-aprendi-en-fisica-y-uso-en-ciencia-de-datos/index.html#se-comparten-fundamentos-matem谩ticos",
    "title": "5 Cosas que Aprend铆 en F铆sica y Uso en Ciencia de Datos",
    "section": "Se comparten fundamentos matem谩ticos",
    "text": "Se comparten fundamentos matem谩ticos\nLas matem谩ticas que aprend铆 en f铆sica sin duda alguna me han sido de gran utilidad en la ciencia de datos. Desde el c谩lculo, base de la mec谩nica cl谩sica, o el 谩lgebra lineal de la mec谩nica cu谩ntica; que a su vez son fundamental para las computaciones y la optimizaci贸n en ciencia de datos. Por ejemplo, en machine learning para entrenar un modelo hay que minimizar una funci贸n de costo con c谩lculo, as铆 mismo, los modelos se definen mediante operaciones matriciales.\nNi hablar de la probabilidad y la estad铆stica, que aprend铆 durante mi carrera en f铆sica, tambi茅n son un pilar en ciencia de datos. Desde histogramas, diagramas de cajas, modelos estad铆sticos de regresi贸n y clasificaci贸n, hasta el enfoque probabil铆stico del machine learning. Cabe resaltar que si se hace un cambio de carrera desde f铆sica a la ciencia de datos, las matem谩ticas ser谩n un fuerte."
  },
  {
    "objectID": "posts/5-cosas-que-aprendi-en-fisica-y-uso-en-ciencia-de-datos/index.html#los-modelos-son-representaciones-aproximadas-de-la-realidad",
    "href": "posts/5-cosas-que-aprendi-en-fisica-y-uso-en-ciencia-de-datos/index.html#los-modelos-son-representaciones-aproximadas-de-la-realidad",
    "title": "5 Cosas que Aprend铆 en F铆sica y Uso en Ciencia de Datos",
    "section": "Los modelos son representaciones aproximadas de la realidad",
    "text": "Los modelos son representaciones aproximadas de la realidad\n\nAll models are wrong, but some are useful. - George Box\n\nLos modelos que surgen de las teor铆as f铆sicas que se fundamentan en experimentos, en realidad son representaciones aproximadas de la realidad. Pero pueden ser aproximaciones tan exactas que funcionan perfectamente. En f铆sica siempre puedes crear modelos ignorando ciertos factores, y no siempre el modelo debe ser una representaci贸n id茅ntica a la realidad, si no que debe resolver el problema del sistema que se est茅 estudiando.\nSimilarmente en la ciencia de datos, en el machine learning, los modelos no van a ser una representaci贸n exacta de la tarea a la cual se est茅n ajustando. Siempre hay que tener en cuenta que el modelo aprende de los datos, generaliza patrones, pero debe tenerse cuidado de no obviar al modelo como una replica de la realidad."
  }
]