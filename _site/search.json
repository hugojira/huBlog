[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hugo Valenzuela Chaparro",
    "section": "",
    "text": "Soy cient√≠fico de datos, f√≠sico de formaci√≥n. Comenc√© trabajando en proyectos de consultor√≠a estad√≠stica en la universidad, relacionados con la miner√≠a y el manejo de las islas de calor en ciudades, fue ah√≠ donde me enamor√© de la ciencia de datos usando R. Posteriormente curs√© la maestr√≠a en ciencia de datos, en la Universidad de Sonora, donde prosegu√≠ con mi especializaci√≥n en Machine Learning.\nActualmente entre mis intereses est√°n: GenAI, Machine Learning y su aplicaci√≥n en audio, MLOps. Visualizaci√≥n de datos y soluciones mediante dashboards.\nTrabajo profesionalmente tanto con R como Python.\nEste sitio web es mi portafolio, donde pueden encontrar mi blog personal huBlog (bueno, en realidad, m√°s que un blog se considerar√° mi jard√≠n digital) y mis proyectos.\n\n\n\nBI Support Engineer @ Orbtiware | Enero 2023 - presente\n\n\n\nUniversidad de Sonora | Licenciatura en F√≠sica | üìç Hermosillo, Sonora, M√©xico | 2019\nUniversidad de Sonora | Maestr√≠a en Ciencia de Datos | üìç Hermosillo, Sonora, M√©xico | 2023"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "huBlog",
    "section": "",
    "text": "El contenido escrito aqu√≠ estar√° en espa√±ol, pudiendo contener palabras en ingl√©s.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5 Cosas que Aprend√≠ en F√≠sica y Uso en Ciencia de Datos\n\n\n\n\n\n\nCiencia de Datos\n\n\nF√≠sica\n\n\n\nConexiones de la f√≠sica y la ciencia de datos\n\n\n\n\n\n28 sept 2024\n\n\nHugo Valenzuela Chaparro\n\n\n5 minutos\n\n\n\n\n\n\n\n\n\n\n\n\nLa Regresi√≥n Lineal en Machine Learning\n\n\n\n\n\n\nMachine Learning\n\n\nRegresi√≥n\n\n\n\nEl alcance de la regresi√≥n lineal en machine learning, su ajuste de par√°metros con descenso del gradiente y m√©todos de regularizaci√≥n\n\n\n\n\n\n17 sept 2024\n\n\nHugo Valenzuela Chaparro\n\n\n20 minutos\n\n\n\n\n\n\n\n\n\n\n\n\nPost de bienvenida\n\n\n\n\n\n\nInfo\n\n\n\n\n\n\n\n\n\n27 mar 2024\n\n\nHugo Valenzuela Chaparro\n\n\n1 minutos\n\n\n\n\n\n\nNo hay resultados"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Hugo Valenzuela Chaparro",
    "section": "",
    "text": "Universidad de Sonora | Licenciatura en F√≠sica | üìç Hermosillo, Sonora, M√©xico | 2019\nUniversidad de Sonora | Maestr√≠a en Ciencia de Datos | üìç Hermosillo, Sonora, M√©xico | 2023"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Hugo Valenzuela Chaparro",
    "section": "",
    "text": "Orbitware | Report Monitoring | Enero 2023 - presente"
  },
  {
    "objectID": "index.html#bio",
    "href": "index.html#bio",
    "title": "Hugo Valenzuela Chaparro",
    "section": "",
    "text": "Soy cient√≠fico de datos, f√≠sico de formaci√≥n. Comenc√© trabajando en proyectos de consultor√≠a estad√≠stica en la universidad, relacionados con la miner√≠a y el manejo de las islas de calor en ciudades, fue ah√≠ donde me enamor√© de la ciencia de datos usando R. Posteriormente curs√© la maestr√≠a en ciencia de datos, en la Universidad de Sonora, donde prosegu√≠ con mi especializaci√≥n en Machine Learning.\nActualmente entre mis intereses est√°n: GenAI, Machine Learning y su aplicaci√≥n en audio, MLOps. Visualizaci√≥n de datos y soluciones mediante dashboards.\nTrabajo profesionalmente tanto con R como Python.\nEste sitio web es mi portafolio, donde pueden encontrar mi blog personal huBlog (bueno, en realidad, m√°s que un blog se considerar√° mi jard√≠n digital) y mis proyectos."
  },
  {
    "objectID": "index.html#educaci√≥n",
    "href": "index.html#educaci√≥n",
    "title": "Hugo Valenzuela Chaparro",
    "section": "",
    "text": "Universidad de Sonora | Licenciatura en F√≠sica | üìç Hermosillo, Sonora, M√©xico | 2019\nUniversidad de Sonora | Maestr√≠a en Ciencia de Datos | üìç Hermosillo, Sonora, M√©xico | 2023"
  },
  {
    "objectID": "index.html#experiencia",
    "href": "index.html#experiencia",
    "title": "Hugo Valenzuela Chaparro",
    "section": "",
    "text": "BI Support Engineer @ Orbtiware | Enero 2023 - presente"
  },
  {
    "objectID": "index.html#formaci√≥n",
    "href": "index.html#formaci√≥n",
    "title": "Hugo Valenzuela Chaparro",
    "section": "",
    "text": "Universidad de Sonora | Licenciatura en F√≠sica | üìç Hermosillo, Sonora, M√©xico | 2019\nUniversidad de Sonora | Maestr√≠a en Ciencia de Datos | üìç Hermosillo, Sonora, M√©xico | 2023"
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "CC BY-SA 4.0 DEED",
    "section": "",
    "text": "Todo el contenido de este sitio web, incluido el blog, est√°n bajo la licencia Creative Commons Atribuci√≥n-CompartirIgual 4.0 Internacional."
  },
  {
    "objectID": "license.html#atribuci√≥n-compartirigual-4.0-internacional",
    "href": "license.html#atribuci√≥n-compartirigual-4.0-internacional",
    "title": "CC BY-SA 4.0 DEED",
    "section": "",
    "text": "Todo el contenido de este sitio web, incluido el blog, est√°n bajo la licencia Creative Commons Atribuci√≥n-CompartirIgual 4.0 Internacional."
  },
  {
    "objectID": "posts/2024-03-30-post-de-bienvenida/index.html",
    "href": "posts/2024-03-30-post-de-bienvenida/index.html",
    "title": "Post de bienvenida",
    "section": "",
    "text": "¬°Hola! Les doy la bienvenida a este primer post. Este blog en realidad es un jard√≠n digital, esto quiere decir que es un espacio para almacenar mis notas de aprendizaje, as√≠ c√≥mo tambi√©n entradas de blog usuales. Este contenido se ir√° mejorando y actualizando con el tiempo, nada es est√°tico.\nPr√≥ximamente estar√© populando este sitio con entradas de blog y notas, con contenido relacionado a la ciencia de datos que espero tambi√©n les sea de utilidad como a m√≠.\nPara la creaci√≥n del primer esqueleto de este sitio web, me inspir√© en los sitios de Alison Hill, Beatriz Milz, Mike Mahoney, as√≠ como en la documentaci√≥n de Quarto.\nSin m√°s que decir, les invito a explorar el blog."
  },
  {
    "objectID": "proyectos/proyectos/2024-03-30-post-de-bienvenida/index.html",
    "href": "proyectos/proyectos/2024-03-30-post-de-bienvenida/index.html",
    "title": "Post de bienvenida",
    "section": "",
    "text": "¬°Hola! Les doy la bienvenida a este primer post. Este blog en realidad es un jard√≠n digital, esto quiere decir que es un espacio para almacenar mis notas de aprendizaje, as√≠ c√≥mo tambi√©n entradas de blog usuales. Este contenido se ir√° mejorando y actualizando con el tiempo, nada es est√°tico.\nPr√≥ximamente estar√© populando este sitio con entradas de blog y notas, con contenido relacionado a la ciencia de datos que espero tambi√©n les sea de utilidad como a m√≠.\nPara la creaci√≥n del primer esqueleto de este sitio web, me inspir√© en los sitios de Mike Mahoney, Alison Hill, as√≠ como en la documentaci√≥n de Quarto.\nSin m√°s que decir, les invito a explorar el blog."
  },
  {
    "objectID": "proyectos/index.html",
    "href": "proyectos/index.html",
    "title": "Proyectos",
    "section": "",
    "text": "Aqu√≠ se encuentran los proyectos personales en los que he trabajado. No se incluyen aquellos proyectos en los que he colaborado en donde existan acuerdos de confidencialidad NDA.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTop Hits 1970-2019\n\n\n\nAudio\n\n\nAPI\n\n\nDashboard\n\n\nShiny\n\n\n\n\n\n\n\nHugo Valenzuela\n\n\n25 abr 2024\n\n\n\n\n\n\n\n\nNo hay resultados"
  },
  {
    "objectID": "proyectos/proyectos/2024-04-25-top-hits-features/index.html",
    "href": "proyectos/proyectos/2024-04-25-top-hits-features/index.html",
    "title": "Top Hits 1970-2019",
    "section": "",
    "text": "Para este proyecto de dashboard interactivo, se buscaba obtener patr√≥n en las caracter√≠sticas de audio de las canciones m√°s populares (Top Hits) desde 1970 a 2019. Esto con el objetivo de identificar la tendencia y ver c√≥mo podr√≠a entrarse a esas listas de reproducci√≥n de popularidad.\nPara la construcci√≥n del dashboard, se utiliz√≥ R, con la librer√≠a flexdashboard, agreg√°ndole shiny para la interactividad.\nEnlace al dashboard: https://hugojira.shinyapps.io/TopHits1970-2019/.\nEl dashboard cuenta con 2 pesta√±as principales que se ven de la siguiente manera:\n\n\n\nP√°gina de informaci√≥n general\n\n\n\n\n\n\nP√°gina interactiva con shiny\n\n\n\n\nLos datos fueron descargados desde la API Web de Spotify con c√≥digo en R, utilizando el wrapper Spotifyr. El c√≥digo y una descripci√≥n detallada puede ser encontrada en este repo de GitHub.\nLas caracter√≠sticas de audio extra√≠das y usadas en este Dashboard son las siguientes, con la traducci√≥n al Espa√±ol inidcada en los par√©ntesis:\n\nLoudness (Sonoridad): Est√° medida en decibelios relativos a escala completa (dBFS), lo que indica qu√© tan amplificado est√° el audio. Los valores normales son abajo de 0, de lo contrario suele ocurrir saturaci√≥n y distorsi√≥n en el audio.\n\nLas siguientes est√°n entre 0 y 1, y no tienen unidades\n\nAcousticness (Ac√∫stica): Intervalo que muestra qu√© tan ac√∫stica es una canci√≥n, valores cercanos a 0 es poco mientras que cercanos a 1 es mucho.\nDanceability (Bailable): Indicador calculado en base a propiedades como el tempo, estabilidad del ritmo, regularidad. Cercano a 0 indica que la canci√≥n es poco bailable mientras que cercano a 1 lo es mucho.\nEnergy (Energ√≠a): Intervalo que muestra qu√© tan energ√©tica se siente una canci√≥n, basado en propiedades como rango din√°mico o el timbre. El valor cercano a 0 se asocia a canciones no muy ruidosas, mientras que cercano a 1 es ruidosa.\nInstrumentalness (Instrumentaci√≥n): Indica si una canci√≥n sea instrumental, es decir si el valor es cercano a 1 es muy probable que sea instrumental mientras que cercano a 0 debe tener mucho contenido cantado.\nLiveness (En vivo): Indicador que se calcula en base a la detecci√≥n de audiencia en la canci√≥n, si es cercano a 1 es muy probable que la grabaci√≥n fue hecha en vivo.\nValence (Valencia): Medida que representa la sencaci√≥n de positivo o negativo de una canci√≥n. Un valor cercano a 1 indica un sonido positivo (alegre, animador) mientras que cercano a 0 se asocia a un sonido m√°s negativo (deprimente, furioso).\n\nEstas y otras caracter√≠sticas de audio pueden consultarse con m√°s detalle en la documentaci√≥n de la API Web de Spotify."
  },
  {
    "objectID": "proyectos/proyectos/2024-04-25-top-hits-features/index.html#descripci√≥n-general",
    "href": "proyectos/proyectos/2024-04-25-top-hits-features/index.html#descripci√≥n-general",
    "title": "Top Hits 1970-2019",
    "section": "",
    "text": "Para este proyecto de dashboard interactivo, se buscaba obtener patr√≥n en las caracter√≠sticas de audio de las canciones m√°s populares (Top Hits) desde 1970 a 2019. Esto con el objetivo de identificar la tendencia y ver c√≥mo podr√≠a entrarse a esas listas de reproducci√≥n de popularidad.\nPara la construcci√≥n del dashboard, se utiliz√≥ R, con la librer√≠a flexdashboard, agreg√°ndole shiny para la interactividad.\nEnlace al dashboard: https://hugojira.shinyapps.io/TopHits1970-2019/.\nEl dashboard cuenta con 2 pesta√±as principales que se ven de la siguiente manera:\n\n\n\nP√°gina de informaci√≥n general\n\n\n\n\n\n\nP√°gina interactiva con shiny\n\n\n\n\nLos datos fueron descargados desde la API Web de Spotify con c√≥digo en R, utilizando el wrapper Spotifyr. El c√≥digo y una descripci√≥n detallada puede ser encontrada en este repo de GitHub.\nLas caracter√≠sticas de audio extra√≠das y usadas en este Dashboard son las siguientes, con la traducci√≥n al Espa√±ol inidcada en los par√©ntesis:\n\nLoudness (Sonoridad): Est√° medida en decibelios relativos a escala completa (dBFS), lo que indica qu√© tan amplificado est√° el audio. Los valores normales son abajo de 0, de lo contrario suele ocurrir saturaci√≥n y distorsi√≥n en el audio.\n\nLas siguientes est√°n entre 0 y 1, y no tienen unidades\n\nAcousticness (Ac√∫stica): Intervalo que muestra qu√© tan ac√∫stica es una canci√≥n, valores cercanos a 0 es poco mientras que cercanos a 1 es mucho.\nDanceability (Bailable): Indicador calculado en base a propiedades como el tempo, estabilidad del ritmo, regularidad. Cercano a 0 indica que la canci√≥n es poco bailable mientras que cercano a 1 lo es mucho.\nEnergy (Energ√≠a): Intervalo que muestra qu√© tan energ√©tica se siente una canci√≥n, basado en propiedades como rango din√°mico o el timbre. El valor cercano a 0 se asocia a canciones no muy ruidosas, mientras que cercano a 1 es ruidosa.\nInstrumentalness (Instrumentaci√≥n): Indica si una canci√≥n sea instrumental, es decir si el valor es cercano a 1 es muy probable que sea instrumental mientras que cercano a 0 debe tener mucho contenido cantado.\nLiveness (En vivo): Indicador que se calcula en base a la detecci√≥n de audiencia en la canci√≥n, si es cercano a 1 es muy probable que la grabaci√≥n fue hecha en vivo.\nValence (Valencia): Medida que representa la sencaci√≥n de positivo o negativo de una canci√≥n. Un valor cercano a 1 indica un sonido positivo (alegre, animador) mientras que cercano a 0 se asocia a un sonido m√°s negativo (deprimente, furioso).\n\nEstas y otras caracter√≠sticas de audio pueden consultarse con m√°s detalle en la documentaci√≥n de la API Web de Spotify."
  },
  {
    "objectID": "proyectos/proyectos/2024-04-25-top-hits-features/index.html#resultados",
    "href": "proyectos/proyectos/2024-04-25-top-hits-features/index.html#resultados",
    "title": "Top Hits 1970-2019",
    "section": "Resultados",
    "text": "Resultados\nSe encontraron 3 tendencias principales analizando el cambio de las caracter√≠sticas de audio a trav√©s de los a√±os.\nLa primera tendencia, fue un aumento considerable de la sonoridad, siendo medido con el promedio de todas las canciones de cada a√±o. Se observa que a partir del a√±o 1990 la sonoridad sube, lo que hace referencia al fen√≥meno conocido como Loudness War (guerra del sonido), en el que se intentaba sonar cada vez m√°s fuerte en los masters de los discos. Se resume en la siguiente gr√°fica extra√≠da del dashboard:\n\n\n\nLoudness War a partir de 1990\n\n\nEn la segunda tendencia, se encontr√≥ que la duraci√≥n de las canciones se reduce considerablemente conforme pasan los a√±os. Con lo que actualmente las canciones con corta duraci√≥n son las que llegan a las listas de √©xitos. Esto, por ejemplo, puede observarse comparando los diagramas de caja de 1970 y 2019 extra√≠dos del dashboard:\n\n\n\n\nDuraci√≥n de las canciones m√°s populares de 1970\n\n\n\n\n\n\nDuraci√≥n de las canciones m√°s populares de 2019\n\n\nPara la tercera tendencia, se observa que la caracter√≠stica de audio Ac√∫stica va disminuyendo conforme pasan los a√±os. Esto es un reflejo del avance de la tecnolog√≠a e instrumentos y su incorporaci√≥n en la m√∫sica, pues recordemos que dicha caracter√≠stica de audio indica qu√© tan ac√∫stica es una canci√≥n."
  },
  {
    "objectID": "proyectos/proyectos/2024-04-25-top-hits-features/index.html#conclusiones",
    "href": "proyectos/proyectos/2024-04-25-top-hits-features/index.html#conclusiones",
    "title": "Top Hits 1970-2019",
    "section": "Conclusiones",
    "text": "Conclusiones\nEn conclusi√≥n, podemos ver que de las herramientas visuales dadas por un dashboard, se puede extraer informaci√≥n muy √∫til para tomar decisiones, en este caso en c√≥mo realizar la producci√≥n de una canci√≥n para tener m√°s probabilidad de llegar a los √©xitos.\nLes agradezco su tiempo de lectura, cualquier comentario es bienvenido."
  },
  {
    "objectID": "proyectos/2024-04-25-top-hits-features/index.html",
    "href": "proyectos/2024-04-25-top-hits-features/index.html",
    "title": "Top Hits 1970-2019",
    "section": "",
    "text": "Para este proyecto de dashboard interactivo, se buscaba obtener patr√≥n en las caracter√≠sticas de audio de las canciones m√°s populares (Top Hits) desde 1970 a 2019. Esto con el objetivo de identificar la tendencia y ver c√≥mo podr√≠a entrarse a esas listas de reproducci√≥n de popularidad.\nPara la construcci√≥n del dashboard, se utiliz√≥ R, con la librer√≠a flexdashboard, agreg√°ndole shiny para la interactividad.\nEnlace al dashboard: https://hugojira.shinyapps.io/TopHits1970-2019/.\nEl dashboard cuenta con 2 pesta√±as principales que se ven de la siguiente manera:\n\n\n\nP√°gina de informaci√≥n general\n\n\n\n\n\n\nP√°gina interactiva con shiny\n\n\n\n\nLos datos fueron descargados desde la API Web de Spotify con c√≥digo en R, utilizando el wrapper Spotifyr. El c√≥digo y una descripci√≥n detallada puede ser encontrada en este repo de GitHub.\nLas caracter√≠sticas de audio extra√≠das y usadas en este Dashboard son las siguientes, con la traducci√≥n al Espa√±ol inidcada en los par√©ntesis:\n\nLoudness (Sonoridad): Est√° medida en decibelios relativos a escala completa (dBFS), lo que indica qu√© tan amplificado est√° el audio. Los valores normales son abajo de 0, de lo contrario suele ocurrir saturaci√≥n y distorsi√≥n en el audio.\n\nLas siguientes est√°n entre 0 y 1, y no tienen unidades\n\nAcousticness (Ac√∫stica): Intervalo que muestra qu√© tan ac√∫stica es una canci√≥n, valores cercanos a 0 es poco mientras que cercanos a 1 es mucho.\nDanceability (Bailable): Indicador calculado en base a propiedades como el tempo, estabilidad del ritmo, regularidad. Cercano a 0 indica que la canci√≥n es poco bailable mientras que cercano a 1 lo es mucho.\nEnergy (Energ√≠a): Intervalo que muestra qu√© tan energ√©tica se siente una canci√≥n, basado en propiedades como rango din√°mico o el timbre. El valor cercano a 0 se asocia a canciones no muy ruidosas, mientras que cercano a 1 es ruidosa.\nInstrumentalness (Instrumentaci√≥n): Indica si una canci√≥n sea instrumental, es decir si el valor es cercano a 1 es muy probable que sea instrumental mientras que cercano a 0 debe tener mucho contenido cantado.\nLiveness (En vivo): Indicador que se calcula en base a la detecci√≥n de audiencia en la canci√≥n, si es cercano a 1 es muy probable que la grabaci√≥n fue hecha en vivo.\nValence (Valencia): Medida que representa la sencaci√≥n de positivo o negativo de una canci√≥n. Un valor cercano a 1 indica un sonido positivo (alegre, animador) mientras que cercano a 0 se asocia a un sonido m√°s negativo (deprimente, furioso).\n\nEstas y otras caracter√≠sticas de audio pueden consultarse con m√°s detalle en la documentaci√≥n de la API Web de Spotify."
  },
  {
    "objectID": "proyectos/2024-04-25-top-hits-features/index.html#descripci√≥n-general",
    "href": "proyectos/2024-04-25-top-hits-features/index.html#descripci√≥n-general",
    "title": "Top Hits 1970-2019",
    "section": "",
    "text": "Para este proyecto de dashboard interactivo, se buscaba obtener patr√≥n en las caracter√≠sticas de audio de las canciones m√°s populares (Top Hits) desde 1970 a 2019. Esto con el objetivo de identificar la tendencia y ver c√≥mo podr√≠a entrarse a esas listas de reproducci√≥n de popularidad.\nPara la construcci√≥n del dashboard, se utiliz√≥ R, con la librer√≠a flexdashboard, agreg√°ndole shiny para la interactividad.\nEnlace al dashboard: https://hugojira.shinyapps.io/TopHits1970-2019/.\nEl dashboard cuenta con 2 pesta√±as principales que se ven de la siguiente manera:\n\n\n\nP√°gina de informaci√≥n general\n\n\n\n\n\n\nP√°gina interactiva con shiny\n\n\n\n\nLos datos fueron descargados desde la API Web de Spotify con c√≥digo en R, utilizando el wrapper Spotifyr. El c√≥digo y una descripci√≥n detallada puede ser encontrada en este repo de GitHub.\nLas caracter√≠sticas de audio extra√≠das y usadas en este Dashboard son las siguientes, con la traducci√≥n al Espa√±ol inidcada en los par√©ntesis:\n\nLoudness (Sonoridad): Est√° medida en decibelios relativos a escala completa (dBFS), lo que indica qu√© tan amplificado est√° el audio. Los valores normales son abajo de 0, de lo contrario suele ocurrir saturaci√≥n y distorsi√≥n en el audio.\n\nLas siguientes est√°n entre 0 y 1, y no tienen unidades\n\nAcousticness (Ac√∫stica): Intervalo que muestra qu√© tan ac√∫stica es una canci√≥n, valores cercanos a 0 es poco mientras que cercanos a 1 es mucho.\nDanceability (Bailable): Indicador calculado en base a propiedades como el tempo, estabilidad del ritmo, regularidad. Cercano a 0 indica que la canci√≥n es poco bailable mientras que cercano a 1 lo es mucho.\nEnergy (Energ√≠a): Intervalo que muestra qu√© tan energ√©tica se siente una canci√≥n, basado en propiedades como rango din√°mico o el timbre. El valor cercano a 0 se asocia a canciones no muy ruidosas, mientras que cercano a 1 es ruidosa.\nInstrumentalness (Instrumentaci√≥n): Indica si una canci√≥n sea instrumental, es decir si el valor es cercano a 1 es muy probable que sea instrumental mientras que cercano a 0 debe tener mucho contenido cantado.\nLiveness (En vivo): Indicador que se calcula en base a la detecci√≥n de audiencia en la canci√≥n, si es cercano a 1 es muy probable que la grabaci√≥n fue hecha en vivo.\nValence (Valencia): Medida que representa la sencaci√≥n de positivo o negativo de una canci√≥n. Un valor cercano a 1 indica un sonido positivo (alegre, animador) mientras que cercano a 0 se asocia a un sonido m√°s negativo (deprimente, furioso).\n\nEstas y otras caracter√≠sticas de audio pueden consultarse con m√°s detalle en la documentaci√≥n de la API Web de Spotify."
  },
  {
    "objectID": "proyectos/2024-04-25-top-hits-features/index.html#resultados",
    "href": "proyectos/2024-04-25-top-hits-features/index.html#resultados",
    "title": "Top Hits 1970-2019",
    "section": "Resultados",
    "text": "Resultados\nSe encontraron 3 tendencias principales analizando el cambio de las caracter√≠sticas de audio a trav√©s de los a√±os.\nLa primera tendencia, fue un aumento considerable de la sonoridad, siendo medido con el promedio de todas las canciones de cada a√±o. Se observa que a partir del a√±o 1990 la sonoridad sube, lo que hace referencia al fen√≥meno conocido como Loudness War (guerra del sonido), en el que se intentaba sonar cada vez m√°s fuerte en los masters de los discos. Se resume en la siguiente gr√°fica extra√≠da del dashboard:\n\n\n\nLoudness War a partir de 1990\n\n\nEn la segunda tendencia, se encontr√≥ que la duraci√≥n de las canciones se reduce considerablemente conforme pasan los a√±os. Con lo que actualmente las canciones con corta duraci√≥n son las que llegan a las listas de √©xitos. Esto, por ejemplo, puede observarse comparando los diagramas de caja de 1970 y 2019 extra√≠dos del dashboard:\n\n\n\n\nDuraci√≥n de las canciones m√°s populares de 1970\n\n\n\n\n\n\nDuraci√≥n de las canciones m√°s populares de 2019\n\n\nPara la tercera tendencia, se observa que la caracter√≠stica de audio Ac√∫stica va disminuyendo conforme pasan los a√±os. Esto es un reflejo del avance de la tecnolog√≠a e instrumentos y su incorporaci√≥n en la m√∫sica, pues recordemos que dicha caracter√≠stica de audio indica qu√© tan ac√∫stica es una canci√≥n."
  },
  {
    "objectID": "proyectos/2024-04-25-top-hits-features/index.html#conclusiones",
    "href": "proyectos/2024-04-25-top-hits-features/index.html#conclusiones",
    "title": "Top Hits 1970-2019",
    "section": "Conclusiones",
    "text": "Conclusiones\nEn conclusi√≥n, podemos ver que de las herramientas visuales dadas por un dashboard, se puede extraer informaci√≥n muy √∫til para tomar decisiones, en este caso en c√≥mo realizar la producci√≥n de una canci√≥n para tener m√°s probabilidad de llegar a los √©xitos.\nLes agradezco su tiempo de lectura, cualquier comentario es bienvenido."
  },
  {
    "objectID": "posts/post-de-bienvenida/index.html",
    "href": "posts/post-de-bienvenida/index.html",
    "title": "Post de bienvenida",
    "section": "",
    "text": "¬°Hola! Les doy la bienvenida a este primer post. Este blog en realidad es un jard√≠n digital, esto quiere decir que es un espacio para almacenar mis notas de aprendizaje, as√≠ c√≥mo tambi√©n entradas de blog usuales. Este contenido se ir√° mejorando y actualizando con el tiempo, nada es est√°tico.\nPr√≥ximamente estar√© populando este sitio con entradas de blog y notas, con contenido relacionado a la ciencia de datos que espero tambi√©n les sea de utilidad como a m√≠.\nPara la creaci√≥n del primer esqueleto de este sitio web, me inspir√© en los sitios de Alison Hill, Beatriz Milz, Mike Mahoney, as√≠ como en la documentaci√≥n de Quarto.\nSin m√°s que decir, les invito a explorar el blog."
  },
  {
    "objectID": "posts/el-poder-de-la-regresion-lineal/index.html#regresi√≥n",
    "href": "posts/el-poder-de-la-regresion-lineal/index.html#regresi√≥n",
    "title": "La Regresi√≥n Lineal en Machine Learning",
    "section": "Regresi√≥n",
    "text": "Regresi√≥n\nLa regresi√≥n es una t√©cnica de modelaci√≥n proveniente de la estad√≠stica para modelar y analizar la relaci√≥n entre variables. Se tiene una variable \\(y\\) llamada independiente, la cual se modela en funci√≥n de otra variable \\(x\\) llamada independiente, que bien se puede tener una \\(n\\) cantidad de variables \\(x_1, x_2, ... , x_n\\) como se ver√° m√°s adelante.\nEs importante mencionar que, para evitar confusiones con la noci√≥n de indepencia de probabilidad, se prefiere nombrar a \\(y\\) como la variable respuesta y a \\(x\\) como las variables predictoras. En el contexto de machine learning, a las variables \\(x\\) se les denomina caracter√≠sticas (features) y a \\(y\\) etiquetas (labels).\nEl tipo de datos con los que la regresi√≥n trabaja son num√©ricos (flotantes), y vienen en duplas \\((x_1, y_1), ... , (x_m, y_m)\\), donde \\(m\\) es la cantidad de datos que tengamos, es decir, cardinalidad del conjunto de datos. N√≥tese que se pueden tener m√∫ltiples caracter√≠sticas \\(x\\).\nPara trabajar con regresi√≥n, si la dimensi√≥n lo permite, suele hacerse un diagrama de dispersi√≥n que es una gr√°fica de los datos representados como puntos. Por ejemplo, consideremos de \\(x\\) e \\(y\\), con 15 registros (\\(m = 15\\)), resultando el siguiente diagrama de dispersi√≥n\n\n\n\n\n\n\n\n\n\nEn este caso, se ve claramente que los puntos tienen una tendencia lineal, esto es, que se les puede ajustar una recta para modelarlos con un error considerablemente bajo. La ecuaci√≥n de la recta, es \\(y = ax + b\\), con \\(a\\) la pendiente y \\(b\\) la intersecci√≥n en el eje \\(y\\). Dicha ecuaci√≥n discribir√≠a adecuadamente ese comportamiento, en las siguientes secciones veremos c√≥mo ajustarla a los datos para encontrar los valores adecuados de \\(a\\) y \\(b\\).\nEn este diagrama en part√≠cular, los puntos fueron simulados de una recta \\(y = 2x\\), a la que se les agreg√≥ un error \\(\\varepsilon\\) que tiene una distribuci√≥n normal con media \\(\\mu = 0\\) y desviaci√≥n estandar \\(\\sigma = 1.5\\). Teniendo as√≠ los par√°metros de la recta \\(a = 2\\) y \\(b = 0\\)\n\nObtenci√≥n de datos para regresi√≥n\nHay 3 maneras principales para obtener datos que puedan usarse en regresi√≥n, los cuales se listan a continuaci√≥n\n\nEstudio retrospectivo: Aqu√≠ se tienen datos que ya fueron capturados en el pasado, y no necesariamente con una intenci√≥n de ser analizados cient√≠ficamente, por lo que pueden no estar en la forma m√°s ideal posible. Los datos y sus resultados ya se tienen, no se puede intervenir ni cambiar nada para mejorar la toma de datos. Este tipo de estudio es muy com√∫n en machine learning y big data, donde obtienes datos con ciertas caracter√≠sticas y ya etiquetados que fueron obtenidos en el pasado.\nEstudio observacional: Como su nombre puede indicarlo, en este tipo de estudio solamente se observa el proceso o experimento que quiere analizarse, interviniendo m√≠nimamente solamente para la captura de los datos. Por temas √©ticos, este estudio es muy usado en el √°rea m√©dica, pues de lo contrario se estar√≠a imponiendo sobre las personas las condiciones que deben tener (como empezar a fumar, por ejemplo).\nDise√±o experimental: Este tipo de estudio posee m√°s estrategia. Aqu√≠ se manipulan las variables predictoras (o caracter√≠sticas) de acuerdo a un dise√±o experimental, para tener ciertos valores y observar su efecto en la variable respuesta."
  },
  {
    "objectID": "posts/el-poder-de-la-regresion-lineal/index.html#regresi√≥n-lineal-simple",
    "href": "posts/el-poder-de-la-regresion-lineal/index.html#regresi√≥n-lineal-simple",
    "title": "La Regresi√≥n Lineal en Machine Learning",
    "section": "Regresi√≥n Lineal Simple",
    "text": "Regresi√≥n Lineal Simple\nLa regresi√≥n lineal simple es √∫til cuando existe una tendencia lineal en los datos. Aunque veremos m√°s adelante que va m√°s all√° de lo lineal. Se le llama simple porque solo hay una caracter√≠stica \\(x\\), y se modela como\n\\[y = w_0 + w_1 x + \\varepsilon\\] La cual puede verse como la ecuaci√≥n de regresi√≥n lineal poblacional, no obstante, en regresi√≥n lineal trabajamos con \\(m\\) n√∫mero de datos, entonces para cada punto se tendr√° una ecuaci√≥n de regresi√≥n lineal muestral\n\\[y_i = w_0 + w_1 x_i + \\varepsilon_i \\quad i=1, 2, 3, ..., m\\]\nCompar√°ndo con la ecuaci√≥n de la recta, podemos interpretar los par√°metros. Se tiene que \\(w_1\\) es el cambio en la media de la distribuci√≥n de \\(y\\) producido por un cambio de unidad en \\(x\\). Mientras que \\(w_0\\) simplemente es la media de la distribuci√≥n de \\(y\\) cuando \\(x = 0\\), si el dominio de \\(x\\) no incluye a \\(0\\) entonces el par√°metro \\(w_0\\) no tiene interpretaci√≥n pr√°ctica.\nPor otro lado, \\(\\varepsilon\\) son los errores o ruido que se tiene en cada predicci√≥n de \\(y\\). Se asumen que dichos errores tienen una distribuci√≥n normal con media igual a cero \\(\\mu = 0\\) y una varianza desconicida \\(\\sigma^2\\). Es importante notar que, esa varianza es la misma en todos los puntos de la regresi√≥n, lo que se conoce como homocedasticidad. A los errores \\(\\varepsilon\\) se les llama residuales pues se definen como \\(y - \\hat{y}\\).\nAhora bien, retomando el diagrama de dispersi√≥n que se vio anteriormente, el cual tiene proviene de la recta \\(y = 2x\\) con un error \\(\\varepsilon\\) con media \\(0\\) y varianza \\(\\sigma^2 = 1.5^2\\). Se le puede ajustar una recta de regresi√≥n lineal y se ver√≠a como la siguiente figura\n\n\n\n\n\n\n\n\n\nObteniendo los coeficientes estimados \\(\\hat{w_0} = 0.3045\\) y \\(\\hat{w_1} = 2.0274\\). Teniendo as√≠ una ecuaci√≥n de regresi√≥n lineal con la que podemos hacer predicciones\n\\[ \\hat{y} = 0.3045 + 2.0274 x\\] Notemos que los par√°metros reales de donde provienen los datos son \\(0\\) y \\(2\\), pero por la naturaleza estoc√°stica de los errores, los estimados no coinciden con exactitud. Entre mayor sea el n√∫mero de datos \\(m\\), los par√°metros estimados ser√°n m√°s cercanos a los originales."
  },
  {
    "objectID": "posts/el-poder-de-la-regresion-lineal/index.html#regresi√≥n-lineal-m√∫ltiple",
    "href": "posts/el-poder-de-la-regresion-lineal/index.html#regresi√≥n-lineal-m√∫ltiple",
    "title": "La Regresi√≥n Lineal en Machine Learning",
    "section": "Regresi√≥n Lineal M√∫ltiple",
    "text": "Regresi√≥n Lineal M√∫ltiple\nPara el caso en que la variable respuesta \\(y\\) est√° realcionada con \\(n\\) variables regresoras, se tiene el modelo de la regresi√≥n lineal m√∫ltiple\n\\[ y = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n + \\varepsilon \\] As√≠, a los par√°metros \\(w_j \\quad j = 0, 1, 2, ..., n\\) se les llama coeficientes de regresi√≥n. Este modelo, al ser de multivariable, describe un hiperplano en el espacio \\(n\\)-dimensional de las variables regresoras \\(x\\).\nHaciendo sentido de los par√°metros. El par√°metro \\(w_j\\) representa el cambio esperado en la respuesta \\(y\\) por unidad de cambio en \\(x_j\\), cuando todo las dem√°s variables regresoras \\(x_i(i\\ne j)\\) se mantienen constantes.Por otra parte, el par√°metro \\(w_0\\) es la intersecci√≥n del hiperplano en \\(y\\), si el dominio de los datos incluye \\(x_1 = x_2 = ... = x_n = 0\\) entonces \\(w_0\\) es la media de \\(y\\) cuando \\(x_1 = x_2 = ... = x_n = 0\\); de lo contrario, \\(w_0\\) no tiene interpretaci√≥n f√≠sica.\nConcluyendo, en el modelo de regresi√≥n simple y m√∫ltiple, la linealidad se tiene en los par√°metros. Esto extiende m√°s all√° el concepto de linealidad aunque las variables regresoras \\(x\\) no sean lineales, esto se ver√° a continuaci√≥n en la secci√≥n de regresi√≥n polinomial."
  },
  {
    "objectID": "posts/el-poder-de-la-regresion-lineal/index.html#regresi√≥n-polinomial",
    "href": "posts/el-poder-de-la-regresion-lineal/index.html#regresi√≥n-polinomial",
    "title": "La Regresi√≥n Lineal en Machine Learning",
    "section": "Regresi√≥n Polinomial",
    "text": "Regresi√≥n Polinomial\nYa vimos que una regresi√≥n lineal de \\(n\\) variables regresoras genera un hiperplano \\(n\\)-dimensional. Ahora bien, cualquier modelo que es lineal en los par√°metros \\(w\\)‚Äôs es un modelo de regresi√≥n lineal, independientemente de la forma de la superficie que genere.\nComo su nombre lo indica, la regresi√≥n polinomial de una variable \\(x\\) se define a partir de los polinomios, y se modela de la siguiente forma\n\\[ y = w_0 + w_1x + w_2x^2 + ... + w_nx^n + \\varepsilon\\] La cual es lineal en los par√°metros, por lo que se puede trabajar como una regresi√≥n lineal m√∫ltiple y usar la misma metodolog√≠a. Para hacer esto, podemos definir \\(x_j = x^j \\quad j=1, 2, 3, ... , n\\). Por ejemplo, para el caso de un polinomio de grado 2 con \\(y = w_0 + w_1x + w_2x^2 + \\varepsilon\\), tendr√≠amos \\(x_1 = x, x_2 = x^2\\) y la regresi√≥n se convertir√≠a en una lineal m√∫ltiple\n\\[ y = w_0 + w_1x_1 + w_2x_2 + \\varepsilon\\] Con este tipo de regresi√≥n, hay que ser muy cuidadosos. Ya que un polinomio de grado considerablemente alto puede apr√≥ximar muy bien una curva no lineal, sin embargo, hay que tratar de hacer transformaciones y mantener el grado lo m√°s bajo posible para evitar un sobreajuste del modelo (empezar a generalizar el error mismo)."
  },
  {
    "objectID": "posts/el-poder-de-la-regresion-lineal/index.html#m√≠nimos-cuadrados-ordinarios-como-funci√≥n-de-costo",
    "href": "posts/el-poder-de-la-regresion-lineal/index.html#m√≠nimos-cuadrados-ordinarios-como-funci√≥n-de-costo",
    "title": "La Regresi√≥n Lineal en Machine Learning",
    "section": "M√≠nimos cuadrados ordinarios como funci√≥n de costo",
    "text": "M√≠nimos cuadrados ordinarios como funci√≥n de costo\nEn machine learning, la base para ajustar los par√°metros a nuestros datos, lo que se conoce como aprender, radica en la optimizaci√≥n de una funci√≥n objetivo. Dicho de otra manera, es minimizar una funci√≥n de costo, donde dicha funci√≥n ser√≠a una medida del error en las predicciones del modelo.\nEn la regresi√≥n lineal no es la excepci√≥n. Es muy conocido el m√©todo de m√≠nimos cuadrados para ajustar los par√°metros, donde se define la funci√≥n de costo \\(J\\) de m√≠nimos cuadrados ordinarios (MCO) de la siguiente manera\n\\[ J(y, \\hat{y}) = \\frac{1}{m} \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2 \\] Donde el factor \\(\\frac{1}{m}\\) se multiplica como una especie de promedio en todos los datos. Algunas referencias tambi√©n manejan multiplicarlo por \\(\\frac{1}{2}\\) por conveniencia algebraica.\nNotemos que la diferencia \\(y_i - \\hat{y}_i\\) es la distancia que hay desde un punto \\(\\hat{y} (x_i)\\) en la recta de la predicci√≥n \\(\\hat{y}\\) hasta el correspondiente punto \\(y(x_i)\\) en la recta original \\(y\\) de donde provienen los datos. Est√° al cuadrado para evitar que las sumas de esos t√©rminos puedan dar cero. As√≠, podemos interpretarlo gr√°ficamente como unos cuadrados en dichos puntos \\(x_i\\), considerando el mismo diagrama de dispersi√≥n de este post, se ver√≠a\n\n\n\n\n\n\n\n\n\nN√≥tese que los ejes fueron escalados para una mejor visualizaci√≥n. Ahora, al ajustar los par√°metros \\(w\\) minimizando la funci√≥n de costo de MCO, lo que se est√° haciendo gr√°ficamente es ajustar la recta de predicci√≥n de tal manera que se minimize el √°rea de esos cuadrados; de ah√≠ el nombre de este m√©todo.\nPara minimizar dicha funci√≥n, es mejor expresarla en t√©rmino de los par√°metros \\(w\\), para esto, en el caso de una variable regresora sustituimos \\(\\hat{y} = w_0 + w_1 x\\) en la funci√≥n de costo de MCO, as√≠\n\\[ J(w_0, w_1) = \\frac{1}{m} \\sum_{i=1}^{m} (y_i - w_0 - w_1 x_i)^2 \\] En la siguiente secci√≥n veremos c√≥mo minimizarla, ejemplificando el caso con una sola variable regresora \\(x\\)."
  },
  {
    "objectID": "posts/el-poder-de-la-regresion-lineal/index.html#ajuste-de-los-par√°metros-minimizando-la-funci√≥n-de-costo",
    "href": "posts/el-poder-de-la-regresion-lineal/index.html#ajuste-de-los-par√°metros-minimizando-la-funci√≥n-de-costo",
    "title": "La Regresi√≥n Lineal en Machine Learning",
    "section": "Ajuste de los par√°metros minimizando la funci√≥n de costo",
    "text": "Ajuste de los par√°metros minimizando la funci√≥n de costo\nHay 3 maneras principales de minimizar la funci√≥n de costo de m√≠nimos cuadrados ordinarios (MCO) \\(J\\). El m√©todo cl√°sico, utiliza c√°lculo diferencial y el criterio de las derivadas para minimizar. La segunda, utiliza el m√©todo de la m√°xima verosimilitud, un concepto estad√≠stico. Se encuentra matem√°ticamente que ambos m√©todos llegan a la misma forma soluci√≥n. La tercera, usa el m√©todo m√°s utilizado en machine learning, el del gradiente descendiente (gradient descent).\nMi objetivo de este post no es hacer demostraciones matem√°ticas ni c√°lculos completos, sino que ilustrar la metodolog√≠a, as√≠ que nos enfocaremos en el m√©todo cl√°sico y el de gradiente descendiente. Veamos a m√°s detalle\n\nEl m√©todo de m√≠nimos cuadrados cl√°sico\nPara minimizar la funci√≥n de costo, los estimados de los par√°metros \\(w_0\\), \\(w_1\\), que son \\(\\hat{w_0}\\), \\(\\hat{w_1}\\) respectivamente, deben satisfacer\n\\[ \\frac{\\partial J}{\\partial w_0} \\bigg|_{\\hat{w_0}, \\hat{w_1}}= - \\frac{2}{m} \\sum_{i=1}^{m} (y_i - \\hat{w_0} - \\hat{w_1} x_i) = 0\\] y\n\\[ \\frac{\\partial J}{\\partial w_1} \\bigg|_{\\hat{w_0}, \\hat{w_1}} = - \\frac{2}{m} \\sum_{i=1}^{m} (y_i - \\hat{w_0} - \\hat{w_1} x_i) x_i = 0\\]\nde acuerdo al criterio de la primer derivada, √©sta debe ser 0 para que sea un m√≠nimo.\nSimplificando las expresiones, separando las sumatorias y resolviendo para \\(\\hat{w_0}\\), \\(\\hat{w_1}\\), se tienen las soluciones a los par√°metros estimados que minimizan la funci√≥n de MCO\n\\[ \\hat{w_0} =  \\bar{y} - \\hat{w_1} \\bar{x}\\] y\n\\[ \\hat{w_1} = \\frac{ \\sum_{i=1}^{m} y_i x_i - \\frac{(\\sum_{i=1}^{m} y_i)(\\sum_{i=1}^{m} x_i)}{n} }{ \\sum_{i=1}^{m} x_i^2 - \\frac{(\\sum_{i=1}^{m}x_i)^2}{m} } \\]\ndonde \\(\\bar{x} = \\frac{1}{m}  \\sum_{i=1}^{m} x_i\\) y \\(\\bar{y} = \\frac{1}{m}  \\sum_{i=1}^{m} y_i\\) son los promedios de \\(x_i\\), \\(y_i\\), respectivamente.\nPara el caso multivariable, se tiene una soluci√≥n de los par√°metros estimados \\(W^{*}\\) en forma matricial, dado por\n\\[ W^{*} = (X^T X)^{-1} X^{T} Y \\] Donde \\(X\\) es la matriz de dise√±o, la cual es de tama√±o \\(m \\times (n + 1)\\). Donde \\(m\\) es el n√∫mero de datos y \\(n\\) el n√∫mero de variables regresoras. Se construye poniendo en filas los vectores de datos \\(x_j = (1, x_{j1}, x_{j2}, ..., x_{jn})\\), donde \\(j = 1, 2, 3, ..., m\\), en todos los datos. Por otro lado, \\(Y\\) es la matriz (o vector) de tama√±o \\(m \\times 1\\), que se construye poniendo todos los puntos \\(y\\) de donde se har√° la regresi√≥n.\nAs√≠,\n\\[ X = \\begin{pmatrix}\n1 & x_{11} & x_{12} & ... & x_{1n} \\\\\n1 & x_{21} & x_{22} & ... & x_{2n} \\\\\n&  & \\vdots &  &  \\\\\n1 & x_{m1} & x_{m2} & ... & x_{mn}\n\\end{pmatrix}  \\]\n\\[ Y = \\begin{pmatrix}\ny_1 \\\\\ny_2 \\\\\ny_3 \\\\\n\\vdots \\\\\ny_m\n\\end{pmatrix}  \\]\n\n\nEl m√©todo del gradiente descendiente\nRetomando conceptos de c√°lculo diferencial para una variable, la derivada de una funci√≥n \\(f\\) en un punto dado \\(x_0\\), nos da la pendiende de la recta tangente a la curva en ese punto. En otras palabras, esa pendiente puede interprestarse como la raz√≥n de cambio de la funci√≥n en ese punto y el signo indica hacia donde est√° cambiando. Extendiendo este concepto hacia varias variables, el gradiente de una funci√≥n \\(f\\), denotado como \\(\\nabla f\\), es un vector que indica hacia d√≥nde la funci√≥n tiene el mayor incremento, y su magnitud es la raz√≥n de cambio.\nEl m√©todo del gradiente descendiente, las derivadas son respecto a los par√°metros \\(w\\) que queremos ajustar, y como su nombre lo indica, hace uso del gradiente para minimizar la funci√≥n de costo \\(J(w)\\). Para el caso de la regresi√≥n lineal, el algoritmo se define de la siguiente manera\n\\[ w_i^{(\\tau + 1)} := w_i^{(\\tau)} - \\alpha \\frac{\\partial J(w)}{\\partial w_i}\\] donde \\(J(w)\\) esl funci√≥n de costo de m√≠nimos cuadrados ordinarios. La derivada est√° multiplicada por un \\(-1\\), lo que asegura que se mueva en la direcci√≥n de menor crecimiento, tenemos que \\(\\alpha\\) es una constante llamada tasa de aprendizaje, controla qu√© tanto se ajusta el valor de \\(w_i\\). En machine learning, \\(\\alpha\\) suele tomar valores muy peque√±os, como \\(0.01, 0.001,\\) etc. Por otro lado, \\(\\tau\\) es un n√∫mero entero que indica en la iteraci√≥n que estamos del algoritmo.\nAs√≠, se estar√°n ajustando los par√°metros \\(w_i\\) de manera iterativa hasta que se cumpla un cierto criterio o se alcance una tolerancia del error. Cabe se√±alar que en la primera iteraci√≥n, los par√°metros se inicializan de manera aleatoria.\nPor lo tanto, para el caso de regresi√≥n lineal simple, si utilizamos el m√©todo del gradiente descendiente tendriamos lo siguiente\n\\[ w_0^{(\\tau + 1)} := w_0^{(\\tau)} + \\alpha \\frac{2}{m} \\sum_{i=1}^{m} (y_i - \\hat{y_i})\\] \\[ w_1^{(\\tau + 1)} := w_1^{(\\tau)} + \\alpha \\frac{2}{m} \\sum_{i=1}^{m} (y_i - \\hat{y_i}) x_i\\] Donde \\(\\hat{y_i}\\) son las predicciones en la iteraci√≥n \\(\\tau\\), o sea antes de hacer la actualizaci√≥n de los par√°metros.\n\n\n¬øCu√°l usar de los 2 m√©todos?\nLa respuesta r√°dica en la complejidad computacional y el tiempo que toman los c√°lculos. En machine learning se trabajan con datos demasiado grandes, m√°s de lo que se sol√≠a hacer en la estad√≠stica convencional, por lo tanto, la forma matricial de obtener los par√°metros puede llegar a ser muy costosa computacionalmente hablando si se tienen muchos datos y muchas caracter√≠sticas (variables regresoras). As√≠, es m√°s conveniente utilizar el m√©todo del gradiente descendiente, que en la pr√°ctica suele ser m√°s eficiente."
  },
  {
    "objectID": "posts/el-poder-de-la-regresion-lineal/index.html#m√©todos-de-regularizaci√≥n",
    "href": "posts/el-poder-de-la-regresion-lineal/index.html#m√©todos-de-regularizaci√≥n",
    "title": "La Regresi√≥n Lineal en Machine Learning",
    "section": "M√©todos de regularizaci√≥n",
    "text": "M√©todos de regularizaci√≥n\nLa regularizaci√≥n en regresi√≥n lineal, es para reducir la complejidad del modelo evitando as√≠ un sobreajuste. Esto se logra a√±adiendo un t√©rmino a la funci√≥n de costo de MCO, lo que acota los par√°metros evitando que crezcan demasiado. Hay 2 casos, Ridge y Lasso, como se ver√° a continuaci√≥n.\n\nRegresi√≥n Lineal Ridge: limitando el tama√±o de los par√°metros\nLa regresi√≥n ridge hace que los par√°metros \\(w_i\\) se encojan, es decir, que no tomen valores tan grandes. Se define con la siguiente funci√≥n de costo \\(J_R(w)\\)\n\\[ J_R(W) = \\frac{1}{m} \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{n} w_j^2 \\] Sumando as√≠ el t√©rmino \\(\\lambda \\sum_{j=1}^{n} w_j^2\\) de penalizaci√≥n. Donde \\(\\lambda\\) controla que tanto se encojen los par√°metros, entre m√°s grande es el valor de \\(\\lambda\\) est√°n m√°s l√≠mitados los par√°metros. Se tiene que cuando \\(\\lambda \\xrightarrow{}{} \\infty\\), los par√°metros \\(w_i\\) tienden a \\(0\\).\nNotemos que el par√°metro \\(w_0\\) no se incluye, ya que solo se toma en cuenta \\(w_1, w_2, ..., w_n\\). Eso es porque \\(w_0\\), como ya vimos, es el valor promedio de \\(y\\) cuando \\(x_1=x_2=...=x_n=0\\). No ser√≠a conveniente imponer restricciones o encoger dicho par√°metro entonces.\nA la regresi√≥n ridge tambi√©n se le conoce como regularizaci√≥n L2, debido a que el t√©rmino \\(\\sum_{j=1}^{n} w_j^2\\) es la norma L2 del vector de par√°metros \\(w = (w_1, w_2, ..., w_n)\\).\n\n\nRegresi√≥n Lineal Lasso: un selector de caracter√≠sticas\nLa regresi√≥n lasso act√∫a como un selector de caracter√≠sticas, ya que impone que algunos de los par√°metros \\(w_i\\) sean igual a \\(0\\). Se define con la siguiente funci√≥n de costo \\(J_l(w)\\)\n\\[ J_l(W) = \\frac{1}{m} \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{n} |w_j| \\] Sumando as√≠ el t√©rmino \\(\\lambda \\sum_{j=1}^{n} |w_j|\\) de penalizaci√≥n. Similarmente, lasso tambi√©n hace que los par√°metros se encojan hac√≠a 0, pero a diferencia de ridge, si \\(\\lambda\\) es suficientemente grande algunos par√°metros \\(w_i\\) van a ser \\(0\\). As√≠, la variable regresora (o caracter√≠stica) \\(x_i\\) asociado al par√°metro \\(w_i\\) no estar√° involucrada en la ecuaci√≥n de regresi√≥n, por esta raz√≥n lasso es un selector de caracter√≠sticas.\nIgual que en el caso de regresi√≥n ridge, el par√°metro \\(w_0\\) no se incluye, ya que no ser√≠a conveniente imponer restricciones o hacer 0 el par√°metro \\(w_0\\), siendo que es es el valor promedio de \\(y\\) cuando \\(x_1=x_2=...=x_n=0\\).\nA la regresi√≥n lasso tambi√©n se le conoce como regularizaci√≥n L1, debido a que el t√©rmino \\(\\sum_{j=1}^{n} |w_j|\\) es la norma L1 del vector de par√°metros \\(w = (w_1, w_2, ..., w_n)\\).\n\n\nInterpretaci√≥n geom√©trica de ridge y lasso\nSurge la pregunta, ¬øPor qu√© la regresi√≥n ridge encoje los coeficientes pero la regresi√≥n lasso fuerza algunos de ellos a ser \\(0\\)? Para contestar esto, encontr√© que la mejor manera es gr√°ficamente.\nLa funci√≥n de costo de m√≠nimos cuadrados ordinarios MCO, es una funci√≥n convexa, por lo que es posible minimizarla. Ahora bien, si la graficamos en el espacio de los par√°metros al minimizar se pueden probar todos las combinaciones de par√°metros, sin embargo, al a√±adir un t√©rmino de penalizaci√≥n como en ridge y lasso, se hace una constricci√≥n en el espacio en el que puede bucarse el m√≠nimo. Veamos la siguiente figura (extra√≠da del libro Introduction to Statistical Learning)\n\n\n\nCurvas de nivel de la funci√≥n de costo de MCO en rojo y las constricciones de ridge y lasso en azul. Graficadas en el espacio de 2 par√°metros \\(w_1\\) y \\(w_2\\). El m√≠nimo se encuentra en \\(\\hat{w}\\).\n\n\nEn este caso, es una regresi√≥n en 2 dimensiones con \\(x_1\\), \\(x_2\\) y las gr√°ficas est√°n hechas en el espacio de los par√°metros \\(w_1\\), \\(w_2\\). Las l√≠neas rojas son las curvas de nivel de la funci√≥n de costo de MCO, y las √°reas s√≥lidas azules son las regiones de constricciones impuestas en los par√°metros \\(w_1\\), \\(w_2\\) por los t√©rminos de penalizaci√≥n de lasso y ridge, respectivamente.\nPara el caso de ridge, tenemos la constricci√≥n el c√≠rculo \\(w_1^2 + w_2^2 \\le s\\), y para lasso el rombo \\(|w_1| + |w_2| \\le s\\). El punto de minimizaci√≥n, donde est√°n los mejores par√°metros estimados, es la intersecci√≥n m√°s pronta entre la curva de nivel de MCO y el √°rea de constricci√≥n. Podemos observar claramente que para lasso, este punto de intersecci√≥n m√°s pronto se encuentra en la punta del rombo, es decir, sobre un eje donde el par√°metro \\(w_2 = 0\\); mientras que para ridge, el punto se encuentra sobre la circunferencia fuera de los ejes, por los que los par√°metros estar√°n encojidos por la constricci√≥n pero no ser√°n 0. Esto se extiende a cualquier dimensi√≥n, para \\(n \\gt 2\\), el rombo se convierte en un politopo mientras que el c√≠rculo en una hiperesfera. Entonces para el politopo siempre habr√° esquinas donde los par√°metros sean 0."
  },
  {
    "objectID": "posts/el-poder-de-la-regresion-lineal/index.html#conclusiones",
    "href": "posts/el-poder-de-la-regresion-lineal/index.html#conclusiones",
    "title": "La Regresi√≥n Lineal en Machine Learning",
    "section": "Conclusiones",
    "text": "Conclusiones\nEn este post, se vi√≥ c√≥mo se define la regresi√≥n lineal en una y varias variables, y los supuestos que se deben cumplir para su utilizaci√≥n. Se explic√≥ c√≥mo minimizar su funci√≥n de costo de m√≠nimos cuadrados ordinarios en la manera cl√°sica y con el m√©todo de gradiente descendiente. Tambi√©n, se vieron los m√©todos de regularizaci√≥n de regresi√≥n ridge y lasso, y c√≥mo influyen en los par√°metros encoji√©ndolos o haciendo algunos de ellos \\(0\\), respectivamente.\nComo trabajo futuro, queda llevar este conocimiento a implementaci√≥n en c√≥digo, para ver su funcionalidad pr√°ctica. Existen librer√≠as para hacer regresi√≥n lineal, ridge y lasso; en Python est√° scikit-learn; mientras que en R tenemos stats y glmnet.\nRecomiendo ampliamente revisar las referencias usadas para este post, hay conocimiento muy bueno en ellas."
  },
  {
    "objectID": "posts/el-poder-de-la-regresion-lineal/index.html#supuestos-de-la-regresi√≥n-lineal",
    "href": "posts/el-poder-de-la-regresion-lineal/index.html#supuestos-de-la-regresi√≥n-lineal",
    "title": "La Regresi√≥n Lineal en Machine Learning",
    "section": "Supuestos de la regresi√≥n lineal",
    "text": "Supuestos de la regresi√≥n lineal\nEs importante saber cu√°ndo es prudente utilizar un modelo de regresi√≥n lineal. Para que se cumplan las condiciones de la regresi√≥n lineal simple y m√∫ltiple, se tienen los siguientes supuestos necesarios\n\nLinealidad: La relaci√≥n entre las variables regresoras \\(x\\) y la variable repuesta \\(y\\) debe ser lineal. Notemos que en la regresi√≥n polinomial esto va m√°s all√°, necesitando solo linealidad en los par√°metros.\nHomocedasticidad: Significa que la varianza de los errores es constante.\nNormalidad de los errores: Los residulales, es decir los errores, deben seguir una distribuci√≥n normal con media 0 y varianza \\(\\sigma^2\\).\nErrores independientes (no autocorrelaci√≥n): Los errores no deben estar correlacionados entre ellos, es otras palabras, deben ser independientes e id√©nticamente distribuidos.\nNo multicolinealidad: No existe correlaci√≥n entre las variables regresoras.\nNo exogeneidad: Las variables regresoras y los errores no est√°n correlacionados."
  },
  {
    "objectID": "posts/el-poder-de-la-regresion-lineal/index.html",
    "href": "posts/el-poder-de-la-regresion-lineal/index.html",
    "title": "La Regresi√≥n Lineal en Machine Learning",
    "section": "",
    "text": "La regresi√≥n lineal puede parecer algo muy simple, pero en realidad tiene un alcance muy grande para modelar datos. Puede verse desde una perspectiva meramente geom√©trica, sin embargo, tiene unos supuestos estad√≠sticos que son sumamente importantes que se cumplan si queremos que el modelo funcione y sea aplicable.\nEn este post, quiero enfocarme en la regresi√≥n lineal desde la perspectiva del machine learning, por lo que asume un conocimiento b√°sico previo de √©ste. Cubrir√° los siguientes puntos:\n\nQu√© es la regresi√≥n lineal en una y varias variables\nQu√© significa que la regresi√≥n lineal se puede aplicar mientras se tenga linealidad en los par√°metros\nSupuestos para la validez de la regresi√≥n lineal\nC√≥mo se define la funci√≥n de costo para la regresi√≥n lineal\nDiferencia entre la soluci√≥n cerrada para minimizar la funci√≥n de costo y el m√©todo del gradiente descendiente\nM√©todos de regularizaci√≥n para regresi√≥n lineal y su interpretaci√≥n geom√©trica\n\nSin m√°s por comentar en la introducci√≥n, comencemos con el post."
  },
  {
    "objectID": "posts/el-poder-de-la-regresion-lineal/index.html#introducci√≥n",
    "href": "posts/el-poder-de-la-regresion-lineal/index.html#introducci√≥n",
    "title": "La Regresi√≥n Lineal en Machine Learning",
    "section": "",
    "text": "La regresi√≥n lineal puede parecer algo muy simple, pero en realidad tiene un alcance muy grande para modelar datos. Puede verse desde una perspectiva meramente geom√©trica, sin embargo, tiene unos supuestos estad√≠sticos que son sumamente importantes que se cumplan si queremos que el modelo funcione y sea aplicable.\nEn este post, quiero enfocarme en la regresi√≥n lineal desde la perspectiva del machine learning, por lo que asume un conocimiento b√°sico previo de √©ste. Cubrir√° los siguientes puntos:\n\nQu√© es la regresi√≥n lineal en una y varias variables\nQu√© significa que la regresi√≥n lineal se puede aplicar mientras se tenga linealidad en los par√°metros\nSupuestos para la validez de la regresi√≥n lineal\nC√≥mo se define la funci√≥n de costo para la regresi√≥n lineal\nDiferencia entre la soluci√≥n cerrada para minimizar la funci√≥n de costo y el m√©todo del gradiente descendiente\nM√©todos de regularizaci√≥n para regresi√≥n lineal y su interpretaci√≥n geom√©trica\n\nSin m√°s por comentar en la introducci√≥n, comencemos con el post."
  },
  {
    "objectID": "posts/el-poder-de-la-regresion-lineal/index.html#referencias",
    "href": "posts/el-poder-de-la-regresion-lineal/index.html#referencias",
    "title": "La Regresi√≥n Lineal en Machine Learning",
    "section": "Referencias",
    "text": "Referencias\n\nDouglas C. Montgomery, Elizabeth A. Peck, G. Geoffrey Vining (2021). Introduction to Linear Regression Analysis. Sixth Edition.\nGareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani (2023). An Introduction to Statistical Learning with Applications in R. 2nd Edition.\nYunpeng Tai (2021). A Survey Of Regression Algorithms And Connections With Deep Learning.\nMukul Ranjan (2022). How does Lasso regression(L1) encourage zero coefficients but not the L2?. Medium. Post link\nTerence Parr. A visual explanation for regularization of linear models. Explained.ai. Post link\nUser: QuantStyle. What is the computational cost of gradient descent vs linear regression?. StackExchange. Post link"
  },
  {
    "objectID": "posts/la-regresion-lneal-en-machine-learning/index.html",
    "href": "posts/la-regresion-lneal-en-machine-learning/index.html",
    "title": "La Regresi√≥n Lineal en Machine Learning",
    "section": "",
    "text": "La regresi√≥n lineal puede parecer algo muy simple, pero en realidad tiene un alcance muy grande para modelar datos. Puede verse desde una perspectiva meramente geom√©trica, sin embargo, tiene unos supuestos estad√≠sticos que son sumamente importantes que se cumplan si queremos que el modelo funcione y sea aplicable.\nEn este post, quiero enfocarme en la regresi√≥n lineal desde la perspectiva del machine learning, por lo que asume un conocimiento b√°sico previo de √©ste. Cubrir√° los siguientes puntos:\n\nQu√© es la regresi√≥n lineal en una y varias variables\nQu√© significa que la regresi√≥n lineal se puede aplicar mientras se tenga linealidad en los par√°metros\nSupuestos para la validez de la regresi√≥n lineal\nC√≥mo se define la funci√≥n de costo para la regresi√≥n lineal\nDiferencia entre la soluci√≥n cerrada para minimizar la funci√≥n de costo y el m√©todo del gradiente descendiente\nM√©todos de regularizaci√≥n para regresi√≥n lineal y su interpretaci√≥n geom√©trica\n\nSin m√°s por comentar en la introducci√≥n, comencemos con el post."
  },
  {
    "objectID": "posts/la-regresion-lneal-en-machine-learning/index.html#introducci√≥n",
    "href": "posts/la-regresion-lneal-en-machine-learning/index.html#introducci√≥n",
    "title": "La Regresi√≥n Lineal en Machine Learning",
    "section": "",
    "text": "La regresi√≥n lineal puede parecer algo muy simple, pero en realidad tiene un alcance muy grande para modelar datos. Puede verse desde una perspectiva meramente geom√©trica, sin embargo, tiene unos supuestos estad√≠sticos que son sumamente importantes que se cumplan si queremos que el modelo funcione y sea aplicable.\nEn este post, quiero enfocarme en la regresi√≥n lineal desde la perspectiva del machine learning, por lo que asume un conocimiento b√°sico previo de √©ste. Cubrir√° los siguientes puntos:\n\nQu√© es la regresi√≥n lineal en una y varias variables\nQu√© significa que la regresi√≥n lineal se puede aplicar mientras se tenga linealidad en los par√°metros\nSupuestos para la validez de la regresi√≥n lineal\nC√≥mo se define la funci√≥n de costo para la regresi√≥n lineal\nDiferencia entre la soluci√≥n cerrada para minimizar la funci√≥n de costo y el m√©todo del gradiente descendiente\nM√©todos de regularizaci√≥n para regresi√≥n lineal y su interpretaci√≥n geom√©trica\n\nSin m√°s por comentar en la introducci√≥n, comencemos con el post."
  },
  {
    "objectID": "posts/la-regresion-lneal-en-machine-learning/index.html#regresi√≥n",
    "href": "posts/la-regresion-lneal-en-machine-learning/index.html#regresi√≥n",
    "title": "La Regresi√≥n Lineal en Machine Learning",
    "section": "Regresi√≥n",
    "text": "Regresi√≥n\nLa regresi√≥n es una t√©cnica de modelaci√≥n proveniente de la estad√≠stica para modelar y analizar la relaci√≥n entre variables. Se tiene una variable \\(y\\) llamada independiente, la cual se modela en funci√≥n de otra variable \\(x\\) llamada independiente, que bien se puede tener una \\(n\\) cantidad de variables \\(x_1, x_2, ... , x_n\\) como se ver√° m√°s adelante.\nEs importante mencionar que, para evitar confusiones con la noci√≥n de indepencia de probabilidad, se prefiere nombrar a \\(y\\) como la variable respuesta y a \\(x\\) como las variables predictoras. En el contexto de machine learning, a las variables \\(x\\) se les denomina caracter√≠sticas (features) y a \\(y\\) etiquetas (labels).\nEl tipo de datos con los que la regresi√≥n trabaja son num√©ricos (flotantes), y vienen en duplas \\((x_1, y_1), ... , (x_m, y_m)\\), donde \\(m\\) es la cantidad de datos que tengamos, es decir, cardinalidad del conjunto de datos. N√≥tese que se pueden tener m√∫ltiples caracter√≠sticas \\(x\\).\nPara trabajar con regresi√≥n, si la dimensi√≥n lo permite, suele hacerse un diagrama de dispersi√≥n que es una gr√°fica de los datos representados como puntos. Por ejemplo, consideremos de \\(x\\) e \\(y\\), con 15 registros (\\(m = 15\\)), resultando el siguiente diagrama de dispersi√≥n\n\n\n\n\n\n\n\n\n\nEn este caso, se ve claramente que los puntos tienen una tendencia lineal, esto es, que se les puede ajustar una recta para modelarlos con un error considerablemente bajo. La ecuaci√≥n de la recta, es \\(y = ax + b\\), con \\(a\\) la pendiente y \\(b\\) la intersecci√≥n en el eje \\(y\\). Dicha ecuaci√≥n discribir√≠a adecuadamente ese comportamiento, en las siguientes secciones veremos c√≥mo ajustarla a los datos para encontrar los valores adecuados de \\(a\\) y \\(b\\).\nEn este diagrama en part√≠cular, los puntos fueron simulados de una recta \\(y = 2x\\), a la que se les agreg√≥ un error \\(\\varepsilon\\) que tiene una distribuci√≥n normal con media \\(\\mu = 0\\) y desviaci√≥n estandar \\(\\sigma = 1.5\\). Teniendo as√≠ los par√°metros de la recta \\(a = 2\\) y \\(b = 0\\)\n\nObtenci√≥n de datos para regresi√≥n\nHay 3 maneras principales para obtener datos que puedan usarse en regresi√≥n, los cuales se listan a continuaci√≥n\n\nEstudio retrospectivo: Aqu√≠ se tienen datos que ya fueron capturados en el pasado, y no necesariamente con una intenci√≥n de ser analizados cient√≠ficamente, por lo que pueden no estar en la forma m√°s ideal posible. Los datos y sus resultados ya se tienen, no se puede intervenir ni cambiar nada para mejorar la toma de datos. Este tipo de estudio es muy com√∫n en machine learning y big data, donde obtienes datos con ciertas caracter√≠sticas y ya etiquetados que fueron obtenidos en el pasado.\nEstudio observacional: Como su nombre puede indicarlo, en este tipo de estudio solamente se observa el proceso o experimento que quiere analizarse, interviniendo m√≠nimamente solamente para la captura de los datos. Por temas √©ticos, este estudio es muy usado en el √°rea m√©dica, pues de lo contrario se estar√≠a imponiendo sobre las personas las condiciones que deben tener (como empezar a fumar, por ejemplo).\nDise√±o experimental: Este tipo de estudio posee m√°s estrategia. Aqu√≠ se manipulan las variables predictoras (o caracter√≠sticas) de acuerdo a un dise√±o experimental, para tener ciertos valores y observar su efecto en la variable respuesta."
  },
  {
    "objectID": "posts/la-regresion-lneal-en-machine-learning/index.html#regresi√≥n-lineal-simple",
    "href": "posts/la-regresion-lneal-en-machine-learning/index.html#regresi√≥n-lineal-simple",
    "title": "La Regresi√≥n Lineal en Machine Learning",
    "section": "Regresi√≥n Lineal Simple",
    "text": "Regresi√≥n Lineal Simple\nLa regresi√≥n lineal simple es √∫til cuando existe una tendencia lineal en los datos. Aunque veremos m√°s adelante que va m√°s all√° de lo lineal. Se le llama simple porque solo hay una caracter√≠stica \\(x\\), y se modela como\n\\[y = w_0 + w_1 x + \\varepsilon\\] La cual puede verse como la ecuaci√≥n de regresi√≥n lineal poblacional, no obstante, en regresi√≥n lineal trabajamos con \\(m\\) n√∫mero de datos, entonces para cada punto se tendr√° una ecuaci√≥n de regresi√≥n lineal muestral\n\\[y_i = w_0 + w_1 x_i + \\varepsilon_i \\quad i=1, 2, 3, ..., m\\]\nCompar√°ndo con la ecuaci√≥n de la recta, podemos interpretar los par√°metros. Se tiene que \\(w_1\\) es el cambio en la media de la distribuci√≥n de \\(y\\) producido por un cambio de unidad en \\(x\\). Mientras que \\(w_0\\) simplemente es la media de la distribuci√≥n de \\(y\\) cuando \\(x = 0\\), si el dominio de \\(x\\) no incluye a \\(0\\) entonces el par√°metro \\(w_0\\) no tiene interpretaci√≥n pr√°ctica.\nPor otro lado, \\(\\varepsilon\\) son los errores o ruido que se tiene en cada predicci√≥n de \\(y\\). Se asumen que dichos errores tienen una distribuci√≥n normal con media igual a cero \\(\\mu = 0\\) y una varianza desconicida \\(\\sigma^2\\). Es importante notar que, esa varianza es la misma en todos los puntos de la regresi√≥n, lo que se conoce como homocedasticidad. A los errores \\(\\varepsilon\\) se les llama residuales pues se definen como \\(y - \\hat{y}\\).\nAhora bien, retomando el diagrama de dispersi√≥n que se vio anteriormente, el cual tiene proviene de la recta \\(y = 2x\\) con un error \\(\\varepsilon\\) con media \\(0\\) y varianza \\(\\sigma^2 = 1.5^2\\). Se le puede ajustar una recta de regresi√≥n lineal y se ver√≠a como la siguiente figura\n\n\n\n\n\n\n\n\n\nObteniendo los coeficientes estimados \\(\\hat{w_0} = 0.3045\\) y \\(\\hat{w_1} = 2.0274\\). Teniendo as√≠ una ecuaci√≥n de regresi√≥n lineal con la que podemos hacer predicciones\n\\[ \\hat{y} = 0.3045 + 2.0274 x\\] Notemos que los par√°metros reales de donde provienen los datos son \\(0\\) y \\(2\\), pero por la naturaleza estoc√°stica de los errores, los estimados no coinciden con exactitud. Entre mayor sea el n√∫mero de datos \\(m\\), los par√°metros estimados ser√°n m√°s cercanos a los originales."
  },
  {
    "objectID": "posts/la-regresion-lneal-en-machine-learning/index.html#regresi√≥n-lineal-m√∫ltiple",
    "href": "posts/la-regresion-lneal-en-machine-learning/index.html#regresi√≥n-lineal-m√∫ltiple",
    "title": "La Regresi√≥n Lineal en Machine Learning",
    "section": "Regresi√≥n Lineal M√∫ltiple",
    "text": "Regresi√≥n Lineal M√∫ltiple\nPara el caso en que la variable respuesta \\(y\\) est√° realcionada con \\(n\\) variables regresoras, se tiene el modelo de la regresi√≥n lineal m√∫ltiple\n\\[ y = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n + \\varepsilon \\] As√≠, a los par√°metros \\(w_j \\quad j = 0, 1, 2, ..., n\\) se les llama coeficientes de regresi√≥n. Este modelo, al ser de multivariable, describe un hiperplano en el espacio \\(n\\)-dimensional de las variables regresoras \\(x\\).\nHaciendo sentido de los par√°metros. El par√°metro \\(w_j\\) representa el cambio esperado en la respuesta \\(y\\) por unidad de cambio en \\(x_j\\), cuando todo las dem√°s variables regresoras \\(x_i(i\\ne j)\\) se mantienen constantes.Por otra parte, el par√°metro \\(w_0\\) es la intersecci√≥n del hiperplano en \\(y\\), si el dominio de los datos incluye \\(x_1 = x_2 = ... = x_n = 0\\) entonces \\(w_0\\) es la media de \\(y\\) cuando \\(x_1 = x_2 = ... = x_n = 0\\); de lo contrario, \\(w_0\\) no tiene interpretaci√≥n f√≠sica.\nConcluyendo, en el modelo de regresi√≥n simple y m√∫ltiple, la linealidad se tiene en los par√°metros. Esto extiende m√°s all√° el concepto de linealidad aunque las variables regresoras \\(x\\) no sean lineales, esto se ver√° a continuaci√≥n en la secci√≥n de regresi√≥n polinomial."
  },
  {
    "objectID": "posts/la-regresion-lneal-en-machine-learning/index.html#regresi√≥n-polinomial",
    "href": "posts/la-regresion-lneal-en-machine-learning/index.html#regresi√≥n-polinomial",
    "title": "La Regresi√≥n Lineal en Machine Learning",
    "section": "Regresi√≥n Polinomial",
    "text": "Regresi√≥n Polinomial\nYa vimos que una regresi√≥n lineal de \\(n\\) variables regresoras genera un hiperplano \\(n\\)-dimensional. Ahora bien, cualquier modelo que es lineal en los par√°metros \\(w\\)‚Äôs es un modelo de regresi√≥n lineal, independientemente de la forma de la superficie que genere.\nComo su nombre lo indica, la regresi√≥n polinomial de una variable \\(x\\) se define a partir de los polinomios, y se modela de la siguiente forma\n\\[ y = w_0 + w_1x + w_2x^2 + ... + w_nx^n + \\varepsilon\\] La cual es lineal en los par√°metros, por lo que se puede trabajar como una regresi√≥n lineal m√∫ltiple y usar la misma metodolog√≠a. Para hacer esto, podemos definir \\(x_j = x^j \\quad j=1, 2, 3, ... , n\\). Por ejemplo, para el caso de un polinomio de grado 2 con \\(y = w_0 + w_1x + w_2x^2 + \\varepsilon\\), tendr√≠amos \\(x_1 = x, x_2 = x^2\\) y la regresi√≥n se convertir√≠a en una lineal m√∫ltiple\n\\[ y = w_0 + w_1x_1 + w_2x_2 + \\varepsilon\\] Con este tipo de regresi√≥n, hay que ser muy cuidadosos. Ya que un polinomio de grado considerablemente alto puede apr√≥ximar muy bien una curva no lineal, sin embargo, hay que tratar de hacer transformaciones y mantener el grado lo m√°s bajo posible para evitar un sobreajuste del modelo (empezar a generalizar el error mismo)."
  },
  {
    "objectID": "posts/la-regresion-lneal-en-machine-learning/index.html#supuestos-de-la-regresi√≥n-lineal",
    "href": "posts/la-regresion-lneal-en-machine-learning/index.html#supuestos-de-la-regresi√≥n-lineal",
    "title": "La Regresi√≥n Lineal en Machine Learning",
    "section": "Supuestos de la regresi√≥n lineal",
    "text": "Supuestos de la regresi√≥n lineal\nEs importante saber cu√°ndo es prudente utilizar un modelo de regresi√≥n lineal. Para que se cumplan las condiciones de la regresi√≥n lineal simple y m√∫ltiple, se tienen los siguientes supuestos necesarios\n\nLinealidad: La relaci√≥n entre las variables regresoras \\(x\\) y la variable repuesta \\(y\\) debe ser lineal. Notemos que en la regresi√≥n polinomial esto va m√°s all√°, necesitando solo linealidad en los par√°metros.\nHomocedasticidad: Significa que la varianza de los errores es constante.\nNormalidad de los errores: Los residulales, es decir los errores, deben seguir una distribuci√≥n normal con media 0 y varianza \\(\\sigma^2\\).\nErrores independientes (no autocorrelaci√≥n): Los errores no deben estar correlacionados entre ellos, es otras palabras, deben ser independientes e id√©nticamente distribuidos.\nNo multicolinealidad: No existe correlaci√≥n entre las variables regresoras.\nNo exogeneidad: Las variables regresoras y los errores no est√°n correlacionados."
  },
  {
    "objectID": "posts/la-regresion-lneal-en-machine-learning/index.html#m√≠nimos-cuadrados-ordinarios-como-funci√≥n-de-costo",
    "href": "posts/la-regresion-lneal-en-machine-learning/index.html#m√≠nimos-cuadrados-ordinarios-como-funci√≥n-de-costo",
    "title": "La Regresi√≥n Lineal en Machine Learning",
    "section": "M√≠nimos cuadrados ordinarios como funci√≥n de costo",
    "text": "M√≠nimos cuadrados ordinarios como funci√≥n de costo\nEn machine learning, la base para ajustar los par√°metros a nuestros datos, lo que se conoce como aprender, radica en la optimizaci√≥n de una funci√≥n objetivo. Dicho de otra manera, es minimizar una funci√≥n de costo, donde dicha funci√≥n ser√≠a una medida del error en las predicciones del modelo.\nEn la regresi√≥n lineal no es la excepci√≥n. Es muy conocido el m√©todo de m√≠nimos cuadrados para ajustar los par√°metros, donde se define la funci√≥n de costo \\(J\\) de m√≠nimos cuadrados ordinarios (MCO) de la siguiente manera\n\\[ J(y, \\hat{y}) = \\frac{1}{m} \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2 \\] Donde el factor \\(\\frac{1}{m}\\) se multiplica como una especie de promedio en todos los datos. Algunas referencias tambi√©n manejan multiplicarlo por \\(\\frac{1}{2}\\) por conveniencia algebraica.\nNotemos que la diferencia \\(y_i - \\hat{y}_i\\) es la distancia que hay desde un punto \\(\\hat{y} (x_i)\\) en la recta de la predicci√≥n \\(\\hat{y}\\) hasta el correspondiente punto \\(y(x_i)\\) en la recta original \\(y\\) de donde provienen los datos. Est√° al cuadrado para evitar que las sumas de esos t√©rminos puedan dar cero. As√≠, podemos interpretarlo gr√°ficamente como unos cuadrados en dichos puntos \\(x_i\\), considerando el mismo diagrama de dispersi√≥n de este post, se ver√≠a\n\n\n\n\n\n\n\n\n\nN√≥tese que los ejes fueron escalados para una mejor visualizaci√≥n. Ahora, al ajustar los par√°metros \\(w\\) minimizando la funci√≥n de costo de MCO, lo que se est√° haciendo gr√°ficamente es ajustar la recta de predicci√≥n de tal manera que se minimize el √°rea de esos cuadrados; de ah√≠ el nombre de este m√©todo.\nPara minimizar dicha funci√≥n, es mejor expresarla en t√©rmino de los par√°metros \\(w\\), para esto, en el caso de una variable regresora sustituimos \\(\\hat{y} = w_0 + w_1 x\\) en la funci√≥n de costo de MCO, as√≠\n\\[ J(w_0, w_1) = \\frac{1}{m} \\sum_{i=1}^{m} (y_i - w_0 - w_1 x_i)^2 \\] En la siguiente secci√≥n veremos c√≥mo minimizarla, ejemplificando el caso con una sola variable regresora \\(x\\)."
  },
  {
    "objectID": "posts/la-regresion-lneal-en-machine-learning/index.html#ajuste-de-los-par√°metros-minimizando-la-funci√≥n-de-costo",
    "href": "posts/la-regresion-lneal-en-machine-learning/index.html#ajuste-de-los-par√°metros-minimizando-la-funci√≥n-de-costo",
    "title": "La Regresi√≥n Lineal en Machine Learning",
    "section": "Ajuste de los par√°metros minimizando la funci√≥n de costo",
    "text": "Ajuste de los par√°metros minimizando la funci√≥n de costo\nHay 3 maneras principales de minimizar la funci√≥n de costo de m√≠nimos cuadrados ordinarios (MCO) \\(J\\). El m√©todo cl√°sico, utiliza c√°lculo diferencial y el criterio de las derivadas para minimizar. La segunda, utiliza el m√©todo de la m√°xima verosimilitud, un concepto estad√≠stico. Se encuentra matem√°ticamente que ambos m√©todos llegan a la misma forma soluci√≥n. La tercera, usa el m√©todo m√°s utilizado en machine learning, el del gradiente descendiente (gradient descent).\nMi objetivo de este post no es hacer demostraciones matem√°ticas ni c√°lculos completos, sino que ilustrar la metodolog√≠a, as√≠ que nos enfocaremos en el m√©todo cl√°sico y el de gradiente descendiente. Veamos a m√°s detalle\n\nEl m√©todo de m√≠nimos cuadrados cl√°sico\nPara minimizar la funci√≥n de costo, los estimados de los par√°metros \\(w_0\\), \\(w_1\\), que son \\(\\hat{w_0}\\), \\(\\hat{w_1}\\) respectivamente, deben satisfacer\n\\[ \\frac{\\partial J}{\\partial w_0} \\bigg|_{\\hat{w_0}, \\hat{w_1}}= - \\frac{2}{m} \\sum_{i=1}^{m} (y_i - \\hat{w_0} - \\hat{w_1} x_i) = 0\\] y\n\\[ \\frac{\\partial J}{\\partial w_1} \\bigg|_{\\hat{w_0}, \\hat{w_1}} = - \\frac{2}{m} \\sum_{i=1}^{m} (y_i - \\hat{w_0} - \\hat{w_1} x_i) x_i = 0\\]\nde acuerdo al criterio de la primer derivada, √©sta debe ser 0 para que sea un m√≠nimo.\nSimplificando las expresiones, separando las sumatorias y resolviendo para \\(\\hat{w_0}\\), \\(\\hat{w_1}\\), se tienen las soluciones a los par√°metros estimados que minimizan la funci√≥n de MCO\n\\[ \\hat{w_0} =  \\bar{y} - \\hat{w_1} \\bar{x}\\] y\n\\[ \\hat{w_1} = \\frac{ \\sum_{i=1}^{m} y_i x_i - \\frac{(\\sum_{i=1}^{m} y_i)(\\sum_{i=1}^{m} x_i)}{n} }{ \\sum_{i=1}^{m} x_i^2 - \\frac{(\\sum_{i=1}^{m}x_i)^2}{m} } \\]\ndonde \\(\\bar{x} = \\frac{1}{m}  \\sum_{i=1}^{m} x_i\\) y \\(\\bar{y} = \\frac{1}{m}  \\sum_{i=1}^{m} y_i\\) son los promedios de \\(x_i\\), \\(y_i\\), respectivamente.\nPara el caso multivariable, se tiene una soluci√≥n de los par√°metros estimados \\(W^{*}\\) en forma matricial, dado por\n\\[ W^{*} = (X^T X)^{-1} X^{T} Y \\] Donde \\(X\\) es la matriz de dise√±o, la cual es de tama√±o \\(m \\times (n + 1)\\). Donde \\(m\\) es el n√∫mero de datos y \\(n\\) el n√∫mero de variables regresoras. Se construye poniendo en filas los vectores de datos \\(x_j = (1, x_{j1}, x_{j2}, ..., x_{jn})\\), donde \\(j = 1, 2, 3, ..., m\\), en todos los datos. Por otro lado, \\(Y\\) es la matriz (o vector) de tama√±o \\(m \\times 1\\), que se construye poniendo todos los puntos \\(y\\) de donde se har√° la regresi√≥n.\nAs√≠,\n\\[ X = \\begin{pmatrix}\n1 & x_{11} & x_{12} & ... & x_{1n} \\\\\n1 & x_{21} & x_{22} & ... & x_{2n} \\\\\n&  & \\vdots &  &  \\\\\n1 & x_{m1} & x_{m2} & ... & x_{mn}\n\\end{pmatrix}  \\]\n\\[ Y = \\begin{pmatrix}\ny_1 \\\\\ny_2 \\\\\ny_3 \\\\\n\\vdots \\\\\ny_m\n\\end{pmatrix}  \\]\n\n\nEl m√©todo del gradiente descendiente\nRetomando conceptos de c√°lculo diferencial para una variable, la derivada de una funci√≥n \\(f\\) en un punto dado \\(x_0\\), nos da la pendiende de la recta tangente a la curva en ese punto. En otras palabras, esa pendiente puede interprestarse como la raz√≥n de cambio de la funci√≥n en ese punto y el signo indica hacia donde est√° cambiando. Extendiendo este concepto hacia varias variables, el gradiente de una funci√≥n \\(f\\), denotado como \\(\\nabla f\\), es un vector que indica hacia d√≥nde la funci√≥n tiene el mayor incremento, y su magnitud es la raz√≥n de cambio.\nEl m√©todo del gradiente descendiente, las derivadas son respecto a los par√°metros \\(w\\) que queremos ajustar, y como su nombre lo indica, hace uso del gradiente para minimizar la funci√≥n de costo \\(J(w)\\). Para el caso de la regresi√≥n lineal, el algoritmo se define de la siguiente manera\n\\[ w_i^{(\\tau + 1)} := w_i^{(\\tau)} - \\alpha \\frac{\\partial J(w)}{\\partial w_i}\\] donde \\(J(w)\\) esl funci√≥n de costo de m√≠nimos cuadrados ordinarios. La derivada est√° multiplicada por un \\(-1\\), lo que asegura que se mueva en la direcci√≥n de menor crecimiento, tenemos que \\(\\alpha\\) es una constante llamada tasa de aprendizaje, controla qu√© tanto se ajusta el valor de \\(w_i\\). En machine learning, \\(\\alpha\\) suele tomar valores muy peque√±os, como \\(0.01, 0.001,\\) etc. Por otro lado, \\(\\tau\\) es un n√∫mero entero que indica en la iteraci√≥n que estamos del algoritmo.\nAs√≠, se estar√°n ajustando los par√°metros \\(w_i\\) de manera iterativa hasta que se cumpla un cierto criterio o se alcance una tolerancia del error. Cabe se√±alar que en la primera iteraci√≥n, los par√°metros se inicializan de manera aleatoria.\nPor lo tanto, para el caso de regresi√≥n lineal simple, si utilizamos el m√©todo del gradiente descendiente tendriamos lo siguiente\n\\[ w_0^{(\\tau + 1)} := w_0^{(\\tau)} + \\alpha \\frac{2}{m} \\sum_{i=1}^{m} (y_i - \\hat{y_i})\\] \\[ w_1^{(\\tau + 1)} := w_1^{(\\tau)} + \\alpha \\frac{2}{m} \\sum_{i=1}^{m} (y_i - \\hat{y_i}) x_i\\] Donde \\(\\hat{y_i}\\) son las predicciones en la iteraci√≥n \\(\\tau\\), o sea antes de hacer la actualizaci√≥n de los par√°metros.\n\n\n¬øCu√°l usar de los 2 m√©todos?\nLa respuesta r√°dica en la complejidad computacional y el tiempo que toman los c√°lculos. En machine learning se trabajan con datos demasiado grandes, m√°s de lo que se sol√≠a hacer en la estad√≠stica convencional, por lo tanto, la forma matricial de obtener los par√°metros puede llegar a ser muy costosa computacionalmente hablando si se tienen muchos datos y muchas caracter√≠sticas (variables regresoras). As√≠, es m√°s conveniente utilizar el m√©todo del gradiente descendiente, que en la pr√°ctica suele ser m√°s eficiente."
  },
  {
    "objectID": "posts/la-regresion-lneal-en-machine-learning/index.html#m√©todos-de-regularizaci√≥n",
    "href": "posts/la-regresion-lneal-en-machine-learning/index.html#m√©todos-de-regularizaci√≥n",
    "title": "La Regresi√≥n Lineal en Machine Learning",
    "section": "M√©todos de regularizaci√≥n",
    "text": "M√©todos de regularizaci√≥n\nLa regularizaci√≥n en regresi√≥n lineal, es para reducir la complejidad del modelo evitando as√≠ un sobreajuste. Esto se logra a√±adiendo un t√©rmino a la funci√≥n de costo de MCO, lo que acota los par√°metros evitando que crezcan demasiado. Hay 2 casos, Ridge y Lasso, como se ver√° a continuaci√≥n.\n\nRegresi√≥n Lineal Ridge: limitando el tama√±o de los par√°metros\nLa regresi√≥n ridge hace que los par√°metros \\(w_i\\) se encojan, es decir, que no tomen valores tan grandes. Se define con la siguiente funci√≥n de costo \\(J_R(w)\\)\n\\[ J_R(W) = \\frac{1}{m} \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{n} w_j^2 \\] Sumando as√≠ el t√©rmino \\(\\lambda \\sum_{j=1}^{n} w_j^2\\) de penalizaci√≥n. Donde \\(\\lambda\\) controla que tanto se encojen los par√°metros, entre m√°s grande es el valor de \\(\\lambda\\) est√°n m√°s l√≠mitados los par√°metros. Se tiene que cuando \\(\\lambda \\xrightarrow{}{} \\infty\\), los par√°metros \\(w_i\\) tienden a \\(0\\).\nNotemos que el par√°metro \\(w_0\\) no se incluye, ya que solo se toma en cuenta \\(w_1, w_2, ..., w_n\\). Eso es porque \\(w_0\\), como ya vimos, es el valor promedio de \\(y\\) cuando \\(x_1=x_2=...=x_n=0\\). No ser√≠a conveniente imponer restricciones o encoger dicho par√°metro entonces.\nA la regresi√≥n ridge tambi√©n se le conoce como regularizaci√≥n L2, debido a que el t√©rmino \\(\\sum_{j=1}^{n} w_j^2\\) es la norma L2 del vector de par√°metros \\(w = (w_1, w_2, ..., w_n)\\).\n\n\nRegresi√≥n Lineal Lasso: un selector de caracter√≠sticas\nLa regresi√≥n lasso act√∫a como un selector de caracter√≠sticas, ya que impone que algunos de los par√°metros \\(w_i\\) sean igual a \\(0\\). Se define con la siguiente funci√≥n de costo \\(J_l(w)\\)\n\\[ J_l(W) = \\frac{1}{m} \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{n} |w_j| \\] Sumando as√≠ el t√©rmino \\(\\lambda \\sum_{j=1}^{n} |w_j|\\) de penalizaci√≥n. Similarmente, lasso tambi√©n hace que los par√°metros se encojan hac√≠a 0, pero a diferencia de ridge, si \\(\\lambda\\) es suficientemente grande algunos par√°metros \\(w_i\\) van a ser \\(0\\). As√≠, la variable regresora (o caracter√≠stica) \\(x_i\\) asociado al par√°metro \\(w_i\\) no estar√° involucrada en la ecuaci√≥n de regresi√≥n, por esta raz√≥n lasso es un selector de caracter√≠sticas.\nIgual que en el caso de regresi√≥n ridge, el par√°metro \\(w_0\\) no se incluye, ya que no ser√≠a conveniente imponer restricciones o hacer 0 el par√°metro \\(w_0\\), siendo que es es el valor promedio de \\(y\\) cuando \\(x_1=x_2=...=x_n=0\\).\nA la regresi√≥n lasso tambi√©n se le conoce como regularizaci√≥n L1, debido a que el t√©rmino \\(\\sum_{j=1}^{n} |w_j|\\) es la norma L1 del vector de par√°metros \\(w = (w_1, w_2, ..., w_n)\\).\n\n\nInterpretaci√≥n geom√©trica de ridge y lasso\nSurge la pregunta, ¬øPor qu√© la regresi√≥n ridge encoje los coeficientes pero la regresi√≥n lasso fuerza algunos de ellos a ser \\(0\\)? Para contestar esto, encontr√© que la mejor manera es gr√°ficamente.\nLa funci√≥n de costo de m√≠nimos cuadrados ordinarios MCO, es una funci√≥n convexa, por lo que es posible minimizarla. Ahora bien, si la graficamos en el espacio de los par√°metros al minimizar se pueden probar todos las combinaciones de par√°metros, sin embargo, al a√±adir un t√©rmino de penalizaci√≥n como en ridge y lasso, se hace una constricci√≥n en el espacio en el que puede bucarse el m√≠nimo. Veamos la siguiente figura (extra√≠da del libro Introduction to Statistical Learning)\n\n\n\nCurvas de nivel de la funci√≥n de costo de MCO en rojo y las constricciones de ridge y lasso en azul. Graficadas en el espacio de 2 par√°metros \\(w_1\\) y \\(w_2\\). El m√≠nimo se encuentra en \\(\\hat{w}\\).\n\n\nEn este caso, es una regresi√≥n en 2 dimensiones con \\(x_1\\), \\(x_2\\) y las gr√°ficas est√°n hechas en el espacio de los par√°metros \\(w_1\\), \\(w_2\\). Las l√≠neas rojas son las curvas de nivel de la funci√≥n de costo de MCO, y las √°reas s√≥lidas azules son las regiones de constricciones impuestas en los par√°metros \\(w_1\\), \\(w_2\\) por los t√©rminos de penalizaci√≥n de lasso y ridge, respectivamente.\nPara el caso de ridge, tenemos la constricci√≥n el c√≠rculo \\(w_1^2 + w_2^2 \\le s\\), y para lasso el rombo \\(|w_1| + |w_2| \\le s\\). El punto de minimizaci√≥n, donde est√°n los mejores par√°metros estimados, es la intersecci√≥n m√°s pronta entre la curva de nivel de MCO y el √°rea de constricci√≥n. Podemos observar claramente que para lasso, este punto de intersecci√≥n m√°s pronto se encuentra en la punta del rombo, es decir, sobre un eje donde el par√°metro \\(w_2 = 0\\); mientras que para ridge, el punto se encuentra sobre la circunferencia fuera de los ejes, por los que los par√°metros estar√°n encojidos por la constricci√≥n pero no ser√°n 0. Esto se extiende a cualquier dimensi√≥n, para \\(n \\gt 2\\), el rombo se convierte en un politopo mientras que el c√≠rculo en una hiperesfera. Entonces para el politopo siempre habr√° esquinas donde los par√°metros sean 0."
  },
  {
    "objectID": "posts/la-regresion-lneal-en-machine-learning/index.html#conclusiones",
    "href": "posts/la-regresion-lneal-en-machine-learning/index.html#conclusiones",
    "title": "La Regresi√≥n Lineal en Machine Learning",
    "section": "Conclusiones",
    "text": "Conclusiones\nEn este post, se vi√≥ c√≥mo se define la regresi√≥n lineal en una y varias variables, y los supuestos que se deben cumplir para su utilizaci√≥n. Se explic√≥ c√≥mo minimizar su funci√≥n de costo de m√≠nimos cuadrados ordinarios en la manera cl√°sica y con el m√©todo de gradiente descendiente. Tambi√©n, se vieron los m√©todos de regularizaci√≥n de regresi√≥n ridge y lasso, y c√≥mo influyen en los par√°metros encoji√©ndolos o haciendo algunos de ellos \\(0\\), respectivamente.\nComo trabajo futuro, queda llevar este conocimiento a implementaci√≥n en c√≥digo, para ver su funcionalidad pr√°ctica. Existen librer√≠as para hacer regresi√≥n lineal, ridge y lasso; en Python est√° scikit-learn; mientras que en R tenemos stats y glmnet.\nRecomiendo ampliamente revisar las referencias usadas para este post, hay conocimiento muy bueno en ellas."
  },
  {
    "objectID": "posts/la-regresion-lneal-en-machine-learning/index.html#referencias",
    "href": "posts/la-regresion-lneal-en-machine-learning/index.html#referencias",
    "title": "La Regresi√≥n Lineal en Machine Learning",
    "section": "Referencias",
    "text": "Referencias\n\nDouglas C. Montgomery, Elizabeth A. Peck, G. Geoffrey Vining (2021). Introduction to Linear Regression Analysis. Sixth Edition.\nGareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani (2023). An Introduction to Statistical Learning with Applications in R. 2nd Edition.\nYunpeng Tai (2021). A Survey Of Regression Algorithms And Connections With Deep Learning.\nMukul Ranjan (2022). How does Lasso regression(L1) encourage zero coefficients but not the L2?. Medium. Post link\nTerence Parr. A visual explanation for regularization of linear models. Explained.ai. Post link\nUser: QuantStyle. What is the computational cost of gradient descent vs linear regression?. StackExchange. Post link"
  },
  {
    "objectID": "posts/la-regresion-lineal-en-machine-learning/index.html",
    "href": "posts/la-regresion-lineal-en-machine-learning/index.html",
    "title": "La Regresi√≥n Lineal en Machine Learning",
    "section": "",
    "text": "La regresi√≥n lineal puede parecer algo muy simple, pero en realidad tiene un alcance muy grande para modelar datos. Puede verse desde una perspectiva meramente geom√©trica, sin embargo, tiene unos supuestos estad√≠sticos que son sumamente importantes que se cumplan si queremos que el modelo funcione y sea aplicable.\nEn este post, quiero enfocarme en la regresi√≥n lineal desde la perspectiva del machine learning, por lo que asume un conocimiento b√°sico previo de √©ste. Cubrir√° los siguientes puntos:\n\nQu√© es la regresi√≥n lineal en una y varias variables\nQu√© significa que la regresi√≥n lineal se puede aplicar mientras se tenga linealidad en los par√°metros\nSupuestos para la validez de la regresi√≥n lineal\nC√≥mo se define la funci√≥n de costo para la regresi√≥n lineal\nDiferencia entre la soluci√≥n cerrada para minimizar la funci√≥n de costo y el m√©todo del gradiente descendiente\nM√©todos de regularizaci√≥n para regresi√≥n lineal y su interpretaci√≥n geom√©trica\n\nSin m√°s por comentar en la introducci√≥n, comencemos con el post."
  },
  {
    "objectID": "posts/la-regresion-lineal-en-machine-learning/index.html#introducci√≥n",
    "href": "posts/la-regresion-lineal-en-machine-learning/index.html#introducci√≥n",
    "title": "La Regresi√≥n Lineal en Machine Learning",
    "section": "",
    "text": "La regresi√≥n lineal puede parecer algo muy simple, pero en realidad tiene un alcance muy grande para modelar datos. Puede verse desde una perspectiva meramente geom√©trica, sin embargo, tiene unos supuestos estad√≠sticos que son sumamente importantes que se cumplan si queremos que el modelo funcione y sea aplicable.\nEn este post, quiero enfocarme en la regresi√≥n lineal desde la perspectiva del machine learning, por lo que asume un conocimiento b√°sico previo de √©ste. Cubrir√° los siguientes puntos:\n\nQu√© es la regresi√≥n lineal en una y varias variables\nQu√© significa que la regresi√≥n lineal se puede aplicar mientras se tenga linealidad en los par√°metros\nSupuestos para la validez de la regresi√≥n lineal\nC√≥mo se define la funci√≥n de costo para la regresi√≥n lineal\nDiferencia entre la soluci√≥n cerrada para minimizar la funci√≥n de costo y el m√©todo del gradiente descendiente\nM√©todos de regularizaci√≥n para regresi√≥n lineal y su interpretaci√≥n geom√©trica\n\nSin m√°s por comentar en la introducci√≥n, comencemos con el post."
  },
  {
    "objectID": "posts/la-regresion-lineal-en-machine-learning/index.html#regresi√≥n",
    "href": "posts/la-regresion-lineal-en-machine-learning/index.html#regresi√≥n",
    "title": "La Regresi√≥n Lineal en Machine Learning",
    "section": "Regresi√≥n",
    "text": "Regresi√≥n\nLa regresi√≥n es una t√©cnica de modelaci√≥n proveniente de la estad√≠stica para modelar y analizar la relaci√≥n entre variables. Se tiene una variable \\(y\\) llamada independiente, la cual se modela en funci√≥n de otra variable \\(x\\) llamada independiente, que bien se puede tener una \\(n\\) cantidad de variables \\(x_1, x_2, ... , x_n\\) como se ver√° m√°s adelante.\nEs importante mencionar que, para evitar confusiones con la noci√≥n de indepencia de probabilidad, se prefiere nombrar a \\(y\\) como la variable respuesta y a \\(x\\) como las variables predictoras. En el contexto de machine learning, a las variables \\(x\\) se les denomina caracter√≠sticas (features) y a \\(y\\) etiquetas (labels).\nEl tipo de datos con los que la regresi√≥n trabaja son num√©ricos (flotantes), y vienen en duplas \\((x_1, y_1), ... , (x_m, y_m)\\), donde \\(m\\) es la cantidad de datos que tengamos, es decir, cardinalidad del conjunto de datos. N√≥tese que se pueden tener m√∫ltiples caracter√≠sticas \\(x\\).\nPara trabajar con regresi√≥n, si la dimensi√≥n lo permite, suele hacerse un diagrama de dispersi√≥n que es una gr√°fica de los datos representados como puntos. Por ejemplo, consideremos de \\(x\\) e \\(y\\), con 15 registros (\\(m = 15\\)), resultando el siguiente diagrama de dispersi√≥n\n\n\n\n\n\n\n\n\n\nEn este caso, se ve claramente que los puntos tienen una tendencia lineal, esto es, que se les puede ajustar una recta para modelarlos con un error considerablemente bajo. La ecuaci√≥n de la recta, es \\(y = ax + b\\), con \\(a\\) la pendiente y \\(b\\) la intersecci√≥n en el eje \\(y\\). Dicha ecuaci√≥n discribir√≠a adecuadamente ese comportamiento, en las siguientes secciones veremos c√≥mo ajustarla a los datos para encontrar los valores adecuados de \\(a\\) y \\(b\\).\nEn este diagrama en part√≠cular, los puntos fueron simulados de una recta \\(y = 2x\\), a la que se les agreg√≥ un error \\(\\varepsilon\\) que tiene una distribuci√≥n normal con media \\(\\mu = 0\\) y desviaci√≥n estandar \\(\\sigma = 1.5\\). Teniendo as√≠ los par√°metros de la recta \\(a = 2\\) y \\(b = 0\\)\n\nObtenci√≥n de datos para regresi√≥n\nHay 3 maneras principales para obtener datos que puedan usarse en regresi√≥n, los cuales se listan a continuaci√≥n\n\nEstudio retrospectivo: Aqu√≠ se tienen datos que ya fueron capturados en el pasado, y no necesariamente con una intenci√≥n de ser analizados cient√≠ficamente, por lo que pueden no estar en la forma m√°s ideal posible. Los datos y sus resultados ya se tienen, no se puede intervenir ni cambiar nada para mejorar la toma de datos. Este tipo de estudio es muy com√∫n en machine learning y big data, donde obtienes datos con ciertas caracter√≠sticas y ya etiquetados que fueron obtenidos en el pasado.\nEstudio observacional: Como su nombre puede indicarlo, en este tipo de estudio solamente se observa el proceso o experimento que quiere analizarse, interviniendo m√≠nimamente solamente para la captura de los datos. Por temas √©ticos, este estudio es muy usado en el √°rea m√©dica, pues de lo contrario se estar√≠a imponiendo sobre las personas las condiciones que deben tener (como empezar a fumar, por ejemplo).\nDise√±o experimental: Este tipo de estudio posee m√°s estrategia. Aqu√≠ se manipulan las variables predictoras (o caracter√≠sticas) de acuerdo a un dise√±o experimental, para tener ciertos valores y observar su efecto en la variable respuesta."
  },
  {
    "objectID": "posts/la-regresion-lineal-en-machine-learning/index.html#regresi√≥n-lineal-simple",
    "href": "posts/la-regresion-lineal-en-machine-learning/index.html#regresi√≥n-lineal-simple",
    "title": "La Regresi√≥n Lineal en Machine Learning",
    "section": "Regresi√≥n Lineal Simple",
    "text": "Regresi√≥n Lineal Simple\nLa regresi√≥n lineal simple es √∫til cuando existe una tendencia lineal en los datos. Aunque veremos m√°s adelante que va m√°s all√° de lo lineal. Se le llama simple porque solo hay una caracter√≠stica \\(x\\), y se modela como\n\\[y = w_0 + w_1 x + \\varepsilon\\] La cual puede verse como la ecuaci√≥n de regresi√≥n lineal poblacional, no obstante, en regresi√≥n lineal trabajamos con \\(m\\) n√∫mero de datos, entonces para cada punto se tendr√° una ecuaci√≥n de regresi√≥n lineal muestral\n\\[y_i = w_0 + w_1 x_i + \\varepsilon_i \\quad i=1, 2, 3, ..., m\\]\nCompar√°ndo con la ecuaci√≥n de la recta, podemos interpretar los par√°metros. Se tiene que \\(w_1\\) es el cambio en la media de la distribuci√≥n de \\(y\\) producido por un cambio de unidad en \\(x\\). Mientras que \\(w_0\\) simplemente es la media de la distribuci√≥n de \\(y\\) cuando \\(x = 0\\), si el dominio de \\(x\\) no incluye a \\(0\\) entonces el par√°metro \\(w_0\\) no tiene interpretaci√≥n pr√°ctica.\nPor otro lado, \\(\\varepsilon\\) son los errores o ruido que se tiene en cada predicci√≥n de \\(y\\). Se asumen que dichos errores tienen una distribuci√≥n normal con media igual a cero \\(\\mu = 0\\) y una varianza desconicida \\(\\sigma^2\\). Es importante notar que, esa varianza es la misma en todos los puntos de la regresi√≥n, lo que se conoce como homocedasticidad. A los errores \\(\\varepsilon\\) se les llama residuales pues se definen como \\(y - \\hat{y}\\).\nAhora bien, retomando el diagrama de dispersi√≥n que se vio anteriormente, el cual tiene proviene de la recta \\(y = 2x\\) con un error \\(\\varepsilon\\) con media \\(0\\) y varianza \\(\\sigma^2 = 1.5^2\\). Se le puede ajustar una recta de regresi√≥n lineal y se ver√≠a como la siguiente figura\n\n\n\n\n\n\n\n\n\nObteniendo los coeficientes estimados \\(\\hat{w_0} = 0.3045\\) y \\(\\hat{w_1} = 2.0274\\). Teniendo as√≠ una ecuaci√≥n de regresi√≥n lineal con la que podemos hacer predicciones\n\\[ \\hat{y} = 0.3045 + 2.0274 x\\] Notemos que los par√°metros reales de donde provienen los datos son \\(0\\) y \\(2\\), pero por la naturaleza estoc√°stica de los errores, los estimados no coinciden con exactitud. Entre mayor sea el n√∫mero de datos \\(m\\), los par√°metros estimados ser√°n m√°s cercanos a los originales."
  },
  {
    "objectID": "posts/la-regresion-lineal-en-machine-learning/index.html#regresi√≥n-lineal-m√∫ltiple",
    "href": "posts/la-regresion-lineal-en-machine-learning/index.html#regresi√≥n-lineal-m√∫ltiple",
    "title": "La Regresi√≥n Lineal en Machine Learning",
    "section": "Regresi√≥n Lineal M√∫ltiple",
    "text": "Regresi√≥n Lineal M√∫ltiple\nPara el caso en que la variable respuesta \\(y\\) est√° realcionada con \\(n\\) variables regresoras, se tiene el modelo de la regresi√≥n lineal m√∫ltiple\n\\[ y = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n + \\varepsilon \\] As√≠, a los par√°metros \\(w_j \\quad j = 0, 1, 2, ..., n\\) se les llama coeficientes de regresi√≥n. Este modelo, al ser de multivariable, describe un hiperplano en el espacio \\(n\\)-dimensional de las variables regresoras \\(x\\).\nHaciendo sentido de los par√°metros. El par√°metro \\(w_j\\) representa el cambio esperado en la respuesta \\(y\\) por unidad de cambio en \\(x_j\\), cuando todo las dem√°s variables regresoras \\(x_i(i\\ne j)\\) se mantienen constantes.Por otra parte, el par√°metro \\(w_0\\) es la intersecci√≥n del hiperplano en \\(y\\), si el dominio de los datos incluye \\(x_1 = x_2 = ... = x_n = 0\\) entonces \\(w_0\\) es la media de \\(y\\) cuando \\(x_1 = x_2 = ... = x_n = 0\\); de lo contrario, \\(w_0\\) no tiene interpretaci√≥n f√≠sica.\nConcluyendo, en el modelo de regresi√≥n simple y m√∫ltiple, la linealidad se tiene en los par√°metros. Esto extiende m√°s all√° el concepto de linealidad aunque las variables regresoras \\(x\\) no sean lineales, esto se ver√° a continuaci√≥n en la secci√≥n de regresi√≥n polinomial."
  },
  {
    "objectID": "posts/la-regresion-lineal-en-machine-learning/index.html#regresi√≥n-polinomial",
    "href": "posts/la-regresion-lineal-en-machine-learning/index.html#regresi√≥n-polinomial",
    "title": "La Regresi√≥n Lineal en Machine Learning",
    "section": "Regresi√≥n Polinomial",
    "text": "Regresi√≥n Polinomial\nYa vimos que una regresi√≥n lineal de \\(n\\) variables regresoras genera un hiperplano \\(n\\)-dimensional. Ahora bien, cualquier modelo que es lineal en los par√°metros \\(w\\)‚Äôs es un modelo de regresi√≥n lineal, independientemente de la forma de la superficie que genere.\nComo su nombre lo indica, la regresi√≥n polinomial de una variable \\(x\\) se define a partir de los polinomios, y se modela de la siguiente forma\n\\[ y = w_0 + w_1x + w_2x^2 + ... + w_nx^n + \\varepsilon\\] La cual es lineal en los par√°metros, por lo que se puede trabajar como una regresi√≥n lineal m√∫ltiple y usar la misma metodolog√≠a. Para hacer esto, podemos definir \\(x_j = x^j \\quad j=1, 2, 3, ... , n\\). Por ejemplo, para el caso de un polinomio de grado 2 con \\(y = w_0 + w_1x + w_2x^2 + \\varepsilon\\), tendr√≠amos \\(x_1 = x, x_2 = x^2\\) y la regresi√≥n se convertir√≠a en una lineal m√∫ltiple\n\\[ y = w_0 + w_1x_1 + w_2x_2 + \\varepsilon\\] Con este tipo de regresi√≥n, hay que ser muy cuidadosos. Ya que un polinomio de grado considerablemente alto puede apr√≥ximar muy bien una curva no lineal, sin embargo, hay que tratar de hacer transformaciones y mantener el grado lo m√°s bajo posible para evitar un sobreajuste del modelo (empezar a generalizar el error mismo)."
  },
  {
    "objectID": "posts/la-regresion-lineal-en-machine-learning/index.html#supuestos-de-la-regresi√≥n-lineal",
    "href": "posts/la-regresion-lineal-en-machine-learning/index.html#supuestos-de-la-regresi√≥n-lineal",
    "title": "La Regresi√≥n Lineal en Machine Learning",
    "section": "Supuestos de la regresi√≥n lineal",
    "text": "Supuestos de la regresi√≥n lineal\nEs importante saber cu√°ndo es prudente utilizar un modelo de regresi√≥n lineal. Para que se cumplan las condiciones de la regresi√≥n lineal simple y m√∫ltiple, se tienen los siguientes supuestos necesarios\n\nLinealidad: La relaci√≥n entre las variables regresoras \\(x\\) y la variable repuesta \\(y\\) debe ser lineal. Notemos que en la regresi√≥n polinomial esto va m√°s all√°, necesitando solo linealidad en los par√°metros.\nHomocedasticidad: Significa que la varianza de los errores es constante.\nNormalidad de los errores: Los residulales, es decir los errores, deben seguir una distribuci√≥n normal con media 0 y varianza \\(\\sigma^2\\).\nErrores independientes (no autocorrelaci√≥n): Los errores no deben estar correlacionados entre ellos, es otras palabras, deben ser independientes e id√©nticamente distribuidos.\nNo multicolinealidad: No existe correlaci√≥n entre las variables regresoras.\nNo exogeneidad: Las variables regresoras y los errores no est√°n correlacionados."
  },
  {
    "objectID": "posts/la-regresion-lineal-en-machine-learning/index.html#m√≠nimos-cuadrados-ordinarios-como-funci√≥n-de-costo",
    "href": "posts/la-regresion-lineal-en-machine-learning/index.html#m√≠nimos-cuadrados-ordinarios-como-funci√≥n-de-costo",
    "title": "La Regresi√≥n Lineal en Machine Learning",
    "section": "M√≠nimos cuadrados ordinarios como funci√≥n de costo",
    "text": "M√≠nimos cuadrados ordinarios como funci√≥n de costo\nEn machine learning, la base para ajustar los par√°metros a nuestros datos, lo que se conoce como aprender, radica en la optimizaci√≥n de una funci√≥n objetivo. Dicho de otra manera, es minimizar una funci√≥n de costo, donde dicha funci√≥n ser√≠a una medida del error en las predicciones del modelo.\nEn la regresi√≥n lineal no es la excepci√≥n. Es muy conocido el m√©todo de m√≠nimos cuadrados para ajustar los par√°metros, donde se define la funci√≥n de costo \\(J\\) de m√≠nimos cuadrados ordinarios (MCO) de la siguiente manera\n\\[ J(y, \\hat{y}) = \\frac{1}{m} \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2 \\] Donde el factor \\(\\frac{1}{m}\\) se multiplica como una especie de promedio en todos los datos. Algunas referencias tambi√©n manejan multiplicarlo por \\(\\frac{1}{2}\\) por conveniencia algebraica.\nNotemos que la diferencia \\(y_i - \\hat{y}_i\\) es la distancia que hay desde un punto \\(\\hat{y} (x_i)\\) en la recta de la predicci√≥n \\(\\hat{y}\\) hasta el correspondiente punto \\(y(x_i)\\) en la recta original \\(y\\) de donde provienen los datos. Est√° al cuadrado para evitar que las sumas de esos t√©rminos puedan dar cero. As√≠, podemos interpretarlo gr√°ficamente como unos cuadrados en dichos puntos \\(x_i\\), considerando el mismo diagrama de dispersi√≥n de este post, se ver√≠a\n\n\n\n\n\n\n\n\n\nN√≥tese que los ejes fueron escalados para una mejor visualizaci√≥n. Ahora, al ajustar los par√°metros \\(w\\) minimizando la funci√≥n de costo de MCO, lo que se est√° haciendo gr√°ficamente es ajustar la recta de predicci√≥n de tal manera que se minimize el √°rea de esos cuadrados; de ah√≠ el nombre de este m√©todo.\nPara minimizar dicha funci√≥n, es mejor expresarla en t√©rmino de los par√°metros \\(w\\), para esto, en el caso de una variable regresora sustituimos \\(\\hat{y} = w_0 + w_1 x\\) en la funci√≥n de costo de MCO, as√≠\n\\[ J(w_0, w_1) = \\frac{1}{m} \\sum_{i=1}^{m} (y_i - w_0 - w_1 x_i)^2 \\] En la siguiente secci√≥n veremos c√≥mo minimizarla, ejemplificando el caso con una sola variable regresora \\(x\\)."
  },
  {
    "objectID": "posts/la-regresion-lineal-en-machine-learning/index.html#ajuste-de-los-par√°metros-minimizando-la-funci√≥n-de-costo",
    "href": "posts/la-regresion-lineal-en-machine-learning/index.html#ajuste-de-los-par√°metros-minimizando-la-funci√≥n-de-costo",
    "title": "La Regresi√≥n Lineal en Machine Learning",
    "section": "Ajuste de los par√°metros minimizando la funci√≥n de costo",
    "text": "Ajuste de los par√°metros minimizando la funci√≥n de costo\nHay 3 maneras principales de minimizar la funci√≥n de costo de m√≠nimos cuadrados ordinarios (MCO) \\(J\\). El m√©todo cl√°sico, utiliza c√°lculo diferencial y el criterio de las derivadas para minimizar. La segunda, utiliza el m√©todo de la m√°xima verosimilitud, un concepto estad√≠stico. Se encuentra matem√°ticamente que ambos m√©todos llegan a la misma forma soluci√≥n. La tercera, usa el m√©todo m√°s utilizado en machine learning, el del gradiente descendiente (gradient descent).\nMi objetivo de este post no es hacer demostraciones matem√°ticas ni c√°lculos completos, sino que ilustrar la metodolog√≠a, as√≠ que nos enfocaremos en el m√©todo cl√°sico y el de gradiente descendiente. Veamos a m√°s detalle\n\nEl m√©todo de m√≠nimos cuadrados cl√°sico\nPara minimizar la funci√≥n de costo, los estimados de los par√°metros \\(w_0\\), \\(w_1\\), que son \\(\\hat{w_0}\\), \\(\\hat{w_1}\\) respectivamente, deben satisfacer\n\\[ \\frac{\\partial J}{\\partial w_0} \\bigg|_{\\hat{w_0}, \\hat{w_1}}= - \\frac{2}{m} \\sum_{i=1}^{m} (y_i - \\hat{w_0} - \\hat{w_1} x_i) = 0\\] y\n\\[ \\frac{\\partial J}{\\partial w_1} \\bigg|_{\\hat{w_0}, \\hat{w_1}} = - \\frac{2}{m} \\sum_{i=1}^{m} (y_i - \\hat{w_0} - \\hat{w_1} x_i) x_i = 0\\]\nde acuerdo al criterio de la primer derivada, √©sta debe ser 0 para que sea un m√≠nimo.\nSimplificando las expresiones, separando las sumatorias y resolviendo para \\(\\hat{w_0}\\), \\(\\hat{w_1}\\), se tienen las soluciones a los par√°metros estimados que minimizan la funci√≥n de MCO\n\\[ \\hat{w_0} =  \\bar{y} - \\hat{w_1} \\bar{x}\\] y\n\\[ \\hat{w_1} = \\frac{ \\sum_{i=1}^{m} y_i x_i - \\frac{(\\sum_{i=1}^{m} y_i)(\\sum_{i=1}^{m} x_i)}{n} }{ \\sum_{i=1}^{m} x_i^2 - \\frac{(\\sum_{i=1}^{m}x_i)^2}{m} } \\]\ndonde \\(\\bar{x} = \\frac{1}{m}  \\sum_{i=1}^{m} x_i\\) y \\(\\bar{y} = \\frac{1}{m}  \\sum_{i=1}^{m} y_i\\) son los promedios de \\(x_i\\), \\(y_i\\), respectivamente.\nPara el caso multivariable, se tiene una soluci√≥n de los par√°metros estimados \\(W^{*}\\) en forma matricial, dado por\n\\[ W^{*} = (X^T X)^{-1} X^{T} Y \\] Donde \\(X\\) es la matriz de dise√±o, la cual es de tama√±o \\(m \\times (n + 1)\\). Donde \\(m\\) es el n√∫mero de datos y \\(n\\) el n√∫mero de variables regresoras. Se construye poniendo en filas los vectores de datos \\(x_j = (1, x_{j1}, x_{j2}, ..., x_{jn})\\), donde \\(j = 1, 2, 3, ..., m\\), en todos los datos. Por otro lado, \\(Y\\) es la matriz (o vector) de tama√±o \\(m \\times 1\\), que se construye poniendo todos los puntos \\(y\\) de donde se har√° la regresi√≥n.\nAs√≠,\n\\[ X = \\begin{pmatrix}\n1 & x_{11} & x_{12} & ... & x_{1n} \\\\\n1 & x_{21} & x_{22} & ... & x_{2n} \\\\\n&  & \\vdots &  &  \\\\\n1 & x_{m1} & x_{m2} & ... & x_{mn}\n\\end{pmatrix}  \\]\n\\[ Y = \\begin{pmatrix}\ny_1 \\\\\ny_2 \\\\\ny_3 \\\\\n\\vdots \\\\\ny_m\n\\end{pmatrix}  \\]\n\n\nEl m√©todo del gradiente descendiente\nRetomando conceptos de c√°lculo diferencial para una variable, la derivada de una funci√≥n \\(f\\) en un punto dado \\(x_0\\), nos da la pendiende de la recta tangente a la curva en ese punto. En otras palabras, esa pendiente puede interprestarse como la raz√≥n de cambio de la funci√≥n en ese punto y el signo indica hacia donde est√° cambiando. Extendiendo este concepto hacia varias variables, el gradiente de una funci√≥n \\(f\\), denotado como \\(\\nabla f\\), es un vector que indica hacia d√≥nde la funci√≥n tiene el mayor incremento, y su magnitud es la raz√≥n de cambio.\nEl m√©todo del gradiente descendiente, las derivadas son respecto a los par√°metros \\(w\\) que queremos ajustar, y como su nombre lo indica, hace uso del gradiente para minimizar la funci√≥n de costo \\(J(w)\\). Para el caso de la regresi√≥n lineal, el algoritmo se define de la siguiente manera\n\\[ w_i^{(\\tau + 1)} := w_i^{(\\tau)} - \\alpha \\frac{\\partial J(w)}{\\partial w_i}\\] donde \\(J(w)\\) esl funci√≥n de costo de m√≠nimos cuadrados ordinarios. La derivada est√° multiplicada por un \\(-1\\), lo que asegura que se mueva en la direcci√≥n de menor crecimiento, tenemos que \\(\\alpha\\) es una constante llamada tasa de aprendizaje, controla qu√© tanto se ajusta el valor de \\(w_i\\). En machine learning, \\(\\alpha\\) suele tomar valores muy peque√±os, como \\(0.01, 0.001,\\) etc. Por otro lado, \\(\\tau\\) es un n√∫mero entero que indica en la iteraci√≥n que estamos del algoritmo.\nAs√≠, se estar√°n ajustando los par√°metros \\(w_i\\) de manera iterativa hasta que se cumpla un cierto criterio o se alcance una tolerancia del error. Cabe se√±alar que en la primera iteraci√≥n, los par√°metros se inicializan de manera aleatoria.\nPor lo tanto, para el caso de regresi√≥n lineal simple, si utilizamos el m√©todo del gradiente descendiente tendriamos lo siguiente\n\\[ w_0^{(\\tau + 1)} := w_0^{(\\tau)} + \\alpha \\frac{2}{m} \\sum_{i=1}^{m} (y_i - \\hat{y_i})\\] \\[ w_1^{(\\tau + 1)} := w_1^{(\\tau)} + \\alpha \\frac{2}{m} \\sum_{i=1}^{m} (y_i - \\hat{y_i}) x_i\\] Donde \\(\\hat{y_i}\\) son las predicciones en la iteraci√≥n \\(\\tau\\), o sea antes de hacer la actualizaci√≥n de los par√°metros.\n\n\n¬øCu√°l usar de los 2 m√©todos?\nLa respuesta r√°dica en la complejidad computacional y el tiempo que toman los c√°lculos. En machine learning se trabajan con datos demasiado grandes, m√°s de lo que se sol√≠a hacer en la estad√≠stica convencional, por lo tanto, la forma matricial de obtener los par√°metros puede llegar a ser muy costosa computacionalmente hablando si se tienen muchos datos y muchas caracter√≠sticas (variables regresoras). As√≠, es m√°s conveniente utilizar el m√©todo del gradiente descendiente, que en la pr√°ctica suele ser m√°s eficiente."
  },
  {
    "objectID": "posts/la-regresion-lineal-en-machine-learning/index.html#m√©todos-de-regularizaci√≥n",
    "href": "posts/la-regresion-lineal-en-machine-learning/index.html#m√©todos-de-regularizaci√≥n",
    "title": "La Regresi√≥n Lineal en Machine Learning",
    "section": "M√©todos de regularizaci√≥n",
    "text": "M√©todos de regularizaci√≥n\nLa regularizaci√≥n en regresi√≥n lineal, es para reducir la complejidad del modelo evitando as√≠ un sobreajuste. Esto se logra a√±adiendo un t√©rmino a la funci√≥n de costo de MCO, lo que acota los par√°metros evitando que crezcan demasiado. Hay 2 casos, Ridge y Lasso, como se ver√° a continuaci√≥n.\n\nRegresi√≥n Lineal Ridge: limitando el tama√±o de los par√°metros\nLa regresi√≥n ridge hace que los par√°metros \\(w_i\\) se encojan, es decir, que no tomen valores tan grandes. Se define con la siguiente funci√≥n de costo \\(J_R(w)\\)\n\\[ J_R(W) = \\frac{1}{m} \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{n} w_j^2 \\] Sumando as√≠ el t√©rmino \\(\\lambda \\sum_{j=1}^{n} w_j^2\\) de penalizaci√≥n. Donde \\(\\lambda\\) controla que tanto se encojen los par√°metros, entre m√°s grande es el valor de \\(\\lambda\\) est√°n m√°s l√≠mitados los par√°metros. Se tiene que cuando \\(\\lambda \\xrightarrow{}{} \\infty\\), los par√°metros \\(w_i\\) tienden a \\(0\\).\nNotemos que el par√°metro \\(w_0\\) no se incluye, ya que solo se toma en cuenta \\(w_1, w_2, ..., w_n\\). Eso es porque \\(w_0\\), como ya vimos, es el valor promedio de \\(y\\) cuando \\(x_1=x_2=...=x_n=0\\). No ser√≠a conveniente imponer restricciones o encoger dicho par√°metro entonces.\nA la regresi√≥n ridge tambi√©n se le conoce como regularizaci√≥n L2, debido a que el t√©rmino \\(\\sum_{j=1}^{n} w_j^2\\) es la norma L2 del vector de par√°metros \\(w = (w_1, w_2, ..., w_n)\\).\n\n\nRegresi√≥n Lineal Lasso: un selector de caracter√≠sticas\nLa regresi√≥n lasso act√∫a como un selector de caracter√≠sticas, ya que impone que algunos de los par√°metros \\(w_i\\) sean igual a \\(0\\). Se define con la siguiente funci√≥n de costo \\(J_l(w)\\)\n\\[ J_l(W) = \\frac{1}{m} \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{n} |w_j| \\] Sumando as√≠ el t√©rmino \\(\\lambda \\sum_{j=1}^{n} |w_j|\\) de penalizaci√≥n. Similarmente, lasso tambi√©n hace que los par√°metros se encojan hac√≠a 0, pero a diferencia de ridge, si \\(\\lambda\\) es suficientemente grande algunos par√°metros \\(w_i\\) van a ser \\(0\\). As√≠, la variable regresora (o caracter√≠stica) \\(x_i\\) asociado al par√°metro \\(w_i\\) no estar√° involucrada en la ecuaci√≥n de regresi√≥n, por esta raz√≥n lasso es un selector de caracter√≠sticas.\nIgual que en el caso de regresi√≥n ridge, el par√°metro \\(w_0\\) no se incluye, ya que no ser√≠a conveniente imponer restricciones o hacer 0 el par√°metro \\(w_0\\), siendo que es es el valor promedio de \\(y\\) cuando \\(x_1=x_2=...=x_n=0\\).\nA la regresi√≥n lasso tambi√©n se le conoce como regularizaci√≥n L1, debido a que el t√©rmino \\(\\sum_{j=1}^{n} |w_j|\\) es la norma L1 del vector de par√°metros \\(w = (w_1, w_2, ..., w_n)\\).\n\n\nInterpretaci√≥n geom√©trica de ridge y lasso\nSurge la pregunta, ¬øPor qu√© la regresi√≥n ridge encoje los coeficientes pero la regresi√≥n lasso fuerza algunos de ellos a ser \\(0\\)? Para contestar esto, encontr√© que la mejor manera es gr√°ficamente.\nLa funci√≥n de costo de m√≠nimos cuadrados ordinarios MCO, es una funci√≥n convexa, por lo que es posible minimizarla. Ahora bien, si la graficamos en el espacio de los par√°metros al minimizar se pueden probar todos las combinaciones de par√°metros, sin embargo, al a√±adir un t√©rmino de penalizaci√≥n como en ridge y lasso, se hace una constricci√≥n en el espacio en el que puede bucarse el m√≠nimo. Veamos la siguiente figura (extra√≠da del libro Introduction to Statistical Learning)\n\n\n\nCurvas de nivel de la funci√≥n de costo de MCO en rojo y las constricciones de ridge y lasso en azul. Graficadas en el espacio de 2 par√°metros \\(w_1\\) y \\(w_2\\). El m√≠nimo se encuentra en \\(\\hat{w}\\).\n\n\nEn este caso, es una regresi√≥n en 2 dimensiones con \\(x_1\\), \\(x_2\\) y las gr√°ficas est√°n hechas en el espacio de los par√°metros \\(w_1\\), \\(w_2\\). Las l√≠neas rojas son las curvas de nivel de la funci√≥n de costo de MCO, y las √°reas s√≥lidas azules son las regiones de constricciones impuestas en los par√°metros \\(w_1\\), \\(w_2\\) por los t√©rminos de penalizaci√≥n de lasso y ridge, respectivamente.\nPara el caso de ridge, tenemos la constricci√≥n el c√≠rculo \\(w_1^2 + w_2^2 \\le s\\), y para lasso el rombo \\(|w_1| + |w_2| \\le s\\). El punto de minimizaci√≥n, donde est√°n los mejores par√°metros estimados, es la intersecci√≥n m√°s pronta entre la curva de nivel de MCO y el √°rea de constricci√≥n. Podemos observar claramente que para lasso, este punto de intersecci√≥n m√°s pronto se encuentra en la punta del rombo, es decir, sobre un eje donde el par√°metro \\(w_2 = 0\\); mientras que para ridge, el punto se encuentra sobre la circunferencia fuera de los ejes, por los que los par√°metros estar√°n encojidos por la constricci√≥n pero no ser√°n 0. Esto se extiende a cualquier dimensi√≥n, para \\(n \\gt 2\\), el rombo se convierte en un politopo mientras que el c√≠rculo en una hiperesfera. Entonces para el politopo siempre habr√° esquinas donde los par√°metros sean 0."
  },
  {
    "objectID": "posts/la-regresion-lineal-en-machine-learning/index.html#conclusiones",
    "href": "posts/la-regresion-lineal-en-machine-learning/index.html#conclusiones",
    "title": "La Regresi√≥n Lineal en Machine Learning",
    "section": "Conclusiones",
    "text": "Conclusiones\nEn este post, se vi√≥ c√≥mo se define la regresi√≥n lineal en una y varias variables, y los supuestos que se deben cumplir para su utilizaci√≥n. Se explic√≥ c√≥mo minimizar su funci√≥n de costo de m√≠nimos cuadrados ordinarios en la manera cl√°sica y con el m√©todo de gradiente descendiente. Tambi√©n, se vieron los m√©todos de regularizaci√≥n de regresi√≥n ridge y lasso, y c√≥mo influyen en los par√°metros encoji√©ndolos o haciendo algunos de ellos \\(0\\), respectivamente.\nComo trabajo futuro, queda llevar este conocimiento a implementaci√≥n en c√≥digo, para ver su funcionalidad pr√°ctica. Existen librer√≠as para hacer regresi√≥n lineal, ridge y lasso; en Python est√° scikit-learn; mientras que en R tenemos stats y glmnet.\nRecomiendo ampliamente revisar las referencias usadas para este post, hay conocimiento muy bueno en ellas."
  },
  {
    "objectID": "posts/la-regresion-lineal-en-machine-learning/index.html#referencias",
    "href": "posts/la-regresion-lineal-en-machine-learning/index.html#referencias",
    "title": "La Regresi√≥n Lineal en Machine Learning",
    "section": "Referencias",
    "text": "Referencias\n\nDouglas C. Montgomery, Elizabeth A. Peck, G. Geoffrey Vining (2021). Introduction to Linear Regression Analysis. Sixth Edition.\nGareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani (2023). An Introduction to Statistical Learning with Applications in R. 2nd Edition.\nYunpeng Tai (2021). A Survey Of Regression Algorithms And Connections With Deep Learning.\nMukul Ranjan (2022). How does Lasso regression(L1) encourage zero coefficients but not the L2?. Medium. Post link\nTerence Parr. A visual explanation for regularization of linear models. Explained.ai. Post link\nUser: QuantStyle. What is the computational cost of gradient descent vs linear regression?. StackExchange. Post link"
  },
  {
    "objectID": "posts/5-cosas-que-aprendi-en-fisica-y-uso-en-ciencia-de-datos/index.html",
    "href": "posts/5-cosas-que-aprendi-en-fisica-y-uso-en-ciencia-de-datos/index.html",
    "title": "5 Cosas que Aprend√≠ en F√≠sica y Uso en Ciencia de Datos",
    "section": "",
    "text": "En este post voy a compartir 5 cosas que aprend√≠ en f√≠sica, que me han sido de utilidad en la ciencia de datos. Ya sea una metodolog√≠a en particular o un concepto an√°logo. Desde las matem√°ticas del √°lgebra lineal y c√°lculo diferencial, hasta como la relatividad especial de Einstein me record√≥ al concepto de Big Data."
  },
  {
    "objectID": "posts/5-cosas-que-aprendi-en-fisica-y-uso-en-ciencia-de-datos/index.html#los-datos-son-lo-m√°s-valioso",
    "href": "posts/5-cosas-que-aprendi-en-fisica-y-uso-en-ciencia-de-datos/index.html#los-datos-son-lo-m√°s-valioso",
    "title": "5 Cosas que Aprend√≠ en F√≠sica y Uso en Ciencia de Datos",
    "section": "Los datos son lo m√°s valioso",
    "text": "Los datos son lo m√°s valioso\n\nGarbage in, garbage out.\n\nEn f√≠sica los datos se obtienen de mediciones hechas en experimentos, ya sea directamente o indirectamente mediante observaciones. Tambi√©n est√° la posibilidad de las simulaciones computacionales. Lo que aprend√≠ durante los laboratorios que curs√©, es que los datos son lo m√°s valioso.\nAl hacer experimentos, se debe procurar no perturbar el sistema en estudio para no inducir ruido en los datos, para que as√≠ las mediciones sean m√°s cercanas a las reales, adem√°s de verificar que no est√©n influyendo factores externos. Similarmente en machine learning, los datos no deben tener sesgos que afecten el resultado de los modelos. En ambas disciplinas, si se quieren modelos y explicaciones s√≥lidas, los datos tienen que ser m√°s que buenos."
  },
  {
    "objectID": "posts/5-cosas-que-aprendi-en-fisica-y-uso-en-ciencia-de-datos/index.html#el-concepto-de-big-data-recuerda-a-la-relatividad-especial-de-einstein",
    "href": "posts/5-cosas-que-aprendi-en-fisica-y-uso-en-ciencia-de-datos/index.html#el-concepto-de-big-data-recuerda-a-la-relatividad-especial-de-einstein",
    "title": "5 Cosas que Aprend√≠ en F√≠sica y Uso en Ciencia de Datos",
    "section": "El concepto de Big Data recuerda a la relatividad especial de Einstein",
    "text": "El concepto de Big Data recuerda a la relatividad especial de Einstein\nLa mec√°nica cl√°sica de Newton formula 3 leyes, de las cuales se derivan las ecuaciones de movimiento. Resolviendo dichas ecuaciones, se puede describir el movimiento de un objeto con su posici√≥n en funci√≥n del tiempo, como lo son el movimiento de un paracaidista en ca√≠da libre, el de un tren o el de una pelota de baseball que va de home run.\nPara el a√±o 1905, Albert Einstein desarroll√≥ su teor√≠a especial de la relatividad, la cual postula que la velocidad de la luz (denotada como c) es una constante universal. Esto tiene implicaciones en las ecuaciones de la mec√°nica cl√°sica antes mencionadas, pues ya hay un l√≠mite en la velocidad m√°xima que se puede alcanzar, por lo que las f√≥rmulas se tienen que ajustar para cuando las velocidades son muy cercanas a la de la luz. Usualmente esto resulta con un factor de multuplicaci√≥n llamado gamma.\nAhora, Big Data surge cuando los datos son tan masivos que los m√©todos y tecnolog√≠a convencionales no pueden procesar esa cantidad. Big Data suele definirse con las 3 V‚Äôs, Volumen, Velocidad y Variedad. Para solventar este problema, surgieron metodolog√≠as como MapReduce, tecnolog√≠as como Hadoop y Spark, adem√°s de los avances en el hardware de c√≥mputo.\nAs√≠, Big Data me recuerda a la relatividad especial de Einstein, en el sentido que cuando traspasamos un cierto umbral (ya sea de cantidad de datos o de velocidad), tenemos que aprender a trabajar con nuevas metodolog√≠as y hacer ajustes a lo ya establecido."
  },
  {
    "objectID": "posts/5-cosas-que-aprendi-en-fisica-y-uso-en-ciencia-de-datos/index.html#el-m√©todo-cient√≠fico-y-la-reproducibilidad",
    "href": "posts/5-cosas-que-aprendi-en-fisica-y-uso-en-ciencia-de-datos/index.html#el-m√©todo-cient√≠fico-y-la-reproducibilidad",
    "title": "5 Cosas que Aprend√≠ en F√≠sica y Uso en Ciencia de Datos",
    "section": "El m√©todo cient√≠fico y la reproducibilidad",
    "text": "El m√©todo cient√≠fico y la reproducibilidad\nLa f√≠sica utiliza el m√©todo cient√≠fico para sus experimentos y teor√≠as. En base a esto, cada experimento debe ser replicable si se tienen las mismas circunstancias. Esto asegura la solidez de una teor√≠a.\nSimilarmente, en ciencia de datos todos los experimentos que se hagan con modelos deben ser replicables. Es por esto que el c√≥digo es tan utilizado en esta √°rea, ya que es m√°s f√°cil de replicar todas las configuraciones y experimentos en c√≥digo, que hacerlo mediante clicks donde se pueden perder pasos. Vaya, es por algo que ciencia de datos lleva el nombre ciencia."
  },
  {
    "objectID": "posts/5-cosas-que-aprendi-en-fisica-y-uso-en-ciencia-de-datos/index.html#los-modelos-son-representaciones-de-la-realidad",
    "href": "posts/5-cosas-que-aprendi-en-fisica-y-uso-en-ciencia-de-datos/index.html#los-modelos-son-representaciones-de-la-realidad",
    "title": "5 cosas que aprend√≠ en f√≠sica y uso en ciencia de datos",
    "section": "Los modelos son representaciones de la realidad",
    "text": "Los modelos son representaciones de la realidad\n\nAll models are wrong, but some are useful. - George Box\n\nLos modelos que surgen de las teor√≠as f√≠sicas que se fundamentan en experimentos, en realidad son representaciones aproximadas de la realidad. Pero pueden ser aproximaciones tan exactas que funcionan perfectamente. En f√≠sica siempre puedes ajustar modelos tomando en cuenta o ignorando ciertos factores, y no siempre el modelo debe ser una representaci√≥n id√©ntica a la realidad, si no que debe resolver el problema del sistema que se est√© estudiando.\nSimilarmente en la ciencia de datos, en el machine learning, los modelos no van a ser una representaci√≥n exacta de la tarea ala que se le est√© ajustando al modelo. Siempre hay que tener en cuenta que el modelo aprende de los datos, generaliza patrones pero debe tenerse cuidado de no obviar que el modelo es una explicaci√≥n de la realidad."
  },
  {
    "objectID": "posts/5-cosas-que-aprendi-en-fisica-y-uso-en-ciencia-de-datos/index.html#se-comparten-fundamentos-matem√°ticos",
    "href": "posts/5-cosas-que-aprendi-en-fisica-y-uso-en-ciencia-de-datos/index.html#se-comparten-fundamentos-matem√°ticos",
    "title": "5 Cosas que Aprend√≠ en F√≠sica y Uso en Ciencia de Datos",
    "section": "Se comparten fundamentos matem√°ticos",
    "text": "Se comparten fundamentos matem√°ticos\nLas matem√°ticas que aprend√≠ en f√≠sica sin duda alguna me han sido de gran utilidad en la ciencia de datos. Desde el c√°lculo, base de la mec√°nica cl√°sica, o el √°lgebra lineal de la mec√°nica cu√°ntica; que a su vez son fundamental para las computaciones y la optimizaci√≥n en ciencia de datos. Por ejemplo, en machine learning para entrenar un modelo hay que minimizar una funci√≥n de costo con c√°lculo, as√≠ mismo, los modelos se definen mediante operaciones matriciales.\nNi hablar de la probabilidad y la estad√≠stica, que aprend√≠ durante mi carrera en f√≠sica, tambi√©n son un pilar en ciencia de datos. Desde histogramas, diagramas de cajas, modelos estad√≠sticos de regresi√≥n y clasificaci√≥n, hasta el enfoque probabil√≠stico del machine learning. Cabe resaltar que si se hace un cambio de carrera desde f√≠sica a la ciencia de datos, las matem√°ticas ser√°n un fuerte."
  },
  {
    "objectID": "posts/5-cosas-que-aprendi-en-fisica-y-uso-en-ciencia-de-datos/index.html#los-modelos-son-representaciones-aproximadas-de-la-realidad",
    "href": "posts/5-cosas-que-aprendi-en-fisica-y-uso-en-ciencia-de-datos/index.html#los-modelos-son-representaciones-aproximadas-de-la-realidad",
    "title": "5 Cosas que Aprend√≠ en F√≠sica y Uso en Ciencia de Datos",
    "section": "Los modelos son representaciones aproximadas de la realidad",
    "text": "Los modelos son representaciones aproximadas de la realidad\n\nAll models are wrong, but some are useful. - George Box\n\nLos modelos que surgen de las teor√≠as f√≠sicas que se fundamentan en experimentos, en realidad son representaciones aproximadas de la realidad. Pero pueden ser aproximaciones tan exactas que funcionan perfectamente. En f√≠sica siempre puedes crear modelos ignorando ciertos factores, y no siempre el modelo debe ser una representaci√≥n id√©ntica a la realidad, si no que debe resolver el problema del sistema que se est√© estudiando.\nSimilarmente en la ciencia de datos, en el machine learning, los modelos no van a ser una representaci√≥n exacta de la tarea a la cual se est√©n ajustando. Siempre hay que tener en cuenta que el modelo aprende de los datos, generaliza patrones, pero debe tenerse cuidado de no obviar al modelo como una replica de la realidad."
  }
]